{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075121d9-02f2-4238-83c1-5a50b1779e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features after Lasso: 20\n",
      "Iteration 1, loss = 1.22544007\n",
      "Iteration 2, loss = 1.22340422\n",
      "Iteration 3, loss = 1.22138118\n",
      "Iteration 4, loss = 1.21936921\n",
      "Iteration 5, loss = 1.21736997\n",
      "Iteration 6, loss = 1.21537646\n",
      "Iteration 7, loss = 1.21338763\n",
      "Iteration 8, loss = 1.21140680\n",
      "Iteration 9, loss = 1.20943458\n",
      "Iteration 10, loss = 1.20746938\n",
      "Iteration 11, loss = 1.20551414\n",
      "Iteration 12, loss = 1.20357218\n",
      "Iteration 13, loss = 1.20162421\n",
      "Iteration 14, loss = 1.19968668\n",
      "Iteration 15, loss = 1.19776181\n",
      "Iteration 16, loss = 1.19584975\n",
      "Iteration 17, loss = 1.19394875\n",
      "Iteration 18, loss = 1.19205525\n",
      "Iteration 19, loss = 1.19017020\n",
      "Iteration 20, loss = 1.18829376\n",
      "Iteration 21, loss = 1.18642736\n",
      "Iteration 22, loss = 1.18456935\n",
      "Iteration 23, loss = 1.18271947\n",
      "Iteration 24, loss = 1.18087764\n",
      "Iteration 25, loss = 1.17904439\n",
      "Iteration 26, loss = 1.17722146\n",
      "Iteration 27, loss = 1.17540802\n",
      "Iteration 28, loss = 1.17360821\n",
      "Iteration 29, loss = 1.17182505\n",
      "Iteration 30, loss = 1.17006114\n",
      "Iteration 31, loss = 1.16830419\n",
      "Iteration 32, loss = 1.16655329\n",
      "Iteration 33, loss = 1.16480334\n",
      "Iteration 34, loss = 1.16306258\n",
      "Iteration 35, loss = 1.16134096\n",
      "Iteration 36, loss = 1.15962923\n",
      "Iteration 37, loss = 1.15792476\n",
      "Iteration 38, loss = 1.15622450\n",
      "Iteration 39, loss = 1.15453281\n",
      "Iteration 40, loss = 1.15285091\n",
      "Iteration 41, loss = 1.15117665\n",
      "Iteration 42, loss = 1.14950791\n",
      "Iteration 43, loss = 1.14783702\n",
      "Iteration 44, loss = 1.14616978\n",
      "Iteration 45, loss = 1.14451098\n",
      "Iteration 46, loss = 1.14285657\n",
      "Iteration 47, loss = 1.14120781\n",
      "Iteration 48, loss = 1.13956454\n",
      "Iteration 49, loss = 1.13792344\n",
      "Iteration 50, loss = 1.13628745\n",
      "Iteration 51, loss = 1.13465635\n",
      "Iteration 52, loss = 1.13303277\n",
      "Iteration 53, loss = 1.13141769\n",
      "Iteration 54, loss = 1.12980828\n",
      "Iteration 55, loss = 1.12821062\n",
      "Iteration 56, loss = 1.12661617\n",
      "Iteration 57, loss = 1.12502940\n",
      "Iteration 58, loss = 1.12344958\n",
      "Iteration 59, loss = 1.12187705\n",
      "Iteration 60, loss = 1.12030997\n",
      "Iteration 61, loss = 1.11874951\n",
      "Iteration 62, loss = 1.11719604\n",
      "Iteration 63, loss = 1.11565523\n",
      "Iteration 64, loss = 1.11413875\n",
      "Iteration 65, loss = 1.11262921\n",
      "Iteration 66, loss = 1.11112963\n",
      "Iteration 67, loss = 1.10963721\n",
      "Iteration 68, loss = 1.10815252\n",
      "Iteration 69, loss = 1.10668066\n",
      "Iteration 70, loss = 1.10522120\n",
      "Iteration 71, loss = 1.10377302\n",
      "Iteration 72, loss = 1.10233109\n",
      "Iteration 73, loss = 1.10089660\n",
      "Iteration 74, loss = 1.09946940\n",
      "Iteration 75, loss = 1.09805062\n",
      "Iteration 76, loss = 1.09664003\n",
      "Iteration 77, loss = 1.09523711\n",
      "Iteration 78, loss = 1.09384129\n",
      "Iteration 79, loss = 1.09245115\n",
      "Iteration 80, loss = 1.09106729\n",
      "Iteration 81, loss = 1.08968943\n",
      "Iteration 82, loss = 1.08832217\n",
      "Iteration 83, loss = 1.08696650\n",
      "Iteration 84, loss = 1.08562652\n",
      "Iteration 85, loss = 1.08429520\n",
      "Iteration 86, loss = 1.08297099\n",
      "Iteration 87, loss = 1.08165280\n",
      "Iteration 88, loss = 1.08034068\n",
      "Iteration 89, loss = 1.07903421\n",
      "Iteration 90, loss = 1.07773340\n",
      "Iteration 91, loss = 1.07643529\n",
      "Iteration 92, loss = 1.07514093\n",
      "Iteration 93, loss = 1.07385188\n",
      "Iteration 94, loss = 1.07256985\n",
      "Iteration 95, loss = 1.07129449\n",
      "Iteration 96, loss = 1.07002570\n",
      "Iteration 97, loss = 1.06876290\n",
      "Iteration 98, loss = 1.06750150\n",
      "Iteration 99, loss = 1.06624777\n",
      "Iteration 100, loss = 1.06499837\n",
      "Iteration 101, loss = 1.06375419\n",
      "Iteration 102, loss = 1.06251706\n",
      "Iteration 103, loss = 1.06128646\n",
      "Iteration 104, loss = 1.06006367\n",
      "Iteration 105, loss = 1.05884859\n",
      "Iteration 106, loss = 1.05763468\n",
      "Iteration 107, loss = 1.05641774\n",
      "Iteration 108, loss = 1.05519941\n",
      "Iteration 109, loss = 1.05398534\n",
      "Iteration 110, loss = 1.05277232\n",
      "Iteration 111, loss = 1.05156284\n",
      "Iteration 112, loss = 1.05035598\n",
      "Iteration 113, loss = 1.04915430\n",
      "Iteration 114, loss = 1.04795569\n",
      "Iteration 115, loss = 1.04676280\n",
      "Iteration 116, loss = 1.04557225\n",
      "Iteration 117, loss = 1.04438438\n",
      "Iteration 118, loss = 1.04319914\n",
      "Iteration 119, loss = 1.04201675\n",
      "Iteration 120, loss = 1.04083811\n",
      "Iteration 121, loss = 1.03966481\n",
      "Iteration 122, loss = 1.03850613\n",
      "Iteration 123, loss = 1.03735045\n",
      "Iteration 124, loss = 1.03619900\n",
      "Iteration 125, loss = 1.03504928\n",
      "Iteration 126, loss = 1.03390491\n",
      "Iteration 127, loss = 1.03276390\n",
      "Iteration 128, loss = 1.03162125\n",
      "Iteration 129, loss = 1.03048109\n",
      "Iteration 130, loss = 1.02934385\n",
      "Iteration 131, loss = 1.02820880\n",
      "Iteration 132, loss = 1.02707738\n",
      "Iteration 133, loss = 1.02595152\n",
      "Iteration 134, loss = 1.02482715\n",
      "Iteration 135, loss = 1.02370613\n",
      "Iteration 136, loss = 1.02259232\n",
      "Iteration 137, loss = 1.02148211\n",
      "Iteration 138, loss = 1.02037172\n",
      "Iteration 139, loss = 1.01926536\n",
      "Iteration 140, loss = 1.01815894\n",
      "Iteration 141, loss = 1.01705446\n",
      "Iteration 142, loss = 1.01595126\n",
      "Iteration 143, loss = 1.01484991\n",
      "Iteration 144, loss = 1.01375093\n",
      "Iteration 145, loss = 1.01265118\n",
      "Iteration 146, loss = 1.01155512\n",
      "Iteration 147, loss = 1.01046479\n",
      "Iteration 148, loss = 1.00937744\n",
      "Iteration 149, loss = 1.00829438\n",
      "Iteration 150, loss = 1.00721374\n",
      "Iteration 151, loss = 1.00613890\n",
      "Iteration 152, loss = 1.00506506\n",
      "Iteration 153, loss = 1.00399582\n",
      "Iteration 154, loss = 1.00293154\n",
      "Iteration 155, loss = 1.00187015\n",
      "Iteration 156, loss = 1.00081198\n",
      "Iteration 157, loss = 0.99975827\n",
      "Iteration 158, loss = 0.99870658\n",
      "Iteration 159, loss = 0.99765713\n",
      "Iteration 160, loss = 0.99661335\n",
      "Iteration 161, loss = 0.99557108\n",
      "Iteration 162, loss = 0.99453330\n",
      "Iteration 163, loss = 0.99349534\n",
      "Iteration 164, loss = 0.99245988\n",
      "Iteration 165, loss = 0.99142656\n",
      "Iteration 166, loss = 0.99039707\n",
      "Iteration 167, loss = 0.98936972\n",
      "Iteration 168, loss = 0.98834498\n",
      "Iteration 169, loss = 0.98732217\n",
      "Iteration 170, loss = 0.98630426\n",
      "Iteration 171, loss = 0.98529400\n",
      "Iteration 172, loss = 0.98428595\n",
      "Iteration 173, loss = 0.98328223\n",
      "Iteration 174, loss = 0.98227791\n",
      "Iteration 175, loss = 0.98127553\n",
      "Iteration 176, loss = 0.98027661\n",
      "Iteration 177, loss = 0.97928059\n",
      "Iteration 178, loss = 0.97828562\n",
      "Iteration 179, loss = 0.97729569\n",
      "Iteration 180, loss = 0.97630529\n",
      "Iteration 181, loss = 0.97531921\n",
      "Iteration 182, loss = 0.97433649\n",
      "Iteration 183, loss = 0.97335484\n",
      "Iteration 184, loss = 0.97237454\n",
      "Iteration 185, loss = 0.97139410\n",
      "Iteration 186, loss = 0.97041695\n",
      "Iteration 187, loss = 0.96944293\n",
      "Iteration 188, loss = 0.96846964\n",
      "Iteration 189, loss = 0.96749813\n",
      "Iteration 190, loss = 0.96652603\n",
      "Iteration 191, loss = 0.96555373\n",
      "Iteration 192, loss = 0.96458241\n",
      "Iteration 193, loss = 0.96361362\n",
      "Iteration 194, loss = 0.96264894\n",
      "Iteration 195, loss = 0.96168676\n",
      "Iteration 196, loss = 0.96072272\n",
      "Iteration 197, loss = 0.95976058\n",
      "Iteration 198, loss = 0.95880093\n",
      "Iteration 199, loss = 0.95784272\n",
      "Iteration 200, loss = 0.95688649\n",
      "Iteration 201, loss = 0.95593074\n",
      "Iteration 202, loss = 0.95497969\n",
      "Iteration 203, loss = 0.95403018\n",
      "Iteration 204, loss = 0.95308304\n",
      "Iteration 205, loss = 0.95213581\n",
      "Iteration 206, loss = 0.95119081\n",
      "Iteration 207, loss = 0.95024756\n",
      "Iteration 208, loss = 0.94930464\n",
      "Iteration 209, loss = 0.94836602\n",
      "Iteration 210, loss = 0.94742590\n",
      "Iteration 211, loss = 0.94648517\n",
      "Iteration 212, loss = 0.94554314\n",
      "Iteration 213, loss = 0.94460269\n",
      "Iteration 214, loss = 0.94366424\n",
      "Iteration 215, loss = 0.94272519\n",
      "Iteration 216, loss = 0.94178958\n",
      "Iteration 217, loss = 0.94085689\n",
      "Iteration 218, loss = 0.93992461\n",
      "Iteration 219, loss = 0.93899879\n",
      "Iteration 220, loss = 0.93807165\n",
      "Iteration 221, loss = 0.93714803\n",
      "Iteration 222, loss = 0.93622804\n",
      "Iteration 223, loss = 0.93531381\n",
      "Iteration 224, loss = 0.93440031\n",
      "Iteration 225, loss = 0.93348692\n",
      "Iteration 226, loss = 0.93257513\n",
      "Iteration 227, loss = 0.93166497\n",
      "Iteration 228, loss = 0.93075695\n",
      "Iteration 229, loss = 0.92985245\n",
      "Iteration 230, loss = 0.92895127\n",
      "Iteration 231, loss = 0.92805080\n",
      "Iteration 232, loss = 0.92715367\n",
      "Iteration 233, loss = 0.92625987\n",
      "Iteration 234, loss = 0.92536943\n",
      "Iteration 235, loss = 0.92448050\n",
      "Iteration 236, loss = 0.92359227\n",
      "Iteration 237, loss = 0.92270577\n",
      "Iteration 238, loss = 0.92182035\n",
      "Iteration 239, loss = 0.92093913\n",
      "Iteration 240, loss = 0.92006150\n",
      "Iteration 241, loss = 0.91918935\n",
      "Iteration 242, loss = 0.91831852\n",
      "Iteration 243, loss = 0.91744717\n",
      "Iteration 244, loss = 0.91658018\n",
      "Iteration 245, loss = 0.91570888\n",
      "Iteration 246, loss = 0.91483398\n",
      "Iteration 247, loss = 0.91395733\n",
      "Iteration 248, loss = 0.91308178\n",
      "Iteration 249, loss = 0.91221016\n",
      "Iteration 250, loss = 0.91133983\n",
      "Iteration 251, loss = 0.91046784\n",
      "Iteration 252, loss = 0.90959580\n",
      "Iteration 253, loss = 0.90872945\n",
      "Iteration 254, loss = 0.90786254\n",
      "Iteration 255, loss = 0.90699300\n",
      "Iteration 256, loss = 0.90612277\n",
      "Iteration 257, loss = 0.90525526\n",
      "Iteration 258, loss = 0.90438744\n",
      "Iteration 259, loss = 0.90352040\n",
      "Iteration 260, loss = 0.90265227\n",
      "Iteration 261, loss = 0.90178356\n",
      "Iteration 262, loss = 0.90091505\n",
      "Iteration 263, loss = 0.90004523\n",
      "Iteration 264, loss = 0.89918143\n",
      "Iteration 265, loss = 0.89831861\n",
      "Iteration 266, loss = 0.89745538\n",
      "Iteration 267, loss = 0.89658926\n",
      "Iteration 268, loss = 0.89572408\n",
      "Iteration 269, loss = 0.89485839\n",
      "Iteration 270, loss = 0.89399577\n",
      "Iteration 271, loss = 0.89313657\n",
      "Iteration 272, loss = 0.89227836\n",
      "Iteration 273, loss = 0.89142239\n",
      "Iteration 274, loss = 0.89056565\n",
      "Iteration 275, loss = 0.88971027\n",
      "Iteration 276, loss = 0.88885701\n",
      "Iteration 277, loss = 0.88800522\n",
      "Iteration 278, loss = 0.88715714\n",
      "Iteration 279, loss = 0.88630853\n",
      "Iteration 280, loss = 0.88545911\n",
      "Iteration 281, loss = 0.88461482\n",
      "Iteration 282, loss = 0.88377073\n",
      "Iteration 283, loss = 0.88292951\n",
      "Iteration 284, loss = 0.88208867\n",
      "Iteration 285, loss = 0.88125268\n",
      "Iteration 286, loss = 0.88041555\n",
      "Iteration 287, loss = 0.87958273\n",
      "Iteration 288, loss = 0.87875035\n",
      "Iteration 289, loss = 0.87791800\n",
      "Iteration 290, loss = 0.87708912\n",
      "Iteration 291, loss = 0.87626161\n",
      "Iteration 292, loss = 0.87543540\n",
      "Iteration 293, loss = 0.87461036\n",
      "Iteration 294, loss = 0.87378995\n",
      "Iteration 295, loss = 0.87297018\n",
      "Iteration 296, loss = 0.87214962\n",
      "Iteration 297, loss = 0.87132748\n",
      "Iteration 298, loss = 0.87050789\n",
      "Iteration 299, loss = 0.86969007\n",
      "Iteration 300, loss = 0.86887298\n",
      "Iteration 301, loss = 0.86805842\n",
      "Iteration 302, loss = 0.86724412\n",
      "Iteration 303, loss = 0.86643222\n",
      "Iteration 304, loss = 0.86562465\n",
      "Iteration 305, loss = 0.86481982\n",
      "Iteration 306, loss = 0.86401667\n",
      "Iteration 307, loss = 0.86321532\n",
      "Iteration 308, loss = 0.86241237\n",
      "Iteration 309, loss = 0.86160981\n",
      "Iteration 310, loss = 0.86080719\n",
      "Iteration 311, loss = 0.86000565\n",
      "Iteration 312, loss = 0.85920543\n",
      "Iteration 313, loss = 0.85841737\n",
      "Iteration 314, loss = 0.85763239\n",
      "Iteration 315, loss = 0.85684555\n",
      "Iteration 316, loss = 0.85606040\n",
      "Iteration 317, loss = 0.85527610\n",
      "Iteration 318, loss = 0.85449053\n",
      "Iteration 319, loss = 0.85370464\n",
      "Iteration 320, loss = 0.85291983\n",
      "Iteration 321, loss = 0.85213335\n",
      "Iteration 322, loss = 0.85134972\n",
      "Iteration 323, loss = 0.85056668\n",
      "Iteration 324, loss = 0.84978360\n",
      "Iteration 325, loss = 0.84900141\n",
      "Iteration 326, loss = 0.84822103\n",
      "Iteration 327, loss = 0.84744281\n",
      "Iteration 328, loss = 0.84666836\n",
      "Iteration 329, loss = 0.84589602\n",
      "Iteration 330, loss = 0.84512393\n",
      "Iteration 331, loss = 0.84435119\n",
      "Iteration 332, loss = 0.84357735\n",
      "Iteration 333, loss = 0.84280687\n",
      "Iteration 334, loss = 0.84203591\n",
      "Iteration 335, loss = 0.84126898\n",
      "Iteration 336, loss = 0.84050660\n",
      "Iteration 337, loss = 0.83974356\n",
      "Iteration 338, loss = 0.83898380\n",
      "Iteration 339, loss = 0.83822555\n",
      "Iteration 340, loss = 0.83746796\n",
      "Iteration 341, loss = 0.83671210\n",
      "Iteration 342, loss = 0.83595797\n",
      "Iteration 343, loss = 0.83520426\n",
      "Iteration 344, loss = 0.83445225\n",
      "Iteration 345, loss = 0.83370124\n",
      "Iteration 346, loss = 0.83295220\n",
      "Iteration 347, loss = 0.83220551\n",
      "Iteration 348, loss = 0.83145971\n",
      "Iteration 349, loss = 0.83071479\n",
      "Iteration 350, loss = 0.82996822\n",
      "Iteration 351, loss = 0.82921874\n",
      "Iteration 352, loss = 0.82847082\n",
      "Iteration 353, loss = 0.82772354\n",
      "Iteration 354, loss = 0.82697710\n",
      "Iteration 355, loss = 0.82622932\n",
      "Iteration 356, loss = 0.82548221\n",
      "Iteration 357, loss = 0.82473527\n",
      "Iteration 358, loss = 0.82398905\n",
      "Iteration 359, loss = 0.82324614\n",
      "Iteration 360, loss = 0.82250270\n",
      "Iteration 361, loss = 0.82176009\n",
      "Iteration 362, loss = 0.82102110\n",
      "Iteration 363, loss = 0.82028222\n",
      "Iteration 364, loss = 0.81954579\n",
      "Iteration 365, loss = 0.81881157\n",
      "Iteration 366, loss = 0.81807773\n",
      "Iteration 367, loss = 0.81734556\n",
      "Iteration 368, loss = 0.81661418\n",
      "Iteration 369, loss = 0.81588322\n",
      "Iteration 370, loss = 0.81515326\n",
      "Iteration 371, loss = 0.81442675\n",
      "Iteration 372, loss = 0.81370305\n",
      "Iteration 373, loss = 0.81297943\n",
      "Iteration 374, loss = 0.81225688\n",
      "Iteration 375, loss = 0.81153640\n",
      "Iteration 376, loss = 0.81081658\n",
      "Iteration 377, loss = 0.81009673\n",
      "Iteration 378, loss = 0.80937790\n",
      "Iteration 379, loss = 0.80866114\n",
      "Iteration 380, loss = 0.80794740\n",
      "Iteration 381, loss = 0.80723678\n",
      "Iteration 382, loss = 0.80652637\n",
      "Iteration 383, loss = 0.80581708\n",
      "Iteration 384, loss = 0.80511039\n",
      "Iteration 385, loss = 0.80440530\n",
      "Iteration 386, loss = 0.80369944\n",
      "Iteration 387, loss = 0.80299760\n",
      "Iteration 388, loss = 0.80229514\n",
      "Iteration 389, loss = 0.80159181\n",
      "Iteration 390, loss = 0.80088870\n",
      "Iteration 391, loss = 0.80018704\n",
      "Iteration 392, loss = 0.79948479\n",
      "Iteration 393, loss = 0.79878552\n",
      "Iteration 394, loss = 0.79808777\n",
      "Iteration 395, loss = 0.79739171\n",
      "Iteration 396, loss = 0.79669804\n",
      "Iteration 397, loss = 0.79600796\n",
      "Iteration 398, loss = 0.79531883\n",
      "Iteration 399, loss = 0.79463232\n",
      "Iteration 400, loss = 0.79394839\n",
      "Iteration 401, loss = 0.79326352\n",
      "Iteration 402, loss = 0.79257730\n",
      "Iteration 403, loss = 0.79189225\n",
      "Iteration 404, loss = 0.79120745\n",
      "Iteration 405, loss = 0.79052425\n",
      "Iteration 406, loss = 0.78984358\n",
      "Iteration 407, loss = 0.78916186\n",
      "Iteration 408, loss = 0.78848051\n",
      "Iteration 409, loss = 0.78779884\n",
      "Iteration 410, loss = 0.78711863\n",
      "Iteration 411, loss = 0.78644123\n",
      "Iteration 412, loss = 0.78576535\n",
      "Iteration 413, loss = 0.78508998\n",
      "Iteration 414, loss = 0.78441620\n",
      "Iteration 415, loss = 0.78374421\n",
      "Iteration 416, loss = 0.78307406\n",
      "Iteration 417, loss = 0.78240477\n",
      "Iteration 418, loss = 0.78173688\n",
      "Iteration 419, loss = 0.78107226\n",
      "Iteration 420, loss = 0.78040779\n",
      "Iteration 421, loss = 0.77974557\n",
      "Iteration 422, loss = 0.77908645\n",
      "Iteration 423, loss = 0.77842782\n",
      "Iteration 424, loss = 0.77777125\n",
      "Iteration 425, loss = 0.77711674\n",
      "Iteration 426, loss = 0.77646467\n",
      "Iteration 427, loss = 0.77581402\n",
      "Iteration 428, loss = 0.77516463\n",
      "Iteration 429, loss = 0.77451778\n",
      "Iteration 430, loss = 0.77387420\n",
      "Iteration 431, loss = 0.77323272\n",
      "Iteration 432, loss = 0.77259305\n",
      "Iteration 433, loss = 0.77195522\n",
      "Iteration 434, loss = 0.77132024\n",
      "Iteration 435, loss = 0.77068735\n",
      "Iteration 436, loss = 0.77005628\n",
      "Iteration 437, loss = 0.76942488\n",
      "Iteration 438, loss = 0.76879643\n",
      "Iteration 439, loss = 0.76816964\n",
      "Iteration 440, loss = 0.76754385\n",
      "Iteration 441, loss = 0.76691851\n",
      "Iteration 442, loss = 0.76629423\n",
      "Iteration 443, loss = 0.76567320\n",
      "Iteration 444, loss = 0.76505211\n",
      "Iteration 445, loss = 0.76443003\n",
      "Iteration 446, loss = 0.76381032\n",
      "Iteration 447, loss = 0.76319204\n",
      "Iteration 448, loss = 0.76257643\n",
      "Iteration 449, loss = 0.76196231\n",
      "Iteration 450, loss = 0.76134896\n",
      "Iteration 451, loss = 0.76073829\n",
      "Iteration 452, loss = 0.76012961\n",
      "Iteration 453, loss = 0.75952244\n",
      "Iteration 454, loss = 0.75891572\n",
      "Iteration 455, loss = 0.75831068\n",
      "Iteration 456, loss = 0.75770780\n",
      "Iteration 457, loss = 0.75710647\n",
      "Iteration 458, loss = 0.75650633\n",
      "Iteration 459, loss = 0.75590933\n",
      "Iteration 460, loss = 0.75531475\n",
      "Iteration 461, loss = 0.75472005\n",
      "Iteration 462, loss = 0.75412699\n",
      "Iteration 463, loss = 0.75353532\n",
      "Iteration 464, loss = 0.75294565\n",
      "Iteration 465, loss = 0.75235693\n",
      "Iteration 466, loss = 0.75176709\n",
      "Iteration 467, loss = 0.75117793\n",
      "Iteration 468, loss = 0.75059056\n",
      "Iteration 469, loss = 0.75000640\n",
      "Iteration 470, loss = 0.74942805\n",
      "Iteration 471, loss = 0.74885222\n",
      "Iteration 472, loss = 0.74827819\n",
      "Iteration 473, loss = 0.74770559\n",
      "Iteration 474, loss = 0.74713468\n",
      "Iteration 475, loss = 0.74656595\n",
      "Iteration 476, loss = 0.74599902\n",
      "Iteration 477, loss = 0.74543293\n",
      "Iteration 478, loss = 0.74486844\n",
      "Iteration 479, loss = 0.74430638\n",
      "Iteration 480, loss = 0.74374595\n",
      "Iteration 481, loss = 0.74318670\n",
      "Iteration 482, loss = 0.74262773\n",
      "Iteration 483, loss = 0.74207033\n",
      "Iteration 484, loss = 0.74151317\n",
      "Iteration 485, loss = 0.74095810\n",
      "Iteration 486, loss = 0.74040529\n",
      "Iteration 487, loss = 0.73985344\n",
      "Iteration 488, loss = 0.73930196\n",
      "Iteration 489, loss = 0.73875038\n",
      "Iteration 490, loss = 0.73819923\n",
      "Iteration 491, loss = 0.73765007\n",
      "Iteration 492, loss = 0.73710239\n",
      "Iteration 493, loss = 0.73655451\n",
      "Iteration 494, loss = 0.73600875\n",
      "Iteration 495, loss = 0.73546490\n",
      "Iteration 496, loss = 0.73492159\n",
      "Iteration 497, loss = 0.73437898\n",
      "Iteration 498, loss = 0.73383863\n",
      "Iteration 499, loss = 0.73329980\n",
      "Iteration 500, loss = 0.73276031\n",
      "Iteration 501, loss = 0.73222298\n",
      "Iteration 502, loss = 0.73168673\n",
      "Iteration 503, loss = 0.73115134\n",
      "Iteration 504, loss = 0.73061536\n",
      "Iteration 505, loss = 0.73007752\n",
      "Iteration 506, loss = 0.72953937\n",
      "Iteration 507, loss = 0.72900155\n",
      "Iteration 508, loss = 0.72846371\n",
      "Iteration 509, loss = 0.72792579\n",
      "Iteration 510, loss = 0.72738643\n",
      "Iteration 511, loss = 0.72684540\n",
      "Iteration 512, loss = 0.72630467\n",
      "Iteration 513, loss = 0.72576623\n",
      "Iteration 514, loss = 0.72522708\n",
      "Iteration 515, loss = 0.72468776\n",
      "Iteration 516, loss = 0.72414782\n",
      "Iteration 517, loss = 0.72360887\n",
      "Iteration 518, loss = 0.72307233\n",
      "Iteration 519, loss = 0.72253575\n",
      "Iteration 520, loss = 0.72199876\n",
      "Iteration 521, loss = 0.72146286\n",
      "Iteration 522, loss = 0.72092808\n",
      "Iteration 523, loss = 0.72039433\n",
      "Iteration 524, loss = 0.71986191\n",
      "Iteration 525, loss = 0.71933019\n",
      "Iteration 526, loss = 0.71879998\n",
      "Iteration 527, loss = 0.71827192\n",
      "Iteration 528, loss = 0.71774576\n",
      "Iteration 529, loss = 0.71721984\n",
      "Iteration 530, loss = 0.71669574\n",
      "Iteration 531, loss = 0.71617153\n",
      "Iteration 532, loss = 0.71564760\n",
      "Iteration 533, loss = 0.71512364\n",
      "Iteration 534, loss = 0.71459900\n",
      "Iteration 535, loss = 0.71407507\n",
      "Iteration 536, loss = 0.71355184\n",
      "Iteration 537, loss = 0.71302985\n",
      "Iteration 538, loss = 0.71250806\n",
      "Iteration 539, loss = 0.71198618\n",
      "Iteration 540, loss = 0.71146571\n",
      "Iteration 541, loss = 0.71094717\n",
      "Iteration 542, loss = 0.71042892\n",
      "Iteration 543, loss = 0.70991170\n",
      "Iteration 544, loss = 0.70939682\n",
      "Iteration 545, loss = 0.70888284\n",
      "Iteration 546, loss = 0.70837100\n",
      "Iteration 547, loss = 0.70786126\n",
      "Iteration 548, loss = 0.70735185\n",
      "Iteration 549, loss = 0.70684437\n",
      "Iteration 550, loss = 0.70633813\n",
      "Iteration 551, loss = 0.70583246\n",
      "Iteration 552, loss = 0.70532789\n",
      "Iteration 553, loss = 0.70482590\n",
      "Iteration 554, loss = 0.70432454\n",
      "Iteration 555, loss = 0.70382339\n",
      "Iteration 556, loss = 0.70332466\n",
      "Iteration 557, loss = 0.70282739\n",
      "Iteration 558, loss = 0.70233043\n",
      "Iteration 559, loss = 0.70183533\n",
      "Iteration 560, loss = 0.70134242\n",
      "Iteration 561, loss = 0.70084803\n",
      "Iteration 562, loss = 0.70035331\n",
      "Iteration 563, loss = 0.69985763\n",
      "Iteration 564, loss = 0.69936230\n",
      "Iteration 565, loss = 0.69886731\n",
      "Iteration 566, loss = 0.69837182\n",
      "Iteration 567, loss = 0.69787673\n",
      "Iteration 568, loss = 0.69738012\n",
      "Iteration 569, loss = 0.69688412\n",
      "Iteration 570, loss = 0.69638925\n",
      "Iteration 571, loss = 0.69589713\n",
      "Iteration 572, loss = 0.69540471\n",
      "Iteration 573, loss = 0.69491239\n",
      "Iteration 574, loss = 0.69442113\n",
      "Iteration 575, loss = 0.69393060\n",
      "Iteration 576, loss = 0.69344015\n",
      "Iteration 577, loss = 0.69295062\n",
      "Iteration 578, loss = 0.69246101\n",
      "Iteration 579, loss = 0.69197265\n",
      "Iteration 580, loss = 0.69148385\n",
      "Iteration 581, loss = 0.69099496\n",
      "Iteration 582, loss = 0.69050677\n",
      "Iteration 583, loss = 0.69001891\n",
      "Iteration 584, loss = 0.68953103\n",
      "Iteration 585, loss = 0.68904376\n",
      "Iteration 586, loss = 0.68855767\n",
      "Iteration 587, loss = 0.68807256\n",
      "Iteration 588, loss = 0.68758882\n",
      "Iteration 589, loss = 0.68710627\n",
      "Iteration 590, loss = 0.68662648\n",
      "Iteration 591, loss = 0.68614917\n",
      "Iteration 592, loss = 0.68567556\n",
      "Iteration 593, loss = 0.68520266\n",
      "Iteration 594, loss = 0.68473294\n",
      "Iteration 595, loss = 0.68426593\n",
      "Iteration 596, loss = 0.68379976\n",
      "Iteration 597, loss = 0.68333499\n",
      "Iteration 598, loss = 0.68287045\n",
      "Iteration 599, loss = 0.68240774\n",
      "Iteration 600, loss = 0.68195033\n",
      "Iteration 601, loss = 0.68149393\n",
      "Iteration 602, loss = 0.68103864\n",
      "Iteration 603, loss = 0.68058454\n",
      "Iteration 604, loss = 0.68013095\n",
      "Iteration 605, loss = 0.67967769\n",
      "Iteration 606, loss = 0.67922564\n",
      "Iteration 607, loss = 0.67877416\n",
      "Iteration 608, loss = 0.67832326\n",
      "Iteration 609, loss = 0.67787293\n",
      "Iteration 610, loss = 0.67742396\n",
      "Iteration 611, loss = 0.67697582\n",
      "Iteration 612, loss = 0.67652861\n",
      "Iteration 613, loss = 0.67608195\n",
      "Iteration 614, loss = 0.67563639\n",
      "Iteration 615, loss = 0.67519174\n",
      "Iteration 616, loss = 0.67474852\n",
      "Iteration 617, loss = 0.67430600\n",
      "Iteration 618, loss = 0.67386369\n",
      "Iteration 619, loss = 0.67342256\n",
      "Iteration 620, loss = 0.67298055\n",
      "Iteration 621, loss = 0.67253732\n",
      "Iteration 622, loss = 0.67209439\n",
      "Iteration 623, loss = 0.67165061\n",
      "Iteration 624, loss = 0.67120789\n",
      "Iteration 625, loss = 0.67076561\n",
      "Iteration 626, loss = 0.67032322\n",
      "Iteration 627, loss = 0.66988203\n",
      "Iteration 628, loss = 0.66944106\n",
      "Iteration 629, loss = 0.66900054\n",
      "Iteration 630, loss = 0.66856071\n",
      "Iteration 631, loss = 0.66812125\n",
      "Iteration 632, loss = 0.66767910\n",
      "Iteration 633, loss = 0.66723528\n",
      "Iteration 634, loss = 0.66679180\n",
      "Iteration 635, loss = 0.66634815\n",
      "Iteration 636, loss = 0.66590534\n",
      "Iteration 637, loss = 0.66546463\n",
      "Iteration 638, loss = 0.66502383\n",
      "Iteration 639, loss = 0.66458346\n",
      "Iteration 640, loss = 0.66414374\n",
      "Iteration 641, loss = 0.66370248\n",
      "Iteration 642, loss = 0.66326188\n",
      "Iteration 643, loss = 0.66282322\n",
      "Iteration 644, loss = 0.66238489\n",
      "Iteration 645, loss = 0.66194849\n",
      "Iteration 646, loss = 0.66151665\n",
      "Iteration 647, loss = 0.66108575\n",
      "Iteration 648, loss = 0.66065223\n",
      "Iteration 649, loss = 0.66021879\n",
      "Iteration 650, loss = 0.65978629\n",
      "Iteration 651, loss = 0.65935287\n",
      "Iteration 652, loss = 0.65892076\n",
      "Iteration 653, loss = 0.65848768\n",
      "Iteration 654, loss = 0.65805469\n",
      "Iteration 655, loss = 0.65762264\n",
      "Iteration 656, loss = 0.65719230\n",
      "Iteration 657, loss = 0.65676226\n",
      "Iteration 658, loss = 0.65633233\n",
      "Iteration 659, loss = 0.65590359\n",
      "Iteration 660, loss = 0.65547470\n",
      "Iteration 661, loss = 0.65504614\n",
      "Iteration 662, loss = 0.65461812\n",
      "Iteration 663, loss = 0.65419127\n",
      "Iteration 664, loss = 0.65376598\n",
      "Iteration 665, loss = 0.65334119\n",
      "Iteration 666, loss = 0.65291683\n",
      "Iteration 667, loss = 0.65249457\n",
      "Iteration 668, loss = 0.65207336\n",
      "Iteration 669, loss = 0.65165202\n",
      "Iteration 670, loss = 0.65123285\n",
      "Iteration 671, loss = 0.65081509\n",
      "Iteration 672, loss = 0.65039881\n",
      "Iteration 673, loss = 0.64998279\n",
      "Iteration 674, loss = 0.64956716\n",
      "Iteration 675, loss = 0.64915252\n",
      "Iteration 676, loss = 0.64873924\n",
      "Iteration 677, loss = 0.64832901\n",
      "Iteration 678, loss = 0.64792053\n",
      "Iteration 679, loss = 0.64751322\n",
      "Iteration 680, loss = 0.64710679\n",
      "Iteration 681, loss = 0.64670114\n",
      "Iteration 682, loss = 0.64629749\n",
      "Iteration 683, loss = 0.64589426\n",
      "Iteration 684, loss = 0.64549155\n",
      "Iteration 685, loss = 0.64508941\n",
      "Iteration 686, loss = 0.64468824\n",
      "Iteration 687, loss = 0.64428738\n",
      "Iteration 688, loss = 0.64388771\n",
      "Iteration 689, loss = 0.64348903\n",
      "Iteration 690, loss = 0.64309149\n",
      "Iteration 691, loss = 0.64269466\n",
      "Iteration 692, loss = 0.64229873\n",
      "Iteration 693, loss = 0.64190441\n",
      "Iteration 694, loss = 0.64151100\n",
      "Iteration 695, loss = 0.64111781\n",
      "Iteration 696, loss = 0.64072411\n",
      "Iteration 697, loss = 0.64032894\n",
      "Iteration 698, loss = 0.63993375\n",
      "Iteration 699, loss = 0.63953854\n",
      "Iteration 700, loss = 0.63914374\n",
      "Iteration 701, loss = 0.63874951\n",
      "Iteration 702, loss = 0.63835600\n",
      "Iteration 703, loss = 0.63796262\n",
      "Iteration 704, loss = 0.63756953\n",
      "Iteration 705, loss = 0.63717810\n",
      "Iteration 706, loss = 0.63678718\n",
      "Iteration 707, loss = 0.63639735\n",
      "Iteration 708, loss = 0.63600808\n",
      "Iteration 709, loss = 0.63561944\n",
      "Iteration 710, loss = 0.63523139\n",
      "Iteration 711, loss = 0.63484374\n",
      "Iteration 712, loss = 0.63445647\n",
      "Iteration 713, loss = 0.63407066\n",
      "Iteration 714, loss = 0.63368559\n",
      "Iteration 715, loss = 0.63330159\n",
      "Iteration 716, loss = 0.63291873\n",
      "Iteration 717, loss = 0.63253665\n",
      "Iteration 718, loss = 0.63215510\n",
      "Iteration 719, loss = 0.63177419\n",
      "Iteration 720, loss = 0.63139425\n",
      "Iteration 721, loss = 0.63101512\n",
      "Iteration 722, loss = 0.63063653\n",
      "Iteration 723, loss = 0.63025793\n",
      "Iteration 724, loss = 0.62988069\n",
      "Iteration 725, loss = 0.62950445\n",
      "Iteration 726, loss = 0.62912910\n",
      "Iteration 727, loss = 0.62875429\n",
      "Iteration 728, loss = 0.62837995\n",
      "Iteration 729, loss = 0.62800690\n",
      "Iteration 730, loss = 0.62763436\n",
      "Iteration 731, loss = 0.62726087\n",
      "Iteration 732, loss = 0.62688739\n",
      "Iteration 733, loss = 0.62651520\n",
      "Iteration 734, loss = 0.62614300\n",
      "Iteration 735, loss = 0.62577115\n",
      "Iteration 736, loss = 0.62540044\n",
      "Iteration 737, loss = 0.62502975\n",
      "Iteration 738, loss = 0.62465977\n",
      "Iteration 739, loss = 0.62429118\n",
      "Iteration 740, loss = 0.62392288\n",
      "Iteration 741, loss = 0.62355516\n",
      "Iteration 742, loss = 0.62318897\n",
      "Iteration 743, loss = 0.62282305\n",
      "Iteration 744, loss = 0.62245896\n",
      "Iteration 745, loss = 0.62209555\n",
      "Iteration 746, loss = 0.62173348\n",
      "Iteration 747, loss = 0.62137239\n",
      "Iteration 748, loss = 0.62101179\n",
      "Iteration 749, loss = 0.62065080\n",
      "Iteration 750, loss = 0.62028983\n",
      "Iteration 751, loss = 0.61992941\n",
      "Iteration 752, loss = 0.61956994\n",
      "Iteration 753, loss = 0.61921087\n",
      "Iteration 754, loss = 0.61885277\n",
      "Iteration 755, loss = 0.61849527\n",
      "Iteration 756, loss = 0.61813845\n",
      "Iteration 757, loss = 0.61778236\n",
      "Iteration 758, loss = 0.61742732\n",
      "Iteration 759, loss = 0.61707281\n",
      "Iteration 760, loss = 0.61671943\n",
      "Iteration 761, loss = 0.61636731\n",
      "Iteration 762, loss = 0.61601503\n",
      "Iteration 763, loss = 0.61566294\n",
      "Iteration 764, loss = 0.61531105\n",
      "Iteration 765, loss = 0.61495950\n",
      "Iteration 766, loss = 0.61460906\n",
      "Iteration 767, loss = 0.61425905\n",
      "Iteration 768, loss = 0.61390992\n",
      "Iteration 769, loss = 0.61356111\n",
      "Iteration 770, loss = 0.61321266\n",
      "Iteration 771, loss = 0.61286466\n",
      "Iteration 772, loss = 0.61251759\n",
      "Iteration 773, loss = 0.61217091\n",
      "Iteration 774, loss = 0.61182534\n",
      "Iteration 775, loss = 0.61148047\n",
      "Iteration 776, loss = 0.61113583\n",
      "Iteration 777, loss = 0.61079175\n",
      "Iteration 778, loss = 0.61044854\n",
      "Iteration 779, loss = 0.61010733\n",
      "Iteration 780, loss = 0.60976677\n",
      "Iteration 781, loss = 0.60942686\n",
      "Iteration 782, loss = 0.60908784\n",
      "Iteration 783, loss = 0.60874922\n",
      "Iteration 784, loss = 0.60841120\n",
      "Iteration 785, loss = 0.60807422\n",
      "Iteration 786, loss = 0.60773785\n",
      "Iteration 787, loss = 0.60740189\n",
      "Iteration 788, loss = 0.60706661\n",
      "Iteration 789, loss = 0.60673210\n",
      "Iteration 790, loss = 0.60639776\n",
      "Iteration 791, loss = 0.60606419\n",
      "Iteration 792, loss = 0.60573165\n",
      "Iteration 793, loss = 0.60539954\n",
      "Iteration 794, loss = 0.60506780\n",
      "Iteration 795, loss = 0.60473669\n",
      "Iteration 796, loss = 0.60440662\n",
      "Iteration 797, loss = 0.60407695\n",
      "Iteration 798, loss = 0.60374774\n",
      "Iteration 799, loss = 0.60341952\n",
      "Iteration 800, loss = 0.60309227\n",
      "Iteration 801, loss = 0.60276549\n",
      "Iteration 802, loss = 0.60243979\n",
      "Iteration 803, loss = 0.60211457\n",
      "Iteration 804, loss = 0.60178990\n",
      "Iteration 805, loss = 0.60146630\n",
      "Iteration 806, loss = 0.60114357\n",
      "Iteration 807, loss = 0.60082112\n",
      "Iteration 808, loss = 0.60049824\n",
      "Iteration 809, loss = 0.60017549\n",
      "Iteration 810, loss = 0.59985376\n",
      "Iteration 811, loss = 0.59953276\n",
      "Iteration 812, loss = 0.59921197\n",
      "Iteration 813, loss = 0.59889114\n",
      "Iteration 814, loss = 0.59857077\n",
      "Iteration 815, loss = 0.59825164\n",
      "Iteration 816, loss = 0.59793337\n",
      "Iteration 817, loss = 0.59761530\n",
      "Iteration 818, loss = 0.59729758\n",
      "Iteration 819, loss = 0.59698035\n",
      "Iteration 820, loss = 0.59666377\n",
      "Iteration 821, loss = 0.59634748\n",
      "Iteration 822, loss = 0.59603186\n",
      "Iteration 823, loss = 0.59571675\n",
      "Iteration 824, loss = 0.59540074\n",
      "Iteration 825, loss = 0.59508511\n",
      "Iteration 826, loss = 0.59476992\n",
      "Iteration 827, loss = 0.59445481\n",
      "Iteration 828, loss = 0.59413868\n",
      "Iteration 829, loss = 0.59382348\n",
      "Iteration 830, loss = 0.59350868\n",
      "Iteration 831, loss = 0.59319416\n",
      "Iteration 832, loss = 0.59288045\n",
      "Iteration 833, loss = 0.59256705\n",
      "Iteration 834, loss = 0.59225303\n",
      "Iteration 835, loss = 0.59193706\n",
      "Iteration 836, loss = 0.59162101\n",
      "Iteration 837, loss = 0.59130523\n",
      "Iteration 838, loss = 0.59098945\n",
      "Iteration 839, loss = 0.59067383\n",
      "Iteration 840, loss = 0.59035857\n",
      "Iteration 841, loss = 0.59004367\n",
      "Iteration 842, loss = 0.58972919\n",
      "Iteration 843, loss = 0.58941466\n",
      "Iteration 844, loss = 0.58910038\n",
      "Iteration 845, loss = 0.58878662\n",
      "Iteration 846, loss = 0.58847310\n",
      "Iteration 847, loss = 0.58816023\n",
      "Iteration 848, loss = 0.58784803\n",
      "Iteration 849, loss = 0.58753581\n",
      "Iteration 850, loss = 0.58722398\n",
      "Iteration 851, loss = 0.58691229\n",
      "Iteration 852, loss = 0.58660093\n",
      "Iteration 853, loss = 0.58628989\n",
      "Iteration 854, loss = 0.58597942\n",
      "Iteration 855, loss = 0.58566965\n",
      "Iteration 856, loss = 0.58536035\n",
      "Iteration 857, loss = 0.58505107\n",
      "Iteration 858, loss = 0.58474237\n",
      "Iteration 859, loss = 0.58443453\n",
      "Iteration 860, loss = 0.58412734\n",
      "Iteration 861, loss = 0.58382125\n",
      "Iteration 862, loss = 0.58351645\n",
      "Iteration 863, loss = 0.58321197\n",
      "Iteration 864, loss = 0.58290832\n",
      "Iteration 865, loss = 0.58260528\n",
      "Iteration 866, loss = 0.58230283\n",
      "Iteration 867, loss = 0.58200068\n",
      "Iteration 868, loss = 0.58169858\n",
      "Iteration 869, loss = 0.58139553\n",
      "Iteration 870, loss = 0.58109290\n",
      "Iteration 871, loss = 0.58079118\n",
      "Iteration 872, loss = 0.58048931\n",
      "Iteration 873, loss = 0.58018795\n",
      "Iteration 874, loss = 0.57988673\n",
      "Iteration 875, loss = 0.57958586\n",
      "Iteration 876, loss = 0.57928544\n",
      "Iteration 877, loss = 0.57898532\n",
      "Iteration 878, loss = 0.57868550\n",
      "Iteration 879, loss = 0.57838635\n",
      "Iteration 880, loss = 0.57808778\n",
      "Iteration 881, loss = 0.57778962\n",
      "Iteration 882, loss = 0.57749187\n",
      "Iteration 883, loss = 0.57719466\n",
      "Iteration 884, loss = 0.57689787\n",
      "Iteration 885, loss = 0.57660192\n",
      "Iteration 886, loss = 0.57630656\n",
      "Iteration 887, loss = 0.57601177\n",
      "Iteration 888, loss = 0.57571610\n",
      "Iteration 889, loss = 0.57542055\n",
      "Iteration 890, loss = 0.57512595\n",
      "Iteration 891, loss = 0.57483236\n",
      "Iteration 892, loss = 0.57453974\n",
      "Iteration 893, loss = 0.57424821\n",
      "Iteration 894, loss = 0.57395745\n",
      "Iteration 895, loss = 0.57366757\n",
      "Iteration 896, loss = 0.57337801\n",
      "Iteration 897, loss = 0.57308754\n",
      "Iteration 898, loss = 0.57279875\n",
      "Iteration 899, loss = 0.57251060\n",
      "Iteration 900, loss = 0.57222295\n",
      "Iteration 901, loss = 0.57193566\n",
      "Iteration 902, loss = 0.57164839\n",
      "Iteration 903, loss = 0.57136194\n",
      "Iteration 904, loss = 0.57107588\n",
      "Iteration 905, loss = 0.57079031\n",
      "Iteration 906, loss = 0.57050497\n",
      "Iteration 907, loss = 0.57022011\n",
      "Iteration 908, loss = 0.56993612\n",
      "Iteration 909, loss = 0.56965282\n",
      "Iteration 910, loss = 0.56936904\n",
      "Iteration 911, loss = 0.56908558\n",
      "Iteration 912, loss = 0.56880276\n",
      "Iteration 913, loss = 0.56852048\n",
      "Iteration 914, loss = 0.56823835\n",
      "Iteration 915, loss = 0.56795646\n",
      "Iteration 916, loss = 0.56767493\n",
      "Iteration 917, loss = 0.56739379\n",
      "Iteration 918, loss = 0.56711300\n",
      "Iteration 919, loss = 0.56683244\n",
      "Iteration 920, loss = 0.56655220\n",
      "Iteration 921, loss = 0.56627175\n",
      "Iteration 922, loss = 0.56599111\n",
      "Iteration 923, loss = 0.56571130\n",
      "Iteration 924, loss = 0.56543271\n",
      "Iteration 925, loss = 0.56515526\n",
      "Iteration 926, loss = 0.56487879\n",
      "Iteration 927, loss = 0.56460266\n",
      "Iteration 928, loss = 0.56432729\n",
      "Iteration 929, loss = 0.56405283\n",
      "Iteration 930, loss = 0.56377948\n",
      "Iteration 931, loss = 0.56350657\n",
      "Iteration 932, loss = 0.56323402\n",
      "Iteration 933, loss = 0.56296203\n",
      "Iteration 934, loss = 0.56269018\n",
      "Iteration 935, loss = 0.56241924\n",
      "Iteration 936, loss = 0.56214762\n",
      "Iteration 937, loss = 0.56187659\n",
      "Iteration 938, loss = 0.56160588\n",
      "Iteration 939, loss = 0.56133533\n",
      "Iteration 940, loss = 0.56106513\n",
      "Iteration 941, loss = 0.56079488\n",
      "Iteration 942, loss = 0.56052526\n",
      "Iteration 943, loss = 0.56025700\n",
      "Iteration 944, loss = 0.55998872\n",
      "Iteration 945, loss = 0.55972098\n",
      "Iteration 946, loss = 0.55945358\n",
      "Iteration 947, loss = 0.55918603\n",
      "Iteration 948, loss = 0.55891822\n",
      "Iteration 949, loss = 0.55865111\n",
      "Iteration 950, loss = 0.55838480\n",
      "Iteration 951, loss = 0.55811894\n",
      "Iteration 952, loss = 0.55785283\n",
      "Iteration 953, loss = 0.55758737\n",
      "Iteration 954, loss = 0.55732217\n",
      "Iteration 955, loss = 0.55705775\n",
      "Iteration 956, loss = 0.55679338\n",
      "Iteration 957, loss = 0.55652939\n",
      "Iteration 958, loss = 0.55626656\n",
      "Iteration 959, loss = 0.55600389\n",
      "Iteration 960, loss = 0.55574163\n",
      "Iteration 961, loss = 0.55547980\n",
      "Iteration 962, loss = 0.55521823\n",
      "Iteration 963, loss = 0.55495705\n",
      "Iteration 964, loss = 0.55469638\n",
      "Iteration 965, loss = 0.55443535\n",
      "Iteration 966, loss = 0.55417441\n",
      "Iteration 967, loss = 0.55391382\n",
      "Iteration 968, loss = 0.55365415\n",
      "Iteration 969, loss = 0.55339491\n",
      "Iteration 970, loss = 0.55313595\n",
      "Iteration 971, loss = 0.55287726\n",
      "Iteration 972, loss = 0.55261883\n",
      "Iteration 973, loss = 0.55236089\n",
      "Iteration 974, loss = 0.55210328\n",
      "Iteration 975, loss = 0.55184613\n",
      "Iteration 976, loss = 0.55158926\n",
      "Iteration 977, loss = 0.55133291\n",
      "Iteration 978, loss = 0.55107704\n",
      "Iteration 979, loss = 0.55082156\n",
      "Iteration 980, loss = 0.55056638\n",
      "Iteration 981, loss = 0.55031157\n",
      "Iteration 982, loss = 0.55005701\n",
      "Iteration 983, loss = 0.54980253\n",
      "Iteration 984, loss = 0.54954851\n",
      "Iteration 985, loss = 0.54929476\n",
      "Iteration 986, loss = 0.54903985\n",
      "Iteration 987, loss = 0.54878495\n",
      "Iteration 988, loss = 0.54853018\n",
      "Iteration 989, loss = 0.54827563\n",
      "Iteration 990, loss = 0.54802126\n",
      "Iteration 991, loss = 0.54776696\n",
      "Iteration 992, loss = 0.54751285\n",
      "Iteration 993, loss = 0.54725913\n",
      "Iteration 994, loss = 0.54700555\n",
      "Iteration 995, loss = 0.54675219\n",
      "Iteration 996, loss = 0.54649898\n",
      "Iteration 997, loss = 0.54624611\n",
      "Iteration 998, loss = 0.54599403\n",
      "Iteration 999, loss = 0.54574288\n",
      "Iteration 1000, loss = 0.54549251\n",
      "Iteration 1001, loss = 0.54524240\n",
      "Iteration 1002, loss = 0.54499243\n",
      "Iteration 1003, loss = 0.54474242\n",
      "Iteration 1004, loss = 0.54449276\n",
      "Iteration 1005, loss = 0.54424397\n",
      "Iteration 1006, loss = 0.54399581\n",
      "Iteration 1007, loss = 0.54374795\n",
      "Iteration 1008, loss = 0.54350160\n",
      "Iteration 1009, loss = 0.54325533\n",
      "Iteration 1010, loss = 0.54300956\n",
      "Iteration 1011, loss = 0.54276543\n",
      "Iteration 1012, loss = 0.54252341\n",
      "Iteration 1013, loss = 0.54228121\n",
      "Iteration 1014, loss = 0.54203921\n",
      "Iteration 1015, loss = 0.54179748\n",
      "Iteration 1016, loss = 0.54155561\n",
      "Iteration 1017, loss = 0.54131368\n",
      "Iteration 1018, loss = 0.54107078\n",
      "Iteration 1019, loss = 0.54082725\n",
      "Iteration 1020, loss = 0.54058416\n",
      "Iteration 1021, loss = 0.54034173\n",
      "Iteration 1022, loss = 0.54009921\n",
      "Iteration 1023, loss = 0.53985737\n",
      "Iteration 1024, loss = 0.53961573\n",
      "Iteration 1025, loss = 0.53937395\n",
      "Iteration 1026, loss = 0.53913231\n",
      "Iteration 1027, loss = 0.53889057\n",
      "Iteration 1028, loss = 0.53864906\n",
      "Iteration 1029, loss = 0.53840761\n",
      "Iteration 1030, loss = 0.53816571\n",
      "Iteration 1031, loss = 0.53792371\n",
      "Iteration 1032, loss = 0.53768155\n",
      "Iteration 1033, loss = 0.53743913\n",
      "Iteration 1034, loss = 0.53719658\n",
      "Iteration 1035, loss = 0.53695444\n",
      "Iteration 1036, loss = 0.53671229\n",
      "Iteration 1037, loss = 0.53646994\n",
      "Iteration 1038, loss = 0.53622765\n",
      "Iteration 1039, loss = 0.53598557\n",
      "Iteration 1040, loss = 0.53574352\n",
      "Iteration 1041, loss = 0.53550160\n",
      "Iteration 1042, loss = 0.53525966\n",
      "Iteration 1043, loss = 0.53501782\n",
      "Iteration 1044, loss = 0.53477634\n",
      "Iteration 1045, loss = 0.53453510\n",
      "Iteration 1046, loss = 0.53429427\n",
      "Iteration 1047, loss = 0.53405335\n",
      "Iteration 1048, loss = 0.53381263\n",
      "Iteration 1049, loss = 0.53357195\n",
      "Iteration 1050, loss = 0.53333162\n",
      "Iteration 1051, loss = 0.53309089\n",
      "Iteration 1052, loss = 0.53285051\n",
      "Iteration 1053, loss = 0.53261139\n",
      "Iteration 1054, loss = 0.53237267\n",
      "Iteration 1055, loss = 0.53213421\n",
      "Iteration 1056, loss = 0.53189593\n",
      "Iteration 1057, loss = 0.53165856\n",
      "Iteration 1058, loss = 0.53142089\n",
      "Iteration 1059, loss = 0.53118265\n",
      "Iteration 1060, loss = 0.53094439\n",
      "Iteration 1061, loss = 0.53070653\n",
      "Iteration 1062, loss = 0.53046888\n",
      "Iteration 1063, loss = 0.53023121\n",
      "Iteration 1064, loss = 0.52999370\n",
      "Iteration 1065, loss = 0.52975638\n",
      "Iteration 1066, loss = 0.52951902\n",
      "Iteration 1067, loss = 0.52928222\n",
      "Iteration 1068, loss = 0.52904683\n",
      "Iteration 1069, loss = 0.52880994\n",
      "Iteration 1070, loss = 0.52857341\n",
      "Iteration 1071, loss = 0.52833731\n",
      "Iteration 1072, loss = 0.52810168\n",
      "Iteration 1073, loss = 0.52786608\n",
      "Iteration 1074, loss = 0.52763071\n",
      "Iteration 1075, loss = 0.52739541\n",
      "Iteration 1076, loss = 0.52716029\n",
      "Iteration 1077, loss = 0.52692525\n",
      "Iteration 1078, loss = 0.52669052\n",
      "Iteration 1079, loss = 0.52645590\n",
      "Iteration 1080, loss = 0.52622145\n",
      "Iteration 1081, loss = 0.52598649\n",
      "Iteration 1082, loss = 0.52575186\n",
      "Iteration 1083, loss = 0.52551715\n",
      "Iteration 1084, loss = 0.52528250\n",
      "Iteration 1085, loss = 0.52505015\n",
      "Iteration 1086, loss = 0.52481821\n",
      "Iteration 1087, loss = 0.52458643\n",
      "Iteration 1088, loss = 0.52435527\n",
      "Iteration 1089, loss = 0.52412387\n",
      "Iteration 1090, loss = 0.52389252\n",
      "Iteration 1091, loss = 0.52366121\n",
      "Iteration 1092, loss = 0.52343033\n",
      "Iteration 1093, loss = 0.52319964\n",
      "Iteration 1094, loss = 0.52296876\n",
      "Iteration 1095, loss = 0.52273800\n",
      "Iteration 1096, loss = 0.52250721\n",
      "Iteration 1097, loss = 0.52227653\n",
      "Iteration 1098, loss = 0.52204610\n",
      "Iteration 1099, loss = 0.52181635\n",
      "Iteration 1100, loss = 0.52158672\n",
      "Iteration 1101, loss = 0.52135760\n",
      "Iteration 1102, loss = 0.52112950\n",
      "Iteration 1103, loss = 0.52090187\n",
      "Iteration 1104, loss = 0.52067447\n",
      "Iteration 1105, loss = 0.52044718\n",
      "Iteration 1106, loss = 0.52022017\n",
      "Iteration 1107, loss = 0.51999364\n",
      "Iteration 1108, loss = 0.51976673\n",
      "Iteration 1109, loss = 0.51953956\n",
      "Iteration 1110, loss = 0.51931241\n",
      "Iteration 1111, loss = 0.51908528\n",
      "Iteration 1112, loss = 0.51885864\n",
      "Iteration 1113, loss = 0.51863207\n",
      "Iteration 1114, loss = 0.51840539\n",
      "Iteration 1115, loss = 0.51817902\n",
      "Iteration 1116, loss = 0.51795308\n",
      "Iteration 1117, loss = 0.51772773\n",
      "Iteration 1118, loss = 0.51750252\n",
      "Iteration 1119, loss = 0.51727755\n",
      "Iteration 1120, loss = 0.51705279\n",
      "Iteration 1121, loss = 0.51682840\n",
      "Iteration 1122, loss = 0.51660417\n",
      "Iteration 1123, loss = 0.51638015\n",
      "Iteration 1124, loss = 0.51615680\n",
      "Iteration 1125, loss = 0.51593370\n",
      "Iteration 1126, loss = 0.51571084\n",
      "Iteration 1127, loss = 0.51548830\n",
      "Iteration 1128, loss = 0.51526613\n",
      "Iteration 1129, loss = 0.51504406\n",
      "Iteration 1130, loss = 0.51482216\n",
      "Iteration 1131, loss = 0.51460030\n",
      "Iteration 1132, loss = 0.51437905\n",
      "Iteration 1133, loss = 0.51415804\n",
      "Iteration 1134, loss = 0.51393707\n",
      "Iteration 1135, loss = 0.51371639\n",
      "Iteration 1136, loss = 0.51349573\n",
      "Iteration 1137, loss = 0.51327527\n",
      "Iteration 1138, loss = 0.51305509\n",
      "Iteration 1139, loss = 0.51283506\n",
      "Iteration 1140, loss = 0.51261536\n",
      "Iteration 1141, loss = 0.51239573\n",
      "Iteration 1142, loss = 0.51217611\n",
      "Iteration 1143, loss = 0.51195679\n",
      "Iteration 1144, loss = 0.51173843\n",
      "Iteration 1145, loss = 0.51152081\n",
      "Iteration 1146, loss = 0.51130330\n",
      "Iteration 1147, loss = 0.51108601\n",
      "Iteration 1148, loss = 0.51086906\n",
      "Iteration 1149, loss = 0.51065352\n",
      "Iteration 1150, loss = 0.51043911\n",
      "Iteration 1151, loss = 0.51022506\n",
      "Iteration 1152, loss = 0.51001114\n",
      "Iteration 1153, loss = 0.50979752\n",
      "Iteration 1154, loss = 0.50958424\n",
      "Iteration 1155, loss = 0.50937151\n",
      "Iteration 1156, loss = 0.50915884\n",
      "Iteration 1157, loss = 0.50894629\n",
      "Iteration 1158, loss = 0.50873439\n",
      "Iteration 1159, loss = 0.50852279\n",
      "Iteration 1160, loss = 0.50831141\n",
      "Iteration 1161, loss = 0.50810031\n",
      "Iteration 1162, loss = 0.50788946\n",
      "Iteration 1163, loss = 0.50767889\n",
      "Iteration 1164, loss = 0.50746870\n",
      "Iteration 1165, loss = 0.50725882\n",
      "Iteration 1166, loss = 0.50704912\n",
      "Iteration 1167, loss = 0.50683948\n",
      "Iteration 1168, loss = 0.50663016\n",
      "Iteration 1169, loss = 0.50642224\n",
      "Iteration 1170, loss = 0.50621511\n",
      "Iteration 1171, loss = 0.50600792\n",
      "Iteration 1172, loss = 0.50580109\n",
      "Iteration 1173, loss = 0.50559481\n",
      "Iteration 1174, loss = 0.50538857\n",
      "Iteration 1175, loss = 0.50518265\n",
      "Iteration 1176, loss = 0.50497692\n",
      "Iteration 1177, loss = 0.50477143\n",
      "Iteration 1178, loss = 0.50456610\n",
      "Iteration 1179, loss = 0.50436108\n",
      "Iteration 1180, loss = 0.50415666\n",
      "Iteration 1181, loss = 0.50395226\n",
      "Iteration 1182, loss = 0.50374815\n",
      "Iteration 1183, loss = 0.50354425\n",
      "Iteration 1184, loss = 0.50334065\n",
      "Iteration 1185, loss = 0.50313719\n",
      "Iteration 1186, loss = 0.50293373\n",
      "Iteration 1187, loss = 0.50273052\n",
      "Iteration 1188, loss = 0.50252757\n",
      "Iteration 1189, loss = 0.50232480\n",
      "Iteration 1190, loss = 0.50212226\n",
      "Iteration 1191, loss = 0.50191984\n",
      "Iteration 1192, loss = 0.50171776\n",
      "Iteration 1193, loss = 0.50151582\n",
      "Iteration 1194, loss = 0.50131408\n",
      "Iteration 1195, loss = 0.50111262\n",
      "Iteration 1196, loss = 0.50091148\n",
      "Iteration 1197, loss = 0.50071038\n",
      "Iteration 1198, loss = 0.50050950\n",
      "Iteration 1199, loss = 0.50030879\n",
      "Iteration 1200, loss = 0.50010826\n",
      "Iteration 1201, loss = 0.49990812\n",
      "Iteration 1202, loss = 0.49970821\n",
      "Iteration 1203, loss = 0.49950850\n",
      "Iteration 1204, loss = 0.49930891\n",
      "Iteration 1205, loss = 0.49910951\n",
      "Iteration 1206, loss = 0.49891053\n",
      "Iteration 1207, loss = 0.49871175\n",
      "Iteration 1208, loss = 0.49851335\n",
      "Iteration 1209, loss = 0.49831521\n",
      "Iteration 1210, loss = 0.49811744\n",
      "Iteration 1211, loss = 0.49791995\n",
      "Iteration 1212, loss = 0.49772244\n",
      "Iteration 1213, loss = 0.49752529\n",
      "Iteration 1214, loss = 0.49732850\n",
      "Iteration 1215, loss = 0.49713184\n",
      "Iteration 1216, loss = 0.49693532\n",
      "Iteration 1217, loss = 0.49673898\n",
      "Iteration 1218, loss = 0.49654307\n",
      "Iteration 1219, loss = 0.49634727\n",
      "Iteration 1220, loss = 0.49615152\n",
      "Iteration 1221, loss = 0.49595628\n",
      "Iteration 1222, loss = 0.49576110\n",
      "Iteration 1223, loss = 0.49556608\n",
      "Iteration 1224, loss = 0.49537138\n",
      "Iteration 1225, loss = 0.49517702\n",
      "Iteration 1226, loss = 0.49498300\n",
      "Iteration 1227, loss = 0.49478915\n",
      "Iteration 1228, loss = 0.49459549\n",
      "Iteration 1229, loss = 0.49440199\n",
      "Iteration 1230, loss = 0.49420875\n",
      "Iteration 1231, loss = 0.49401547\n",
      "Iteration 1232, loss = 0.49382224\n",
      "Iteration 1233, loss = 0.49362933\n",
      "Iteration 1234, loss = 0.49343652\n",
      "Iteration 1235, loss = 0.49324371\n",
      "Iteration 1236, loss = 0.49305102\n",
      "Iteration 1237, loss = 0.49285852\n",
      "Iteration 1238, loss = 0.49266621\n",
      "Iteration 1239, loss = 0.49247408\n",
      "Iteration 1240, loss = 0.49228218\n",
      "Iteration 1241, loss = 0.49209048\n",
      "Iteration 1242, loss = 0.49189883\n",
      "Iteration 1243, loss = 0.49170720\n",
      "Iteration 1244, loss = 0.49151617\n",
      "Iteration 1245, loss = 0.49132527\n",
      "Iteration 1246, loss = 0.49113456\n",
      "Iteration 1247, loss = 0.49094392\n",
      "Iteration 1248, loss = 0.49075341\n",
      "Iteration 1249, loss = 0.49056316\n",
      "Iteration 1250, loss = 0.49037331\n",
      "Iteration 1251, loss = 0.49018344\n",
      "Iteration 1252, loss = 0.48999373\n",
      "Iteration 1253, loss = 0.48980423\n",
      "Iteration 1254, loss = 0.48961488\n",
      "Iteration 1255, loss = 0.48942583\n",
      "Iteration 1256, loss = 0.48923676\n",
      "Iteration 1257, loss = 0.48904794\n",
      "Iteration 1258, loss = 0.48885941\n",
      "Iteration 1259, loss = 0.48867136\n",
      "Iteration 1260, loss = 0.48848345\n",
      "Iteration 1261, loss = 0.48829565\n",
      "Iteration 1262, loss = 0.48810788\n",
      "Iteration 1263, loss = 0.48792043\n",
      "Iteration 1264, loss = 0.48773313\n",
      "Iteration 1265, loss = 0.48754592\n",
      "Iteration 1266, loss = 0.48735876\n",
      "Iteration 1267, loss = 0.48717193\n",
      "Iteration 1268, loss = 0.48698513\n",
      "Iteration 1269, loss = 0.48679853\n",
      "Iteration 1270, loss = 0.48661214\n",
      "Iteration 1271, loss = 0.48642609\n",
      "Iteration 1272, loss = 0.48624008\n",
      "Iteration 1273, loss = 0.48605423\n",
      "Iteration 1274, loss = 0.48586850\n",
      "Iteration 1275, loss = 0.48568292\n",
      "Iteration 1276, loss = 0.48549762\n",
      "Iteration 1277, loss = 0.48531287\n",
      "Iteration 1278, loss = 0.48512839\n",
      "Iteration 1279, loss = 0.48494414\n",
      "Iteration 1280, loss = 0.48476001\n",
      "Iteration 1281, loss = 0.48457598\n",
      "Iteration 1282, loss = 0.48439215\n",
      "Iteration 1283, loss = 0.48420851\n",
      "Iteration 1284, loss = 0.48402500\n",
      "Iteration 1285, loss = 0.48384163\n",
      "Iteration 1286, loss = 0.48365858\n",
      "Iteration 1287, loss = 0.48347559\n",
      "Iteration 1288, loss = 0.48329290\n",
      "Iteration 1289, loss = 0.48311032\n",
      "Iteration 1290, loss = 0.48292793\n",
      "Iteration 1291, loss = 0.48274579\n",
      "Iteration 1292, loss = 0.48256393\n",
      "Iteration 1293, loss = 0.48238220\n",
      "Iteration 1294, loss = 0.48220045\n",
      "Iteration 1295, loss = 0.48201906\n",
      "Iteration 1296, loss = 0.48183783\n",
      "Iteration 1297, loss = 0.48165690\n",
      "Iteration 1298, loss = 0.48147617\n",
      "Iteration 1299, loss = 0.48129596\n",
      "Iteration 1300, loss = 0.48111573\n",
      "Iteration 1301, loss = 0.48093563\n",
      "Iteration 1302, loss = 0.48075576\n",
      "Iteration 1303, loss = 0.48057608\n",
      "Iteration 1304, loss = 0.48039650\n",
      "Iteration 1305, loss = 0.48021720\n",
      "Iteration 1306, loss = 0.48003795\n",
      "Iteration 1307, loss = 0.47985893\n",
      "Iteration 1308, loss = 0.47968026\n",
      "Iteration 1309, loss = 0.47950173\n",
      "Iteration 1310, loss = 0.47932338\n",
      "Iteration 1311, loss = 0.47914531\n",
      "Iteration 1312, loss = 0.47896786\n",
      "Iteration 1313, loss = 0.47879057\n",
      "Iteration 1314, loss = 0.47861340\n",
      "Iteration 1315, loss = 0.47843655\n",
      "Iteration 1316, loss = 0.47825980\n",
      "Iteration 1317, loss = 0.47808308\n",
      "Iteration 1318, loss = 0.47790637\n",
      "Iteration 1319, loss = 0.47772997\n",
      "Iteration 1320, loss = 0.47755381\n",
      "Iteration 1321, loss = 0.47737771\n",
      "Iteration 1322, loss = 0.47720174\n",
      "Iteration 1323, loss = 0.47702567\n",
      "Iteration 1324, loss = 0.47684898\n",
      "Iteration 1325, loss = 0.47667227\n",
      "Iteration 1326, loss = 0.47649569\n",
      "Iteration 1327, loss = 0.47631923\n",
      "Iteration 1328, loss = 0.47614272\n",
      "Iteration 1329, loss = 0.47596648\n",
      "Iteration 1330, loss = 0.47579019\n",
      "Iteration 1331, loss = 0.47561418\n",
      "Iteration 1332, loss = 0.47543824\n",
      "Iteration 1333, loss = 0.47526248\n",
      "Iteration 1334, loss = 0.47508672\n",
      "Iteration 1335, loss = 0.47491103\n",
      "Iteration 1336, loss = 0.47473540\n",
      "Iteration 1337, loss = 0.47455994\n",
      "Iteration 1338, loss = 0.47438465\n",
      "Iteration 1339, loss = 0.47420952\n",
      "Iteration 1340, loss = 0.47403458\n",
      "Iteration 1341, loss = 0.47385990\n",
      "Iteration 1342, loss = 0.47368527\n",
      "Iteration 1343, loss = 0.47351089\n",
      "Iteration 1344, loss = 0.47333665\n",
      "Iteration 1345, loss = 0.47316235\n",
      "Iteration 1346, loss = 0.47298775\n",
      "Iteration 1347, loss = 0.47281317\n",
      "Iteration 1348, loss = 0.47263863\n",
      "Iteration 1349, loss = 0.47246417\n",
      "Iteration 1350, loss = 0.47228970\n",
      "Iteration 1351, loss = 0.47211539\n",
      "Iteration 1352, loss = 0.47194114\n",
      "Iteration 1353, loss = 0.47176678\n",
      "Iteration 1354, loss = 0.47159259\n",
      "Iteration 1355, loss = 0.47141850\n",
      "Iteration 1356, loss = 0.47124462\n",
      "Iteration 1357, loss = 0.47107074\n",
      "Iteration 1358, loss = 0.47089704\n",
      "Iteration 1359, loss = 0.47072345\n",
      "Iteration 1360, loss = 0.47055009\n",
      "Iteration 1361, loss = 0.47037694\n",
      "Iteration 1362, loss = 0.47020384\n",
      "Iteration 1363, loss = 0.47003080\n",
      "Iteration 1364, loss = 0.46985801\n",
      "Iteration 1365, loss = 0.46968535\n",
      "Iteration 1366, loss = 0.46951293\n",
      "Iteration 1367, loss = 0.46934055\n",
      "Iteration 1368, loss = 0.46916843\n",
      "Iteration 1369, loss = 0.46899654\n",
      "Iteration 1370, loss = 0.46882463\n",
      "Iteration 1371, loss = 0.46865310\n",
      "Iteration 1372, loss = 0.46848167\n",
      "Iteration 1373, loss = 0.46831025\n",
      "Iteration 1374, loss = 0.46813921\n",
      "Iteration 1375, loss = 0.46796831\n",
      "Iteration 1376, loss = 0.46779758\n",
      "Iteration 1377, loss = 0.46762710\n",
      "Iteration 1378, loss = 0.46745701\n",
      "Iteration 1379, loss = 0.46728665\n",
      "Iteration 1380, loss = 0.46711563\n",
      "Iteration 1381, loss = 0.46694461\n",
      "Iteration 1382, loss = 0.46677358\n",
      "Iteration 1383, loss = 0.46660264\n",
      "Iteration 1384, loss = 0.46643186\n",
      "Iteration 1385, loss = 0.46626110\n",
      "Iteration 1386, loss = 0.46609132\n",
      "Iteration 1387, loss = 0.46592295\n",
      "Iteration 1388, loss = 0.46575484\n",
      "Iteration 1389, loss = 0.46558693\n",
      "Iteration 1390, loss = 0.46541903\n",
      "Iteration 1391, loss = 0.46525142\n",
      "Iteration 1392, loss = 0.46508386\n",
      "Iteration 1393, loss = 0.46491648\n",
      "Iteration 1394, loss = 0.46474926\n",
      "Iteration 1395, loss = 0.46458228\n",
      "Iteration 1396, loss = 0.46441566\n",
      "Iteration 1397, loss = 0.46424910\n",
      "Iteration 1398, loss = 0.46408276\n",
      "Iteration 1399, loss = 0.46391662\n",
      "Iteration 1400, loss = 0.46375062\n",
      "Iteration 1401, loss = 0.46358515\n",
      "Iteration 1402, loss = 0.46341984\n",
      "Iteration 1403, loss = 0.46325476\n",
      "Iteration 1404, loss = 0.46308985\n",
      "Iteration 1405, loss = 0.46292510\n",
      "Iteration 1406, loss = 0.46276055\n",
      "Iteration 1407, loss = 0.46259619\n",
      "Iteration 1408, loss = 0.46243182\n",
      "Iteration 1409, loss = 0.46226780\n",
      "Iteration 1410, loss = 0.46210399\n",
      "Iteration 1411, loss = 0.46194032\n",
      "Iteration 1412, loss = 0.46177669\n",
      "Iteration 1413, loss = 0.46161321\n",
      "Iteration 1414, loss = 0.46144996\n",
      "Iteration 1415, loss = 0.46128692\n",
      "Iteration 1416, loss = 0.46112393\n",
      "Iteration 1417, loss = 0.46096128\n",
      "Iteration 1418, loss = 0.46079874\n",
      "Iteration 1419, loss = 0.46063636\n",
      "Iteration 1420, loss = 0.46047404\n",
      "Iteration 1421, loss = 0.46031188\n",
      "Iteration 1422, loss = 0.46014992\n",
      "Iteration 1423, loss = 0.45998824\n",
      "Iteration 1424, loss = 0.45982666\n",
      "Iteration 1425, loss = 0.45966525\n",
      "Iteration 1426, loss = 0.45950411\n",
      "Iteration 1427, loss = 0.45934296\n",
      "Iteration 1428, loss = 0.45918196\n",
      "Iteration 1429, loss = 0.45902120\n",
      "Iteration 1430, loss = 0.45886067\n",
      "Iteration 1431, loss = 0.45870041\n",
      "Iteration 1432, loss = 0.45854016\n",
      "Iteration 1433, loss = 0.45838008\n",
      "Iteration 1434, loss = 0.45822006\n",
      "Iteration 1435, loss = 0.45806015\n",
      "Iteration 1436, loss = 0.45790057\n",
      "Iteration 1437, loss = 0.45774112\n",
      "Iteration 1438, loss = 0.45758186\n",
      "Iteration 1439, loss = 0.45742269\n",
      "Iteration 1440, loss = 0.45726372\n",
      "Iteration 1441, loss = 0.45710491\n",
      "Iteration 1442, loss = 0.45694623\n",
      "Iteration 1443, loss = 0.45678783\n",
      "Iteration 1444, loss = 0.45662944\n",
      "Iteration 1445, loss = 0.45647122\n",
      "Iteration 1446, loss = 0.45631308\n",
      "Iteration 1447, loss = 0.45615512\n",
      "Iteration 1448, loss = 0.45599735\n",
      "Iteration 1449, loss = 0.45583964\n",
      "Iteration 1450, loss = 0.45568207\n",
      "Iteration 1451, loss = 0.45552486\n",
      "Iteration 1452, loss = 0.45536765\n",
      "Iteration 1453, loss = 0.45521052\n",
      "Iteration 1454, loss = 0.45505366\n",
      "Iteration 1455, loss = 0.45489696\n",
      "Iteration 1456, loss = 0.45474023\n",
      "Iteration 1457, loss = 0.45458363\n",
      "Iteration 1458, loss = 0.45442741\n",
      "Iteration 1459, loss = 0.45427107\n",
      "Iteration 1460, loss = 0.45411494\n",
      "Iteration 1461, loss = 0.45395907\n",
      "Iteration 1462, loss = 0.45380325\n",
      "Iteration 1463, loss = 0.45364751\n",
      "Iteration 1464, loss = 0.45349199\n",
      "Iteration 1465, loss = 0.45333679\n",
      "Iteration 1466, loss = 0.45318175\n",
      "Iteration 1467, loss = 0.45302670\n",
      "Iteration 1468, loss = 0.45287194\n",
      "Iteration 1469, loss = 0.45271723\n",
      "Iteration 1470, loss = 0.45256262\n",
      "Iteration 1471, loss = 0.45240828\n",
      "Iteration 1472, loss = 0.45225410\n",
      "Iteration 1473, loss = 0.45209994\n",
      "Iteration 1474, loss = 0.45194588\n",
      "Iteration 1475, loss = 0.45179202\n",
      "Iteration 1476, loss = 0.45163834\n",
      "Iteration 1477, loss = 0.45148479\n",
      "Iteration 1478, loss = 0.45133140\n",
      "Iteration 1479, loss = 0.45117811\n",
      "Iteration 1480, loss = 0.45102501\n",
      "Iteration 1481, loss = 0.45087206\n",
      "Iteration 1482, loss = 0.45071919\n",
      "Iteration 1483, loss = 0.45056624\n",
      "Iteration 1484, loss = 0.45041391\n",
      "Iteration 1485, loss = 0.45026173\n",
      "Iteration 1486, loss = 0.45010964\n",
      "Iteration 1487, loss = 0.44995758\n",
      "Iteration 1488, loss = 0.44980560\n",
      "Iteration 1489, loss = 0.44965376\n",
      "Iteration 1490, loss = 0.44950198\n",
      "Iteration 1491, loss = 0.44935031\n",
      "Iteration 1492, loss = 0.44919874\n",
      "Iteration 1493, loss = 0.44904723\n",
      "Iteration 1494, loss = 0.44889577\n",
      "Iteration 1495, loss = 0.44874455\n",
      "Iteration 1496, loss = 0.44859344\n",
      "Iteration 1497, loss = 0.44844234\n",
      "Iteration 1498, loss = 0.44829135\n",
      "Iteration 1499, loss = 0.44814053\n",
      "Iteration 1500, loss = 0.44798981\n",
      "Iteration 1501, loss = 0.44783925\n",
      "Iteration 1502, loss = 0.44768888\n",
      "Iteration 1503, loss = 0.44753843\n",
      "Iteration 1504, loss = 0.44738824\n",
      "Iteration 1505, loss = 0.44723801\n",
      "Iteration 1506, loss = 0.44708780\n",
      "Iteration 1507, loss = 0.44693777\n",
      "Iteration 1508, loss = 0.44678784\n",
      "Iteration 1509, loss = 0.44663801\n",
      "Iteration 1510, loss = 0.44648832\n",
      "Iteration 1511, loss = 0.44633874\n",
      "Iteration 1512, loss = 0.44618926\n",
      "Iteration 1513, loss = 0.44603998\n",
      "Iteration 1514, loss = 0.44589077\n",
      "Iteration 1515, loss = 0.44574164\n",
      "Iteration 1516, loss = 0.44559271\n",
      "Iteration 1517, loss = 0.44544393\n",
      "Iteration 1518, loss = 0.44529526\n",
      "Iteration 1519, loss = 0.44514674\n",
      "Iteration 1520, loss = 0.44499853\n",
      "Iteration 1521, loss = 0.44485027\n",
      "Iteration 1522, loss = 0.44470216\n",
      "Iteration 1523, loss = 0.44455416\n",
      "Iteration 1524, loss = 0.44440636\n",
      "Iteration 1525, loss = 0.44425856\n",
      "Iteration 1526, loss = 0.44411119\n",
      "Iteration 1527, loss = 0.44396381\n",
      "Iteration 1528, loss = 0.44381645\n",
      "Iteration 1529, loss = 0.44366939\n",
      "Iteration 1530, loss = 0.44352227\n",
      "Iteration 1531, loss = 0.44337517\n",
      "Iteration 1532, loss = 0.44322850\n",
      "Iteration 1533, loss = 0.44308176\n",
      "Iteration 1534, loss = 0.44293498\n",
      "Iteration 1535, loss = 0.44278851\n",
      "Iteration 1536, loss = 0.44264224\n",
      "Iteration 1537, loss = 0.44249601\n",
      "Iteration 1538, loss = 0.44234989\n",
      "Iteration 1539, loss = 0.44220392\n",
      "Iteration 1540, loss = 0.44205816\n",
      "Iteration 1541, loss = 0.44191249\n",
      "Iteration 1542, loss = 0.44176697\n",
      "Iteration 1543, loss = 0.44162158\n",
      "Iteration 1544, loss = 0.44147623\n",
      "Iteration 1545, loss = 0.44133116\n",
      "Iteration 1546, loss = 0.44118619\n",
      "Iteration 1547, loss = 0.44104138\n",
      "Iteration 1548, loss = 0.44089662\n",
      "Iteration 1549, loss = 0.44075202\n",
      "Iteration 1550, loss = 0.44060763\n",
      "Iteration 1551, loss = 0.44046329\n",
      "Iteration 1552, loss = 0.44031920\n",
      "Iteration 1553, loss = 0.44017526\n",
      "Iteration 1554, loss = 0.44003133\n",
      "Iteration 1555, loss = 0.43988748\n",
      "Iteration 1556, loss = 0.43974389\n",
      "Iteration 1557, loss = 0.43960034\n",
      "Iteration 1558, loss = 0.43945702\n",
      "Iteration 1559, loss = 0.43931377\n",
      "Iteration 1560, loss = 0.43917069\n",
      "Iteration 1561, loss = 0.43902770\n",
      "Iteration 1562, loss = 0.43888476\n",
      "Iteration 1563, loss = 0.43874215\n",
      "Iteration 1564, loss = 0.43859965\n",
      "Iteration 1565, loss = 0.43845715\n",
      "Iteration 1566, loss = 0.43831484\n",
      "Iteration 1567, loss = 0.43817272\n",
      "Iteration 1568, loss = 0.43803068\n",
      "Iteration 1569, loss = 0.43788875\n",
      "Iteration 1570, loss = 0.43774699\n",
      "Iteration 1571, loss = 0.43760541\n",
      "Iteration 1572, loss = 0.43746381\n",
      "Iteration 1573, loss = 0.43732230\n",
      "Iteration 1574, loss = 0.43718090\n",
      "Iteration 1575, loss = 0.43703955\n",
      "Iteration 1576, loss = 0.43689825\n",
      "Iteration 1577, loss = 0.43675715\n",
      "Iteration 1578, loss = 0.43661617\n",
      "Iteration 1579, loss = 0.43647516\n",
      "Iteration 1580, loss = 0.43633404\n",
      "Iteration 1581, loss = 0.43619280\n",
      "Iteration 1582, loss = 0.43605151\n",
      "Iteration 1583, loss = 0.43591032\n",
      "Iteration 1584, loss = 0.43576908\n",
      "Iteration 1585, loss = 0.43562797\n",
      "Iteration 1586, loss = 0.43548692\n",
      "Iteration 1587, loss = 0.43534586\n",
      "Iteration 1588, loss = 0.43520483\n",
      "Iteration 1589, loss = 0.43506391\n",
      "Iteration 1590, loss = 0.43492327\n",
      "Iteration 1591, loss = 0.43478265\n",
      "Iteration 1592, loss = 0.43464197\n",
      "Iteration 1593, loss = 0.43450167\n",
      "Iteration 1594, loss = 0.43436146\n",
      "Iteration 1595, loss = 0.43422141\n",
      "Iteration 1596, loss = 0.43408131\n",
      "Iteration 1597, loss = 0.43394128\n",
      "Iteration 1598, loss = 0.43380130\n",
      "Iteration 1599, loss = 0.43366140\n",
      "Iteration 1600, loss = 0.43352160\n",
      "Iteration 1601, loss = 0.43338188\n",
      "Iteration 1602, loss = 0.43324218\n",
      "Iteration 1603, loss = 0.43310268\n",
      "Iteration 1604, loss = 0.43296321\n",
      "Iteration 1605, loss = 0.43282378\n",
      "Iteration 1606, loss = 0.43268461\n",
      "Iteration 1607, loss = 0.43254545\n",
      "Iteration 1608, loss = 0.43240632\n",
      "Iteration 1609, loss = 0.43226759\n",
      "Iteration 1610, loss = 0.43212899\n",
      "Iteration 1611, loss = 0.43199065\n",
      "Iteration 1612, loss = 0.43185219\n",
      "Iteration 1613, loss = 0.43171390\n",
      "Iteration 1614, loss = 0.43157527\n",
      "Iteration 1615, loss = 0.43143639\n",
      "Iteration 1616, loss = 0.43129750\n",
      "Iteration 1617, loss = 0.43115886\n",
      "Iteration 1618, loss = 0.43102036\n",
      "Iteration 1619, loss = 0.43088170\n",
      "Iteration 1620, loss = 0.43074338\n",
      "Iteration 1621, loss = 0.43060500\n",
      "Iteration 1622, loss = 0.43046658\n",
      "Iteration 1623, loss = 0.43032837\n",
      "Iteration 1624, loss = 0.43019031\n",
      "Iteration 1625, loss = 0.43005248\n",
      "Iteration 1626, loss = 0.42991467\n",
      "Iteration 1627, loss = 0.42977701\n",
      "Iteration 1628, loss = 0.42963943\n",
      "Iteration 1629, loss = 0.42950213\n",
      "Iteration 1630, loss = 0.42936482\n",
      "Iteration 1631, loss = 0.42922764\n",
      "Iteration 1632, loss = 0.42909060\n",
      "Iteration 1633, loss = 0.42895365\n",
      "Iteration 1634, loss = 0.42881669\n",
      "Iteration 1635, loss = 0.42867996\n",
      "Iteration 1636, loss = 0.42854315\n",
      "Iteration 1637, loss = 0.42840646\n",
      "Iteration 1638, loss = 0.42827007\n",
      "Iteration 1639, loss = 0.42813373\n",
      "Iteration 1640, loss = 0.42799749\n",
      "Iteration 1641, loss = 0.42786141\n",
      "Iteration 1642, loss = 0.42772544\n",
      "Iteration 1643, loss = 0.42758960\n",
      "Iteration 1644, loss = 0.42745382\n",
      "Iteration 1645, loss = 0.42731814\n",
      "Iteration 1646, loss = 0.42718249\n",
      "Iteration 1647, loss = 0.42704689\n",
      "Iteration 1648, loss = 0.42691147\n",
      "Iteration 1649, loss = 0.42677599\n",
      "Iteration 1650, loss = 0.42664075\n",
      "Iteration 1651, loss = 0.42650557\n",
      "Iteration 1652, loss = 0.42637105\n",
      "Iteration 1653, loss = 0.42623688\n",
      "Iteration 1654, loss = 0.42610264\n",
      "Iteration 1655, loss = 0.42596843\n",
      "Iteration 1656, loss = 0.42583401\n",
      "Iteration 1657, loss = 0.42569961\n",
      "Iteration 1658, loss = 0.42556545\n",
      "Iteration 1659, loss = 0.42543128\n",
      "Iteration 1660, loss = 0.42529712\n",
      "Iteration 1661, loss = 0.42516299\n",
      "Iteration 1662, loss = 0.42502846\n",
      "Iteration 1663, loss = 0.42489352\n",
      "Iteration 1664, loss = 0.42475880\n",
      "Iteration 1665, loss = 0.42462401\n",
      "Iteration 1666, loss = 0.42448980\n",
      "Iteration 1667, loss = 0.42435582\n",
      "Iteration 1668, loss = 0.42422227\n",
      "Iteration 1669, loss = 0.42408926\n",
      "Iteration 1670, loss = 0.42395649\n",
      "Iteration 1671, loss = 0.42382342\n",
      "Iteration 1672, loss = 0.42369063\n",
      "Iteration 1673, loss = 0.42355837\n",
      "Iteration 1674, loss = 0.42342677\n",
      "Iteration 1675, loss = 0.42329537\n",
      "Iteration 1676, loss = 0.42316398\n",
      "Iteration 1677, loss = 0.42303257\n",
      "Iteration 1678, loss = 0.42290143\n",
      "Iteration 1679, loss = 0.42277037\n",
      "Iteration 1680, loss = 0.42263921\n",
      "Iteration 1681, loss = 0.42250784\n",
      "Iteration 1682, loss = 0.42237634\n",
      "Iteration 1683, loss = 0.42224484\n",
      "Iteration 1684, loss = 0.42211371\n",
      "Iteration 1685, loss = 0.42198259\n",
      "Iteration 1686, loss = 0.42185134\n",
      "Iteration 1687, loss = 0.42172008\n",
      "Iteration 1688, loss = 0.42158902\n",
      "Iteration 1689, loss = 0.42145816\n",
      "Iteration 1690, loss = 0.42132739\n",
      "Iteration 1691, loss = 0.42119681\n",
      "Iteration 1692, loss = 0.42106659\n",
      "Iteration 1693, loss = 0.42093643\n",
      "Iteration 1694, loss = 0.42080633\n",
      "Iteration 1695, loss = 0.42067636\n",
      "Iteration 1696, loss = 0.42054644\n",
      "Iteration 1697, loss = 0.42041664\n",
      "Iteration 1698, loss = 0.42028686\n",
      "Iteration 1699, loss = 0.42015716\n",
      "Iteration 1700, loss = 0.42002762\n",
      "Iteration 1701, loss = 0.41989820\n",
      "Iteration 1702, loss = 0.41976881\n",
      "Iteration 1703, loss = 0.41963964\n",
      "Iteration 1704, loss = 0.41951053\n",
      "Iteration 1705, loss = 0.41938155\n",
      "Iteration 1706, loss = 0.41925293\n",
      "Iteration 1707, loss = 0.41912446\n",
      "Iteration 1708, loss = 0.41899597\n",
      "Iteration 1709, loss = 0.41886755\n",
      "Iteration 1710, loss = 0.41873933\n",
      "Iteration 1711, loss = 0.41861126\n",
      "Iteration 1712, loss = 0.41848329\n",
      "Iteration 1713, loss = 0.41835524\n",
      "Iteration 1714, loss = 0.41822742\n",
      "Iteration 1715, loss = 0.41809975\n",
      "Iteration 1716, loss = 0.41797217\n",
      "Iteration 1717, loss = 0.41784480\n",
      "Iteration 1718, loss = 0.41771729\n",
      "Iteration 1719, loss = 0.41758995\n",
      "Iteration 1720, loss = 0.41746277\n",
      "Iteration 1721, loss = 0.41733578\n",
      "Iteration 1722, loss = 0.41720873\n",
      "Iteration 1723, loss = 0.41708210\n",
      "Iteration 1724, loss = 0.41695588\n",
      "Iteration 1725, loss = 0.41682963\n",
      "Iteration 1726, loss = 0.41670336\n",
      "Iteration 1727, loss = 0.41657716\n",
      "Iteration 1728, loss = 0.41645120\n",
      "Iteration 1729, loss = 0.41632525\n",
      "Iteration 1730, loss = 0.41619928\n",
      "Iteration 1731, loss = 0.41607354\n",
      "Iteration 1732, loss = 0.41594780\n",
      "Iteration 1733, loss = 0.41582219\n",
      "Iteration 1734, loss = 0.41569660\n",
      "Iteration 1735, loss = 0.41557110\n",
      "Iteration 1736, loss = 0.41544587\n",
      "Iteration 1737, loss = 0.41532073\n",
      "Iteration 1738, loss = 0.41519570\n",
      "Iteration 1739, loss = 0.41507071\n",
      "Iteration 1740, loss = 0.41494583\n",
      "Iteration 1741, loss = 0.41482114\n",
      "Iteration 1742, loss = 0.41469656\n",
      "Iteration 1743, loss = 0.41457213\n",
      "Iteration 1744, loss = 0.41444788\n",
      "Iteration 1745, loss = 0.41432370\n",
      "Iteration 1746, loss = 0.41419952\n",
      "Iteration 1747, loss = 0.41407568\n",
      "Iteration 1748, loss = 0.41395185\n",
      "Iteration 1749, loss = 0.41382807\n",
      "Iteration 1750, loss = 0.41370454\n",
      "Iteration 1751, loss = 0.41358122\n",
      "Iteration 1752, loss = 0.41345800\n",
      "Iteration 1753, loss = 0.41333488\n",
      "Iteration 1754, loss = 0.41321193\n",
      "Iteration 1755, loss = 0.41308884\n",
      "Iteration 1756, loss = 0.41296601\n",
      "Iteration 1757, loss = 0.41284345\n",
      "Iteration 1758, loss = 0.41272094\n",
      "Iteration 1759, loss = 0.41259842\n",
      "Iteration 1760, loss = 0.41247623\n",
      "Iteration 1761, loss = 0.41235402\n",
      "Iteration 1762, loss = 0.41223207\n",
      "Iteration 1763, loss = 0.41211004\n",
      "Iteration 1764, loss = 0.41198812\n",
      "Iteration 1765, loss = 0.41186632\n",
      "Iteration 1766, loss = 0.41174469\n",
      "Iteration 1767, loss = 0.41162328\n",
      "Iteration 1768, loss = 0.41150186\n",
      "Iteration 1769, loss = 0.41138058\n",
      "Iteration 1770, loss = 0.41125922\n",
      "Iteration 1771, loss = 0.41113822\n",
      "Iteration 1772, loss = 0.41101727\n",
      "Iteration 1773, loss = 0.41089628\n",
      "Iteration 1774, loss = 0.41077534\n",
      "Iteration 1775, loss = 0.41065464\n",
      "Iteration 1776, loss = 0.41053396\n",
      "Iteration 1777, loss = 0.41041346\n",
      "Iteration 1778, loss = 0.41029302\n",
      "Iteration 1779, loss = 0.41017259\n",
      "Iteration 1780, loss = 0.41005241\n",
      "Iteration 1781, loss = 0.40993221\n",
      "Iteration 1782, loss = 0.40981207\n",
      "Iteration 1783, loss = 0.40969213\n",
      "Iteration 1784, loss = 0.40957240\n",
      "Iteration 1785, loss = 0.40945278\n",
      "Iteration 1786, loss = 0.40933315\n",
      "Iteration 1787, loss = 0.40921355\n",
      "Iteration 1788, loss = 0.40909421\n",
      "Iteration 1789, loss = 0.40897507\n",
      "Iteration 1790, loss = 0.40885601\n",
      "Iteration 1791, loss = 0.40873654\n",
      "Iteration 1792, loss = 0.40861706\n",
      "Iteration 1793, loss = 0.40849743\n",
      "Iteration 1794, loss = 0.40837777\n",
      "Iteration 1795, loss = 0.40825808\n",
      "Iteration 1796, loss = 0.40813848\n",
      "Iteration 1797, loss = 0.40801889\n",
      "Iteration 1798, loss = 0.40789942\n",
      "Iteration 1799, loss = 0.40778005\n",
      "Iteration 1800, loss = 0.40766052\n",
      "Iteration 1801, loss = 0.40754099\n",
      "Iteration 1802, loss = 0.40742152\n",
      "Iteration 1803, loss = 0.40730231\n",
      "Iteration 1804, loss = 0.40718315\n",
      "Iteration 1805, loss = 0.40706395\n",
      "Iteration 1806, loss = 0.40694484\n",
      "Iteration 1807, loss = 0.40682590\n",
      "Iteration 1808, loss = 0.40670704\n",
      "Iteration 1809, loss = 0.40658823\n",
      "Iteration 1810, loss = 0.40646986\n",
      "Iteration 1811, loss = 0.40635165\n",
      "Iteration 1812, loss = 0.40623371\n",
      "Iteration 1813, loss = 0.40611586\n",
      "Iteration 1814, loss = 0.40599796\n",
      "Iteration 1815, loss = 0.40588023\n",
      "Iteration 1816, loss = 0.40576259\n",
      "Iteration 1817, loss = 0.40564517\n",
      "Iteration 1818, loss = 0.40552785\n",
      "Iteration 1819, loss = 0.40541038\n",
      "Iteration 1820, loss = 0.40529321\n",
      "Iteration 1821, loss = 0.40517612\n",
      "Iteration 1822, loss = 0.40505902\n",
      "Iteration 1823, loss = 0.40494197\n",
      "Iteration 1824, loss = 0.40482527\n",
      "Iteration 1825, loss = 0.40470872\n",
      "Iteration 1826, loss = 0.40459230\n",
      "Iteration 1827, loss = 0.40447588\n",
      "Iteration 1828, loss = 0.40435951\n",
      "Iteration 1829, loss = 0.40424321\n",
      "Iteration 1830, loss = 0.40412719\n",
      "Iteration 1831, loss = 0.40401120\n",
      "Iteration 1832, loss = 0.40389532\n",
      "Iteration 1833, loss = 0.40377965\n",
      "Iteration 1834, loss = 0.40366401\n",
      "Iteration 1835, loss = 0.40354854\n",
      "Iteration 1836, loss = 0.40343330\n",
      "Iteration 1837, loss = 0.40331814\n",
      "Iteration 1838, loss = 0.40320321\n",
      "Iteration 1839, loss = 0.40308815\n",
      "Iteration 1840, loss = 0.40297331\n",
      "Iteration 1841, loss = 0.40285882\n",
      "Iteration 1842, loss = 0.40274424\n",
      "Iteration 1843, loss = 0.40262970\n",
      "Iteration 1844, loss = 0.40251518\n",
      "Iteration 1845, loss = 0.40240103\n",
      "Iteration 1846, loss = 0.40228710\n",
      "Iteration 1847, loss = 0.40217325\n",
      "Iteration 1848, loss = 0.40205951\n",
      "Iteration 1849, loss = 0.40194583\n",
      "Iteration 1850, loss = 0.40183221\n",
      "Iteration 1851, loss = 0.40171858\n",
      "Iteration 1852, loss = 0.40160523\n",
      "Iteration 1853, loss = 0.40149192\n",
      "Iteration 1854, loss = 0.40137886\n",
      "Iteration 1855, loss = 0.40126596\n",
      "Iteration 1856, loss = 0.40115308\n",
      "Iteration 1857, loss = 0.40104009\n",
      "Iteration 1858, loss = 0.40092727\n",
      "Iteration 1859, loss = 0.40081482\n",
      "Iteration 1860, loss = 0.40070237\n",
      "Iteration 1861, loss = 0.40059006\n",
      "Iteration 1862, loss = 0.40047772\n",
      "Iteration 1863, loss = 0.40036552\n",
      "Iteration 1864, loss = 0.40025346\n",
      "Iteration 1865, loss = 0.40014155\n",
      "Iteration 1866, loss = 0.40002973\n",
      "Iteration 1867, loss = 0.39991775\n",
      "Iteration 1868, loss = 0.39980583\n",
      "Iteration 1869, loss = 0.39969398\n",
      "Iteration 1870, loss = 0.39958230\n",
      "Iteration 1871, loss = 0.39947058\n",
      "Iteration 1872, loss = 0.39935904\n",
      "Iteration 1873, loss = 0.39924754\n",
      "Iteration 1874, loss = 0.39913602\n",
      "Iteration 1875, loss = 0.39902455\n",
      "Iteration 1876, loss = 0.39891330\n",
      "Iteration 1877, loss = 0.39880215\n",
      "Iteration 1878, loss = 0.39869109\n",
      "Iteration 1879, loss = 0.39858006\n",
      "Iteration 1880, loss = 0.39846910\n",
      "Iteration 1881, loss = 0.39835831\n",
      "Iteration 1882, loss = 0.39824769\n",
      "Iteration 1883, loss = 0.39813709\n",
      "Iteration 1884, loss = 0.39802655\n",
      "Iteration 1885, loss = 0.39791625\n",
      "Iteration 1886, loss = 0.39780599\n",
      "Iteration 1887, loss = 0.39769586\n",
      "Iteration 1888, loss = 0.39758581\n",
      "Iteration 1889, loss = 0.39747588\n",
      "Iteration 1890, loss = 0.39736614\n",
      "Iteration 1891, loss = 0.39725647\n",
      "Iteration 1892, loss = 0.39714672\n",
      "Iteration 1893, loss = 0.39703724\n",
      "Iteration 1894, loss = 0.39692784\n",
      "Iteration 1895, loss = 0.39681829\n",
      "Iteration 1896, loss = 0.39670888\n",
      "Iteration 1897, loss = 0.39659964\n",
      "Iteration 1898, loss = 0.39649043\n",
      "Iteration 1899, loss = 0.39638124\n",
      "Iteration 1900, loss = 0.39627229\n",
      "Iteration 1901, loss = 0.39616341\n",
      "Iteration 1902, loss = 0.39605428\n",
      "Iteration 1903, loss = 0.39594520\n",
      "Iteration 1904, loss = 0.39583632\n",
      "Iteration 1905, loss = 0.39572712\n",
      "Iteration 1906, loss = 0.39561790\n",
      "Iteration 1907, loss = 0.39550890\n",
      "Iteration 1908, loss = 0.39539984\n",
      "Iteration 1909, loss = 0.39529098\n",
      "Iteration 1910, loss = 0.39518236\n",
      "Iteration 1911, loss = 0.39507401\n",
      "Iteration 1912, loss = 0.39496528\n",
      "Iteration 1913, loss = 0.39485569\n",
      "Iteration 1914, loss = 0.39474570\n",
      "Iteration 1915, loss = 0.39463466\n",
      "Iteration 1916, loss = 0.39452474\n",
      "Iteration 1917, loss = 0.39441691\n",
      "Iteration 1918, loss = 0.39430852\n",
      "Iteration 1919, loss = 0.39419989\n",
      "Iteration 1920, loss = 0.39409129\n",
      "Iteration 1921, loss = 0.39398298\n",
      "Iteration 1922, loss = 0.39387434\n",
      "Iteration 1923, loss = 0.39376525\n",
      "Iteration 1924, loss = 0.39365605\n",
      "Iteration 1925, loss = 0.39354673\n",
      "Iteration 1926, loss = 0.39343794\n",
      "Iteration 1927, loss = 0.39332719\n",
      "Iteration 1928, loss = 0.39321849\n",
      "Iteration 1929, loss = 0.39310926\n",
      "Iteration 1930, loss = 0.39299918\n",
      "Iteration 1931, loss = 0.39288692\n",
      "Iteration 1932, loss = 0.39277393\n",
      "Iteration 1933, loss = 0.39266045\n",
      "Iteration 1934, loss = 0.39254759\n",
      "Iteration 1935, loss = 0.39243478\n",
      "Iteration 1936, loss = 0.39232211\n",
      "Iteration 1937, loss = 0.39221061\n",
      "Iteration 1938, loss = 0.39210185\n",
      "Iteration 1939, loss = 0.39199344\n",
      "Iteration 1940, loss = 0.39188539\n",
      "Iteration 1941, loss = 0.39177753\n",
      "Iteration 1942, loss = 0.39166980\n",
      "Iteration 1943, loss = 0.39156240\n",
      "Iteration 1944, loss = 0.39145523\n",
      "Iteration 1945, loss = 0.39134829\n",
      "Iteration 1946, loss = 0.39124144\n",
      "Iteration 1947, loss = 0.39113480\n",
      "Iteration 1948, loss = 0.39102839\n",
      "Iteration 1949, loss = 0.39092214\n",
      "Iteration 1950, loss = 0.39081597\n",
      "Iteration 1951, loss = 0.39070998\n",
      "Iteration 1952, loss = 0.39060418\n",
      "Iteration 1953, loss = 0.39049884\n",
      "Iteration 1954, loss = 0.39039356\n",
      "Iteration 1955, loss = 0.39028861\n",
      "Iteration 1956, loss = 0.39018360\n",
      "Iteration 1957, loss = 0.39007883\n",
      "Iteration 1958, loss = 0.38997428\n",
      "Iteration 1959, loss = 0.38987002\n",
      "Iteration 1960, loss = 0.38976592\n",
      "Iteration 1961, loss = 0.38966196\n",
      "Iteration 1962, loss = 0.38955807\n",
      "Iteration 1963, loss = 0.38945413\n",
      "Iteration 1964, loss = 0.38935038\n",
      "Iteration 1965, loss = 0.38924687\n",
      "Iteration 1966, loss = 0.38914333\n",
      "Iteration 1967, loss = 0.38903987\n",
      "Iteration 1968, loss = 0.38893670\n",
      "Iteration 1969, loss = 0.38883367\n",
      "Iteration 1970, loss = 0.38873055\n",
      "Iteration 1971, loss = 0.38862758\n",
      "Iteration 1972, loss = 0.38852477\n",
      "Iteration 1973, loss = 0.38842205\n",
      "Iteration 1974, loss = 0.38831941\n",
      "Iteration 1975, loss = 0.38821699\n",
      "Iteration 1976, loss = 0.38811463\n",
      "Iteration 1977, loss = 0.38801232\n",
      "Iteration 1978, loss = 0.38790994\n",
      "Iteration 1979, loss = 0.38780771\n",
      "Iteration 1980, loss = 0.38770553\n",
      "Iteration 1981, loss = 0.38760348\n",
      "Iteration 1982, loss = 0.38750149\n",
      "Iteration 1983, loss = 0.38739942\n",
      "Iteration 1984, loss = 0.38729722\n",
      "Iteration 1985, loss = 0.38719508\n",
      "Iteration 1986, loss = 0.38709305\n",
      "Iteration 1987, loss = 0.38699114\n",
      "Iteration 1988, loss = 0.38688911\n",
      "Iteration 1989, loss = 0.38678718\n",
      "Iteration 1990, loss = 0.38668528\n",
      "Iteration 1991, loss = 0.38658343\n",
      "Iteration 1992, loss = 0.38648207\n",
      "Iteration 1993, loss = 0.38638083\n",
      "Iteration 1994, loss = 0.38627931\n",
      "Iteration 1995, loss = 0.38617800\n",
      "Iteration 1996, loss = 0.38607687\n",
      "Iteration 1997, loss = 0.38597575\n",
      "Iteration 1998, loss = 0.38587461\n",
      "Iteration 1999, loss = 0.38577381\n",
      "Iteration 2000, loss = 0.38567336\n",
      "Iteration 2001, loss = 0.38557291\n",
      "Iteration 2002, loss = 0.38547252\n",
      "Iteration 2003, loss = 0.38537232\n",
      "Iteration 2004, loss = 0.38527197\n",
      "Iteration 2005, loss = 0.38517157\n",
      "Iteration 2006, loss = 0.38507144\n",
      "Iteration 2007, loss = 0.38497128\n",
      "Iteration 2008, loss = 0.38487130\n",
      "Iteration 2009, loss = 0.38477130\n",
      "Iteration 2010, loss = 0.38467140\n",
      "Iteration 2011, loss = 0.38457160\n",
      "Iteration 2012, loss = 0.38447176\n",
      "Iteration 2013, loss = 0.38437214\n",
      "Iteration 2014, loss = 0.38427253\n",
      "Iteration 2015, loss = 0.38417305\n",
      "Iteration 2016, loss = 0.38407384\n",
      "Iteration 2017, loss = 0.38397482\n",
      "Iteration 2018, loss = 0.38387583\n",
      "Iteration 2019, loss = 0.38377693\n",
      "Iteration 2020, loss = 0.38367833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.83      1.00      0.91        29\n",
      "         6.0       1.00      0.78      0.88        27\n",
      "\n",
      "    accuracy                           0.89        56\n",
      "   macro avg       0.91      0.89      0.89        56\n",
      "weighted avg       0.91      0.89      0.89        56\n",
      "\n",
      "Training ROC AUC Score: 0.9655172413793103\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      1.00      0.84         8\n",
      "         6.0       1.00      0.57      0.73         7\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.86      0.79      0.78        15\n",
      "weighted avg       0.85      0.80      0.79        15\n",
      "\n",
      "Test ROC AUC Score: 0.8928571428571428\n",
      "Iteration 1, loss = 1.32408919\n",
      "Iteration 2, loss = 1.32205392\n",
      "Iteration 3, loss = 1.32003396\n",
      "Iteration 4, loss = 1.31802575\n",
      "Iteration 5, loss = 1.31603182\n",
      "Iteration 6, loss = 1.31404364\n",
      "Iteration 7, loss = 1.31205715\n",
      "Iteration 8, loss = 1.31007875\n",
      "Iteration 9, loss = 1.30810794\n",
      "Iteration 10, loss = 1.30615151\n",
      "Iteration 11, loss = 1.30420963\n",
      "Iteration 12, loss = 1.30227450\n",
      "Iteration 13, loss = 1.30034534\n",
      "Iteration 14, loss = 1.29842386\n",
      "Iteration 15, loss = 1.29651467\n",
      "Iteration 16, loss = 1.29461875\n",
      "Iteration 17, loss = 1.29272937\n",
      "Iteration 18, loss = 1.29085173\n",
      "Iteration 19, loss = 1.28898074\n",
      "Iteration 20, loss = 1.28711682\n",
      "Iteration 21, loss = 1.28525926\n",
      "Iteration 22, loss = 1.28340995\n",
      "Iteration 23, loss = 1.28156793\n",
      "Iteration 24, loss = 1.27973278\n",
      "Iteration 25, loss = 1.27790550\n",
      "Iteration 26, loss = 1.27608772\n",
      "Iteration 27, loss = 1.27428212\n",
      "Iteration 28, loss = 1.27248806\n",
      "Iteration 29, loss = 1.27070417\n",
      "Iteration 30, loss = 1.26894399\n",
      "Iteration 31, loss = 1.26718190\n",
      "Iteration 32, loss = 1.26541769\n",
      "Iteration 33, loss = 1.26365807\n",
      "Iteration 34, loss = 1.26190297\n",
      "Iteration 35, loss = 1.26015928\n",
      "Iteration 36, loss = 1.25844623\n",
      "Iteration 37, loss = 1.25674133\n",
      "Iteration 38, loss = 1.25504412\n",
      "Iteration 39, loss = 1.25335256\n",
      "Iteration 40, loss = 1.25166462\n",
      "Iteration 41, loss = 1.24998666\n",
      "Iteration 42, loss = 1.24832191\n",
      "Iteration 43, loss = 1.24665712\n",
      "Iteration 44, loss = 1.24500092\n",
      "Iteration 45, loss = 1.24335050\n",
      "Iteration 46, loss = 1.24170663\n",
      "Iteration 47, loss = 1.24007022\n",
      "Iteration 48, loss = 1.23843824\n",
      "Iteration 49, loss = 1.23681096\n",
      "Iteration 50, loss = 1.23518573\n",
      "Iteration 51, loss = 1.23355304\n",
      "Iteration 52, loss = 1.23192278\n",
      "Iteration 53, loss = 1.23029995\n",
      "Iteration 54, loss = 1.22868793\n",
      "Iteration 55, loss = 1.22708957\n",
      "Iteration 56, loss = 1.22549664\n",
      "Iteration 57, loss = 1.22390912\n",
      "Iteration 58, loss = 1.22232679\n",
      "Iteration 59, loss = 1.22074377\n",
      "Iteration 60, loss = 1.21916313\n",
      "Iteration 61, loss = 1.21758622\n",
      "Iteration 62, loss = 1.21601607\n",
      "Iteration 63, loss = 1.21445084\n",
      "Iteration 64, loss = 1.21289181\n",
      "Iteration 65, loss = 1.21133208\n",
      "Iteration 66, loss = 1.20977725\n",
      "Iteration 67, loss = 1.20822719\n",
      "Iteration 68, loss = 1.20668147\n",
      "Iteration 69, loss = 1.20513814\n",
      "Iteration 70, loss = 1.20360320\n",
      "Iteration 71, loss = 1.20207171\n",
      "Iteration 72, loss = 1.20054404\n",
      "Iteration 73, loss = 1.19902589\n",
      "Iteration 74, loss = 1.19751490\n",
      "Iteration 75, loss = 1.19600924\n",
      "Iteration 76, loss = 1.19450894\n",
      "Iteration 77, loss = 1.19301282\n",
      "Iteration 78, loss = 1.19152970\n",
      "Iteration 79, loss = 1.19005652\n",
      "Iteration 80, loss = 1.18858927\n",
      "Iteration 81, loss = 1.18712817\n",
      "Iteration 82, loss = 1.18567295\n",
      "Iteration 83, loss = 1.18422339\n",
      "Iteration 84, loss = 1.18278016\n",
      "Iteration 85, loss = 1.18134231\n",
      "Iteration 86, loss = 1.17990934\n",
      "Iteration 87, loss = 1.17848676\n",
      "Iteration 88, loss = 1.17706899\n",
      "Iteration 89, loss = 1.17565636\n",
      "Iteration 90, loss = 1.17424933\n",
      "Iteration 91, loss = 1.17284776\n",
      "Iteration 92, loss = 1.17145079\n",
      "Iteration 93, loss = 1.17005821\n",
      "Iteration 94, loss = 1.16866684\n",
      "Iteration 95, loss = 1.16728263\n",
      "Iteration 96, loss = 1.16590287\n",
      "Iteration 97, loss = 1.16452976\n",
      "Iteration 98, loss = 1.16316541\n",
      "Iteration 99, loss = 1.16180466\n",
      "Iteration 100, loss = 1.16044649\n",
      "Iteration 101, loss = 1.15909442\n",
      "Iteration 102, loss = 1.15774858\n",
      "Iteration 103, loss = 1.15640822\n",
      "Iteration 104, loss = 1.15507234\n",
      "Iteration 105, loss = 1.15374076\n",
      "Iteration 106, loss = 1.15241361\n",
      "Iteration 107, loss = 1.15109053\n",
      "Iteration 108, loss = 1.14977487\n",
      "Iteration 109, loss = 1.14846456\n",
      "Iteration 110, loss = 1.14715732\n",
      "Iteration 111, loss = 1.14585540\n",
      "Iteration 112, loss = 1.14455573\n",
      "Iteration 113, loss = 1.14325980\n",
      "Iteration 114, loss = 1.14196944\n",
      "Iteration 115, loss = 1.14068197\n",
      "Iteration 116, loss = 1.13939735\n",
      "Iteration 117, loss = 1.13811619\n",
      "Iteration 118, loss = 1.13683948\n",
      "Iteration 119, loss = 1.13556613\n",
      "Iteration 120, loss = 1.13429673\n",
      "Iteration 121, loss = 1.13302771\n",
      "Iteration 122, loss = 1.13175887\n",
      "Iteration 123, loss = 1.13049350\n",
      "Iteration 124, loss = 1.12923745\n",
      "Iteration 125, loss = 1.12798113\n",
      "Iteration 126, loss = 1.12672592\n",
      "Iteration 127, loss = 1.12547424\n",
      "Iteration 128, loss = 1.12422279\n",
      "Iteration 129, loss = 1.12297410\n",
      "Iteration 130, loss = 1.12172692\n",
      "Iteration 131, loss = 1.12048202\n",
      "Iteration 132, loss = 1.11923981\n",
      "Iteration 133, loss = 1.11800021\n",
      "Iteration 134, loss = 1.11676283\n",
      "Iteration 135, loss = 1.11552962\n",
      "Iteration 136, loss = 1.11430026\n",
      "Iteration 137, loss = 1.11307373\n",
      "Iteration 138, loss = 1.11185543\n",
      "Iteration 139, loss = 1.11064693\n",
      "Iteration 140, loss = 1.10944144\n",
      "Iteration 141, loss = 1.10823844\n",
      "Iteration 142, loss = 1.10703735\n",
      "Iteration 143, loss = 1.10583899\n",
      "Iteration 144, loss = 1.10464384\n",
      "Iteration 145, loss = 1.10345137\n",
      "Iteration 146, loss = 1.10226621\n",
      "Iteration 147, loss = 1.10108306\n",
      "Iteration 148, loss = 1.09990300\n",
      "Iteration 149, loss = 1.09872572\n",
      "Iteration 150, loss = 1.09755280\n",
      "Iteration 151, loss = 1.09638407\n",
      "Iteration 152, loss = 1.09521842\n",
      "Iteration 153, loss = 1.09405463\n",
      "Iteration 154, loss = 1.09289397\n",
      "Iteration 155, loss = 1.09173515\n",
      "Iteration 156, loss = 1.09058070\n",
      "Iteration 157, loss = 1.08943193\n",
      "Iteration 158, loss = 1.08828663\n",
      "Iteration 159, loss = 1.08714666\n",
      "Iteration 160, loss = 1.08601023\n",
      "Iteration 161, loss = 1.08487617\n",
      "Iteration 162, loss = 1.08374286\n",
      "Iteration 163, loss = 1.08261207\n",
      "Iteration 164, loss = 1.08148342\n",
      "Iteration 165, loss = 1.08035635\n",
      "Iteration 166, loss = 1.07923435\n",
      "Iteration 167, loss = 1.07811351\n",
      "Iteration 168, loss = 1.07699298\n",
      "Iteration 169, loss = 1.07587324\n",
      "Iteration 170, loss = 1.07475232\n",
      "Iteration 171, loss = 1.07363494\n",
      "Iteration 172, loss = 1.07251983\n",
      "Iteration 173, loss = 1.07140722\n",
      "Iteration 174, loss = 1.07029639\n",
      "Iteration 175, loss = 1.06918316\n",
      "Iteration 176, loss = 1.06807055\n",
      "Iteration 177, loss = 1.06695931\n",
      "Iteration 178, loss = 1.06585012\n",
      "Iteration 179, loss = 1.06474407\n",
      "Iteration 180, loss = 1.06364031\n",
      "Iteration 181, loss = 1.06253884\n",
      "Iteration 182, loss = 1.06144042\n",
      "Iteration 183, loss = 1.06034376\n",
      "Iteration 184, loss = 1.05924906\n",
      "Iteration 185, loss = 1.05815741\n",
      "Iteration 186, loss = 1.05706702\n",
      "Iteration 187, loss = 1.05597720\n",
      "Iteration 188, loss = 1.05488737\n",
      "Iteration 189, loss = 1.05380066\n",
      "Iteration 190, loss = 1.05271426\n",
      "Iteration 191, loss = 1.05162919\n",
      "Iteration 192, loss = 1.05054899\n",
      "Iteration 193, loss = 1.04946829\n",
      "Iteration 194, loss = 1.04838752\n",
      "Iteration 195, loss = 1.04730809\n",
      "Iteration 196, loss = 1.04623045\n",
      "Iteration 197, loss = 1.04515013\n",
      "Iteration 198, loss = 1.04406964\n",
      "Iteration 199, loss = 1.04299409\n",
      "Iteration 200, loss = 1.04192215\n",
      "Iteration 201, loss = 1.04085268\n",
      "Iteration 202, loss = 1.03978417\n",
      "Iteration 203, loss = 1.03871616\n",
      "Iteration 204, loss = 1.03764770\n",
      "Iteration 205, loss = 1.03658128\n",
      "Iteration 206, loss = 1.03551718\n",
      "Iteration 207, loss = 1.03445194\n",
      "Iteration 208, loss = 1.03338898\n",
      "Iteration 209, loss = 1.03232874\n",
      "Iteration 210, loss = 1.03127316\n",
      "Iteration 211, loss = 1.03021754\n",
      "Iteration 212, loss = 1.02916284\n",
      "Iteration 213, loss = 1.02811297\n",
      "Iteration 214, loss = 1.02706501\n",
      "Iteration 215, loss = 1.02601968\n",
      "Iteration 216, loss = 1.02497933\n",
      "Iteration 217, loss = 1.02394061\n",
      "Iteration 218, loss = 1.02290171\n",
      "Iteration 219, loss = 1.02186517\n",
      "Iteration 220, loss = 1.02082846\n",
      "Iteration 221, loss = 1.01979330\n",
      "Iteration 222, loss = 1.01876421\n",
      "Iteration 223, loss = 1.01773187\n",
      "Iteration 224, loss = 1.01669643\n",
      "Iteration 225, loss = 1.01566021\n",
      "Iteration 226, loss = 1.01462357\n",
      "Iteration 227, loss = 1.01358725\n",
      "Iteration 228, loss = 1.01255412\n",
      "Iteration 229, loss = 1.01152180\n",
      "Iteration 230, loss = 1.01048991\n",
      "Iteration 231, loss = 1.00946085\n",
      "Iteration 232, loss = 1.00843339\n",
      "Iteration 233, loss = 1.00740366\n",
      "Iteration 234, loss = 1.00637631\n",
      "Iteration 235, loss = 1.00535033\n",
      "Iteration 236, loss = 1.00432435\n",
      "Iteration 237, loss = 1.00330162\n",
      "Iteration 238, loss = 1.00227910\n",
      "Iteration 239, loss = 1.00125523\n",
      "Iteration 240, loss = 1.00023089\n",
      "Iteration 241, loss = 0.99920622\n",
      "Iteration 242, loss = 0.99818458\n",
      "Iteration 243, loss = 0.99716498\n",
      "Iteration 244, loss = 0.99614872\n",
      "Iteration 245, loss = 0.99513810\n",
      "Iteration 246, loss = 0.99413052\n",
      "Iteration 247, loss = 0.99312966\n",
      "Iteration 248, loss = 0.99213025\n",
      "Iteration 249, loss = 0.99113098\n",
      "Iteration 250, loss = 0.99013310\n",
      "Iteration 251, loss = 0.98913938\n",
      "Iteration 252, loss = 0.98814865\n",
      "Iteration 253, loss = 0.98715871\n",
      "Iteration 254, loss = 0.98617257\n",
      "Iteration 255, loss = 0.98518880\n",
      "Iteration 256, loss = 0.98420005\n",
      "Iteration 257, loss = 0.98320782\n",
      "Iteration 258, loss = 0.98221536\n",
      "Iteration 259, loss = 0.98122420\n",
      "Iteration 260, loss = 0.98023556\n",
      "Iteration 261, loss = 0.97925577\n",
      "Iteration 262, loss = 0.97828189\n",
      "Iteration 263, loss = 0.97731010\n",
      "Iteration 264, loss = 0.97634167\n",
      "Iteration 265, loss = 0.97537446\n",
      "Iteration 266, loss = 0.97440768\n",
      "Iteration 267, loss = 0.97344292\n",
      "Iteration 268, loss = 0.97247924\n",
      "Iteration 269, loss = 0.97151812\n",
      "Iteration 270, loss = 0.97056082\n",
      "Iteration 271, loss = 0.96960568\n",
      "Iteration 272, loss = 0.96865138\n",
      "Iteration 273, loss = 0.96769998\n",
      "Iteration 274, loss = 0.96675047\n",
      "Iteration 275, loss = 0.96579966\n",
      "Iteration 276, loss = 0.96484881\n",
      "Iteration 277, loss = 0.96389949\n",
      "Iteration 278, loss = 0.96295034\n",
      "Iteration 279, loss = 0.96200228\n",
      "Iteration 280, loss = 0.96105613\n",
      "Iteration 281, loss = 0.96011263\n",
      "Iteration 282, loss = 0.95917207\n",
      "Iteration 283, loss = 0.95823340\n",
      "Iteration 284, loss = 0.95729363\n",
      "Iteration 285, loss = 0.95635538\n",
      "Iteration 286, loss = 0.95541998\n",
      "Iteration 287, loss = 0.95448586\n",
      "Iteration 288, loss = 0.95355387\n",
      "Iteration 289, loss = 0.95262495\n",
      "Iteration 290, loss = 0.95169839\n",
      "Iteration 291, loss = 0.95077352\n",
      "Iteration 292, loss = 0.94985314\n",
      "Iteration 293, loss = 0.94893350\n",
      "Iteration 294, loss = 0.94801793\n",
      "Iteration 295, loss = 0.94710357\n",
      "Iteration 296, loss = 0.94619034\n",
      "Iteration 297, loss = 0.94528090\n",
      "Iteration 298, loss = 0.94437323\n",
      "Iteration 299, loss = 0.94346897\n",
      "Iteration 300, loss = 0.94256775\n",
      "Iteration 301, loss = 0.94167088\n",
      "Iteration 302, loss = 0.94077460\n",
      "Iteration 303, loss = 0.93987993\n",
      "Iteration 304, loss = 0.93898526\n",
      "Iteration 305, loss = 0.93809338\n",
      "Iteration 306, loss = 0.93720077\n",
      "Iteration 307, loss = 0.93630999\n",
      "Iteration 308, loss = 0.93542146\n",
      "Iteration 309, loss = 0.93453432\n",
      "Iteration 310, loss = 0.93364884\n",
      "Iteration 311, loss = 0.93276518\n",
      "Iteration 312, loss = 0.93188411\n",
      "Iteration 313, loss = 0.93100417\n",
      "Iteration 314, loss = 0.93012723\n",
      "Iteration 315, loss = 0.92925227\n",
      "Iteration 316, loss = 0.92838034\n",
      "Iteration 317, loss = 0.92751099\n",
      "Iteration 318, loss = 0.92664369\n",
      "Iteration 319, loss = 0.92577888\n",
      "Iteration 320, loss = 0.92491482\n",
      "Iteration 321, loss = 0.92405158\n",
      "Iteration 322, loss = 0.92318986\n",
      "Iteration 323, loss = 0.92233014\n",
      "Iteration 324, loss = 0.92147200\n",
      "Iteration 325, loss = 0.92061526\n",
      "Iteration 326, loss = 0.91976195\n",
      "Iteration 327, loss = 0.91890957\n",
      "Iteration 328, loss = 0.91805944\n",
      "Iteration 329, loss = 0.91721225\n",
      "Iteration 330, loss = 0.91636553\n",
      "Iteration 331, loss = 0.91552109\n",
      "Iteration 332, loss = 0.91467927\n",
      "Iteration 333, loss = 0.91383888\n",
      "Iteration 334, loss = 0.91300058\n",
      "Iteration 335, loss = 0.91216342\n",
      "Iteration 336, loss = 0.91132550\n",
      "Iteration 337, loss = 0.91048843\n",
      "Iteration 338, loss = 0.90965109\n",
      "Iteration 339, loss = 0.90881437\n",
      "Iteration 340, loss = 0.90797860\n",
      "Iteration 341, loss = 0.90714441\n",
      "Iteration 342, loss = 0.90630974\n",
      "Iteration 343, loss = 0.90547707\n",
      "Iteration 344, loss = 0.90464453\n",
      "Iteration 345, loss = 0.90380939\n",
      "Iteration 346, loss = 0.90297311\n",
      "Iteration 347, loss = 0.90213456\n",
      "Iteration 348, loss = 0.90129628\n",
      "Iteration 349, loss = 0.90046005\n",
      "Iteration 350, loss = 0.89962392\n",
      "Iteration 351, loss = 0.89878840\n",
      "Iteration 352, loss = 0.89795525\n",
      "Iteration 353, loss = 0.89712205\n",
      "Iteration 354, loss = 0.89628960\n",
      "Iteration 355, loss = 0.89545807\n",
      "Iteration 356, loss = 0.89462841\n",
      "Iteration 357, loss = 0.89379959\n",
      "Iteration 358, loss = 0.89297415\n",
      "Iteration 359, loss = 0.89215090\n",
      "Iteration 360, loss = 0.89132824\n",
      "Iteration 361, loss = 0.89050889\n",
      "Iteration 362, loss = 0.88969140\n",
      "Iteration 363, loss = 0.88887668\n",
      "Iteration 364, loss = 0.88806352\n",
      "Iteration 365, loss = 0.88725338\n",
      "Iteration 366, loss = 0.88644594\n",
      "Iteration 367, loss = 0.88564126\n",
      "Iteration 368, loss = 0.88483752\n",
      "Iteration 369, loss = 0.88403487\n",
      "Iteration 370, loss = 0.88323187\n",
      "Iteration 371, loss = 0.88243110\n",
      "Iteration 372, loss = 0.88163120\n",
      "Iteration 373, loss = 0.88083326\n",
      "Iteration 374, loss = 0.88003795\n",
      "Iteration 375, loss = 0.87924507\n",
      "Iteration 376, loss = 0.87845269\n",
      "Iteration 377, loss = 0.87766026\n",
      "Iteration 378, loss = 0.87687146\n",
      "Iteration 379, loss = 0.87608287\n",
      "Iteration 380, loss = 0.87529744\n",
      "Iteration 381, loss = 0.87451351\n",
      "Iteration 382, loss = 0.87373124\n",
      "Iteration 383, loss = 0.87295146\n",
      "Iteration 384, loss = 0.87217310\n",
      "Iteration 385, loss = 0.87139518\n",
      "Iteration 386, loss = 0.87061814\n",
      "Iteration 387, loss = 0.86984331\n",
      "Iteration 388, loss = 0.86906850\n",
      "Iteration 389, loss = 0.86829351\n",
      "Iteration 390, loss = 0.86751722\n",
      "Iteration 391, loss = 0.86674081\n",
      "Iteration 392, loss = 0.86596596\n",
      "Iteration 393, loss = 0.86519447\n",
      "Iteration 394, loss = 0.86442362\n",
      "Iteration 395, loss = 0.86365367\n",
      "Iteration 396, loss = 0.86288490\n",
      "Iteration 397, loss = 0.86211880\n",
      "Iteration 398, loss = 0.86135540\n",
      "Iteration 399, loss = 0.86059297\n",
      "Iteration 400, loss = 0.85983262\n",
      "Iteration 401, loss = 0.85907388\n",
      "Iteration 402, loss = 0.85831670\n",
      "Iteration 403, loss = 0.85756110\n",
      "Iteration 404, loss = 0.85680443\n",
      "Iteration 405, loss = 0.85604771\n",
      "Iteration 406, loss = 0.85529205\n",
      "Iteration 407, loss = 0.85453787\n",
      "Iteration 408, loss = 0.85378551\n",
      "Iteration 409, loss = 0.85303470\n",
      "Iteration 410, loss = 0.85228430\n",
      "Iteration 411, loss = 0.85153431\n",
      "Iteration 412, loss = 0.85078544\n",
      "Iteration 413, loss = 0.85003783\n",
      "Iteration 414, loss = 0.84929200\n",
      "Iteration 415, loss = 0.84854839\n",
      "Iteration 416, loss = 0.84780655\n",
      "Iteration 417, loss = 0.84706577\n",
      "Iteration 418, loss = 0.84632502\n",
      "Iteration 419, loss = 0.84558725\n",
      "Iteration 420, loss = 0.84485187\n",
      "Iteration 421, loss = 0.84411694\n",
      "Iteration 422, loss = 0.84338239\n",
      "Iteration 423, loss = 0.84265097\n",
      "Iteration 424, loss = 0.84191976\n",
      "Iteration 425, loss = 0.84118889\n",
      "Iteration 426, loss = 0.84046007\n",
      "Iteration 427, loss = 0.83973343\n",
      "Iteration 428, loss = 0.83900864\n",
      "Iteration 429, loss = 0.83828582\n",
      "Iteration 430, loss = 0.83756402\n",
      "Iteration 431, loss = 0.83684477\n",
      "Iteration 432, loss = 0.83612733\n",
      "Iteration 433, loss = 0.83541195\n",
      "Iteration 434, loss = 0.83469862\n",
      "Iteration 435, loss = 0.83398885\n",
      "Iteration 436, loss = 0.83328169\n",
      "Iteration 437, loss = 0.83257370\n",
      "Iteration 438, loss = 0.83186759\n",
      "Iteration 439, loss = 0.83116407\n",
      "Iteration 440, loss = 0.83046223\n",
      "Iteration 441, loss = 0.82976144\n",
      "Iteration 442, loss = 0.82906268\n",
      "Iteration 443, loss = 0.82836545\n",
      "Iteration 444, loss = 0.82766940\n",
      "Iteration 445, loss = 0.82697468\n",
      "Iteration 446, loss = 0.82628231\n",
      "Iteration 447, loss = 0.82559194\n",
      "Iteration 448, loss = 0.82490257\n",
      "Iteration 449, loss = 0.82421315\n",
      "Iteration 450, loss = 0.82352545\n",
      "Iteration 451, loss = 0.82283870\n",
      "Iteration 452, loss = 0.82215297\n",
      "Iteration 453, loss = 0.82147091\n",
      "Iteration 454, loss = 0.82079345\n",
      "Iteration 455, loss = 0.82011775\n",
      "Iteration 456, loss = 0.81944309\n",
      "Iteration 457, loss = 0.81877031\n",
      "Iteration 458, loss = 0.81809950\n",
      "Iteration 459, loss = 0.81743156\n",
      "Iteration 460, loss = 0.81676420\n",
      "Iteration 461, loss = 0.81609730\n",
      "Iteration 462, loss = 0.81543141\n",
      "Iteration 463, loss = 0.81476654\n",
      "Iteration 464, loss = 0.81410300\n",
      "Iteration 465, loss = 0.81344048\n",
      "Iteration 466, loss = 0.81277899\n",
      "Iteration 467, loss = 0.81211461\n",
      "Iteration 468, loss = 0.81145379\n",
      "Iteration 469, loss = 0.81079361\n",
      "Iteration 470, loss = 0.81013527\n",
      "Iteration 471, loss = 0.80947822\n",
      "Iteration 472, loss = 0.80882241\n",
      "Iteration 473, loss = 0.80816774\n",
      "Iteration 474, loss = 0.80751426\n",
      "Iteration 475, loss = 0.80686169\n",
      "Iteration 476, loss = 0.80620791\n",
      "Iteration 477, loss = 0.80555463\n",
      "Iteration 478, loss = 0.80490380\n",
      "Iteration 479, loss = 0.80425575\n",
      "Iteration 480, loss = 0.80360868\n",
      "Iteration 481, loss = 0.80296213\n",
      "Iteration 482, loss = 0.80231685\n",
      "Iteration 483, loss = 0.80167325\n",
      "Iteration 484, loss = 0.80103035\n",
      "Iteration 485, loss = 0.80039049\n",
      "Iteration 486, loss = 0.79975499\n",
      "Iteration 487, loss = 0.79912184\n",
      "Iteration 488, loss = 0.79849050\n",
      "Iteration 489, loss = 0.79785910\n",
      "Iteration 490, loss = 0.79722886\n",
      "Iteration 491, loss = 0.79659976\n",
      "Iteration 492, loss = 0.79597194\n",
      "Iteration 493, loss = 0.79534499\n",
      "Iteration 494, loss = 0.79471918\n",
      "Iteration 495, loss = 0.79409461\n",
      "Iteration 496, loss = 0.79347038\n",
      "Iteration 497, loss = 0.79284735\n",
      "Iteration 498, loss = 0.79222310\n",
      "Iteration 499, loss = 0.79159982\n",
      "Iteration 500, loss = 0.79097752\n",
      "Iteration 501, loss = 0.79035682\n",
      "Iteration 502, loss = 0.78973868\n",
      "Iteration 503, loss = 0.78912060\n",
      "Iteration 504, loss = 0.78850367\n",
      "Iteration 505, loss = 0.78788714\n",
      "Iteration 506, loss = 0.78726903\n",
      "Iteration 507, loss = 0.78665001\n",
      "Iteration 508, loss = 0.78603217\n",
      "Iteration 509, loss = 0.78541547\n",
      "Iteration 510, loss = 0.78479927\n",
      "Iteration 511, loss = 0.78418458\n",
      "Iteration 512, loss = 0.78357138\n",
      "Iteration 513, loss = 0.78295843\n",
      "Iteration 514, loss = 0.78234779\n",
      "Iteration 515, loss = 0.78173877\n",
      "Iteration 516, loss = 0.78113146\n",
      "Iteration 517, loss = 0.78052537\n",
      "Iteration 518, loss = 0.77992007\n",
      "Iteration 519, loss = 0.77931546\n",
      "Iteration 520, loss = 0.77871261\n",
      "Iteration 521, loss = 0.77811109\n",
      "Iteration 522, loss = 0.77751018\n",
      "Iteration 523, loss = 0.77691065\n",
      "Iteration 524, loss = 0.77631254\n",
      "Iteration 525, loss = 0.77571580\n",
      "Iteration 526, loss = 0.77512007\n",
      "Iteration 527, loss = 0.77452641\n",
      "Iteration 528, loss = 0.77393371\n",
      "Iteration 529, loss = 0.77334234\n",
      "Iteration 530, loss = 0.77275185\n",
      "Iteration 531, loss = 0.77216238\n",
      "Iteration 532, loss = 0.77157470\n",
      "Iteration 533, loss = 0.77098758\n",
      "Iteration 534, loss = 0.77040230\n",
      "Iteration 535, loss = 0.76981764\n",
      "Iteration 536, loss = 0.76923398\n",
      "Iteration 537, loss = 0.76865116\n",
      "Iteration 538, loss = 0.76806828\n",
      "Iteration 539, loss = 0.76748612\n",
      "Iteration 540, loss = 0.76690523\n",
      "Iteration 541, loss = 0.76632498\n",
      "Iteration 542, loss = 0.76574543\n",
      "Iteration 543, loss = 0.76516676\n",
      "Iteration 544, loss = 0.76458933\n",
      "Iteration 545, loss = 0.76401345\n",
      "Iteration 546, loss = 0.76343806\n",
      "Iteration 547, loss = 0.76286320\n",
      "Iteration 548, loss = 0.76229236\n",
      "Iteration 549, loss = 0.76172330\n",
      "Iteration 550, loss = 0.76115600\n",
      "Iteration 551, loss = 0.76058992\n",
      "Iteration 552, loss = 0.76002593\n",
      "Iteration 553, loss = 0.75946341\n",
      "Iteration 554, loss = 0.75890218\n",
      "Iteration 555, loss = 0.75834389\n",
      "Iteration 556, loss = 0.75778666\n",
      "Iteration 557, loss = 0.75723030\n",
      "Iteration 558, loss = 0.75667419\n",
      "Iteration 559, loss = 0.75612061\n",
      "Iteration 560, loss = 0.75556816\n",
      "Iteration 561, loss = 0.75501678\n",
      "Iteration 562, loss = 0.75446705\n",
      "Iteration 563, loss = 0.75391909\n",
      "Iteration 564, loss = 0.75337127\n",
      "Iteration 565, loss = 0.75282433\n",
      "Iteration 566, loss = 0.75227873\n",
      "Iteration 567, loss = 0.75173840\n",
      "Iteration 568, loss = 0.75120057\n",
      "Iteration 569, loss = 0.75066260\n",
      "Iteration 570, loss = 0.75012444\n",
      "Iteration 571, loss = 0.74958489\n",
      "Iteration 572, loss = 0.74904528\n",
      "Iteration 573, loss = 0.74850585\n",
      "Iteration 574, loss = 0.74796746\n",
      "Iteration 575, loss = 0.74742943\n",
      "Iteration 576, loss = 0.74689229\n",
      "Iteration 577, loss = 0.74635565\n",
      "Iteration 578, loss = 0.74581977\n",
      "Iteration 579, loss = 0.74528552\n",
      "Iteration 580, loss = 0.74475200\n",
      "Iteration 581, loss = 0.74421854\n",
      "Iteration 582, loss = 0.74368681\n",
      "Iteration 583, loss = 0.74315619\n",
      "Iteration 584, loss = 0.74262645\n",
      "Iteration 585, loss = 0.74209799\n",
      "Iteration 586, loss = 0.74157034\n",
      "Iteration 587, loss = 0.74104388\n",
      "Iteration 588, loss = 0.74051857\n",
      "Iteration 589, loss = 0.73999438\n",
      "Iteration 590, loss = 0.73947231\n",
      "Iteration 591, loss = 0.73895077\n",
      "Iteration 592, loss = 0.73842970\n",
      "Iteration 593, loss = 0.73791007\n",
      "Iteration 594, loss = 0.73739027\n",
      "Iteration 595, loss = 0.73687236\n",
      "Iteration 596, loss = 0.73635548\n",
      "Iteration 597, loss = 0.73583920\n",
      "Iteration 598, loss = 0.73532388\n",
      "Iteration 599, loss = 0.73480998\n",
      "Iteration 600, loss = 0.73429732\n",
      "Iteration 601, loss = 0.73378665\n",
      "Iteration 602, loss = 0.73327698\n",
      "Iteration 603, loss = 0.73276862\n",
      "Iteration 604, loss = 0.73226145\n",
      "Iteration 605, loss = 0.73175539\n",
      "Iteration 606, loss = 0.73125044\n",
      "Iteration 607, loss = 0.73074687\n",
      "Iteration 608, loss = 0.73024424\n",
      "Iteration 609, loss = 0.72974260\n",
      "Iteration 610, loss = 0.72924195\n",
      "Iteration 611, loss = 0.72874254\n",
      "Iteration 612, loss = 0.72824392\n",
      "Iteration 613, loss = 0.72774506\n",
      "Iteration 614, loss = 0.72724607\n",
      "Iteration 615, loss = 0.72674745\n",
      "Iteration 616, loss = 0.72624963\n",
      "Iteration 617, loss = 0.72575204\n",
      "Iteration 618, loss = 0.72525479\n",
      "Iteration 619, loss = 0.72475847\n",
      "Iteration 620, loss = 0.72426372\n",
      "Iteration 621, loss = 0.72376994\n",
      "Iteration 622, loss = 0.72327682\n",
      "Iteration 623, loss = 0.72278486\n",
      "Iteration 624, loss = 0.72229389\n",
      "Iteration 625, loss = 0.72180446\n",
      "Iteration 626, loss = 0.72131548\n",
      "Iteration 627, loss = 0.72082703\n",
      "Iteration 628, loss = 0.72034004\n",
      "Iteration 629, loss = 0.71985379\n",
      "Iteration 630, loss = 0.71936873\n",
      "Iteration 631, loss = 0.71888460\n",
      "Iteration 632, loss = 0.71840124\n",
      "Iteration 633, loss = 0.71791850\n",
      "Iteration 634, loss = 0.71743649\n",
      "Iteration 635, loss = 0.71695562\n",
      "Iteration 636, loss = 0.71647542\n",
      "Iteration 637, loss = 0.71599622\n",
      "Iteration 638, loss = 0.71551806\n",
      "Iteration 639, loss = 0.71504070\n",
      "Iteration 640, loss = 0.71456147\n",
      "Iteration 641, loss = 0.71407891\n",
      "Iteration 642, loss = 0.71359656\n",
      "Iteration 643, loss = 0.71311493\n",
      "Iteration 644, loss = 0.71263282\n",
      "Iteration 645, loss = 0.71215123\n",
      "Iteration 646, loss = 0.71167066\n",
      "Iteration 647, loss = 0.71119041\n",
      "Iteration 648, loss = 0.71071107\n",
      "Iteration 649, loss = 0.71023212\n",
      "Iteration 650, loss = 0.70975380\n",
      "Iteration 651, loss = 0.70927638\n",
      "Iteration 652, loss = 0.70880044\n",
      "Iteration 653, loss = 0.70832554\n",
      "Iteration 654, loss = 0.70785077\n",
      "Iteration 655, loss = 0.70737732\n",
      "Iteration 656, loss = 0.70690497\n",
      "Iteration 657, loss = 0.70643323\n",
      "Iteration 658, loss = 0.70596144\n",
      "Iteration 659, loss = 0.70548956\n",
      "Iteration 660, loss = 0.70501903\n",
      "Iteration 661, loss = 0.70454860\n",
      "Iteration 662, loss = 0.70407790\n",
      "Iteration 663, loss = 0.70360904\n",
      "Iteration 664, loss = 0.70314036\n",
      "Iteration 665, loss = 0.70267310\n",
      "Iteration 666, loss = 0.70220595\n",
      "Iteration 667, loss = 0.70173940\n",
      "Iteration 668, loss = 0.70127456\n",
      "Iteration 669, loss = 0.70081004\n",
      "Iteration 670, loss = 0.70034574\n",
      "Iteration 671, loss = 0.69988340\n",
      "Iteration 672, loss = 0.69942123\n",
      "Iteration 673, loss = 0.69895966\n",
      "Iteration 674, loss = 0.69849961\n",
      "Iteration 675, loss = 0.69804044\n",
      "Iteration 676, loss = 0.69758253\n",
      "Iteration 677, loss = 0.69712528\n",
      "Iteration 678, loss = 0.69666895\n",
      "Iteration 679, loss = 0.69621331\n",
      "Iteration 680, loss = 0.69575924\n",
      "Iteration 681, loss = 0.69530510\n",
      "Iteration 682, loss = 0.69485359\n",
      "Iteration 683, loss = 0.69440496\n",
      "Iteration 684, loss = 0.69395667\n",
      "Iteration 685, loss = 0.69350979\n",
      "Iteration 686, loss = 0.69306488\n",
      "Iteration 687, loss = 0.69261994\n",
      "Iteration 688, loss = 0.69217650\n",
      "Iteration 689, loss = 0.69173411\n",
      "Iteration 690, loss = 0.69129310\n",
      "Iteration 691, loss = 0.69085220\n",
      "Iteration 692, loss = 0.69041370\n",
      "Iteration 693, loss = 0.68997243\n",
      "Iteration 694, loss = 0.68953050\n",
      "Iteration 695, loss = 0.68908911\n",
      "Iteration 696, loss = 0.68864862\n",
      "Iteration 697, loss = 0.68820815\n",
      "Iteration 698, loss = 0.68776815\n",
      "Iteration 699, loss = 0.68732841\n",
      "Iteration 700, loss = 0.68689078\n",
      "Iteration 701, loss = 0.68645358\n",
      "Iteration 702, loss = 0.68601693\n",
      "Iteration 703, loss = 0.68558092\n",
      "Iteration 704, loss = 0.68514547\n",
      "Iteration 705, loss = 0.68471042\n",
      "Iteration 706, loss = 0.68427602\n",
      "Iteration 707, loss = 0.68384241\n",
      "Iteration 708, loss = 0.68340913\n",
      "Iteration 709, loss = 0.68297594\n",
      "Iteration 710, loss = 0.68254433\n",
      "Iteration 711, loss = 0.68211308\n",
      "Iteration 712, loss = 0.68168217\n",
      "Iteration 713, loss = 0.68125218\n",
      "Iteration 714, loss = 0.68082319\n",
      "Iteration 715, loss = 0.68039488\n",
      "Iteration 716, loss = 0.67996734\n",
      "Iteration 717, loss = 0.67954076\n",
      "Iteration 718, loss = 0.67911482\n",
      "Iteration 719, loss = 0.67868940\n",
      "Iteration 720, loss = 0.67826499\n",
      "Iteration 721, loss = 0.67784114\n",
      "Iteration 722, loss = 0.67741837\n",
      "Iteration 723, loss = 0.67699613\n",
      "Iteration 724, loss = 0.67657399\n",
      "Iteration 725, loss = 0.67615372\n",
      "Iteration 726, loss = 0.67573538\n",
      "Iteration 727, loss = 0.67531747\n",
      "Iteration 728, loss = 0.67489999\n",
      "Iteration 729, loss = 0.67448300\n",
      "Iteration 730, loss = 0.67406685\n",
      "Iteration 731, loss = 0.67365280\n",
      "Iteration 732, loss = 0.67323885\n",
      "Iteration 733, loss = 0.67282322\n",
      "Iteration 734, loss = 0.67240718\n",
      "Iteration 735, loss = 0.67199027\n",
      "Iteration 736, loss = 0.67157337\n",
      "Iteration 737, loss = 0.67115686\n",
      "Iteration 738, loss = 0.67074070\n",
      "Iteration 739, loss = 0.67032445\n",
      "Iteration 740, loss = 0.66990822\n",
      "Iteration 741, loss = 0.66949292\n",
      "Iteration 742, loss = 0.66907747\n",
      "Iteration 743, loss = 0.66866278\n",
      "Iteration 744, loss = 0.66824765\n",
      "Iteration 745, loss = 0.66783317\n",
      "Iteration 746, loss = 0.66741920\n",
      "Iteration 747, loss = 0.66700516\n",
      "Iteration 748, loss = 0.66659155\n",
      "Iteration 749, loss = 0.66617870\n",
      "Iteration 750, loss = 0.66576520\n",
      "Iteration 751, loss = 0.66535167\n",
      "Iteration 752, loss = 0.66493826\n",
      "Iteration 753, loss = 0.66452484\n",
      "Iteration 754, loss = 0.66411067\n",
      "Iteration 755, loss = 0.66369582\n",
      "Iteration 756, loss = 0.66328143\n",
      "Iteration 757, loss = 0.66286738\n",
      "Iteration 758, loss = 0.66245358\n",
      "Iteration 759, loss = 0.66204031\n",
      "Iteration 760, loss = 0.66162733\n",
      "Iteration 761, loss = 0.66121473\n",
      "Iteration 762, loss = 0.66080274\n",
      "Iteration 763, loss = 0.66039115\n",
      "Iteration 764, loss = 0.65998027\n",
      "Iteration 765, loss = 0.65956987\n",
      "Iteration 766, loss = 0.65916015\n",
      "Iteration 767, loss = 0.65875119\n",
      "Iteration 768, loss = 0.65834250\n",
      "Iteration 769, loss = 0.65793400\n",
      "Iteration 770, loss = 0.65752566\n",
      "Iteration 771, loss = 0.65711800\n",
      "Iteration 772, loss = 0.65671104\n",
      "Iteration 773, loss = 0.65630452\n",
      "Iteration 774, loss = 0.65589854\n",
      "Iteration 775, loss = 0.65549326\n",
      "Iteration 776, loss = 0.65508813\n",
      "Iteration 777, loss = 0.65468351\n",
      "Iteration 778, loss = 0.65427967\n",
      "Iteration 779, loss = 0.65387664\n",
      "Iteration 780, loss = 0.65347421\n",
      "Iteration 781, loss = 0.65307220\n",
      "Iteration 782, loss = 0.65267282\n",
      "Iteration 783, loss = 0.65227416\n",
      "Iteration 784, loss = 0.65187667\n",
      "Iteration 785, loss = 0.65147977\n",
      "Iteration 786, loss = 0.65108344\n",
      "Iteration 787, loss = 0.65068765\n",
      "Iteration 788, loss = 0.65029257\n",
      "Iteration 789, loss = 0.64989831\n",
      "Iteration 790, loss = 0.64950514\n",
      "Iteration 791, loss = 0.64911199\n",
      "Iteration 792, loss = 0.64871926\n",
      "Iteration 793, loss = 0.64832771\n",
      "Iteration 794, loss = 0.64793712\n",
      "Iteration 795, loss = 0.64754926\n",
      "Iteration 796, loss = 0.64716212\n",
      "Iteration 797, loss = 0.64677613\n",
      "Iteration 798, loss = 0.64639036\n",
      "Iteration 799, loss = 0.64600525\n",
      "Iteration 800, loss = 0.64562063\n",
      "Iteration 801, loss = 0.64523623\n",
      "Iteration 802, loss = 0.64485230\n",
      "Iteration 803, loss = 0.64446844\n",
      "Iteration 804, loss = 0.64408529\n",
      "Iteration 805, loss = 0.64370358\n",
      "Iteration 806, loss = 0.64332195\n",
      "Iteration 807, loss = 0.64294059\n",
      "Iteration 808, loss = 0.64255942\n",
      "Iteration 809, loss = 0.64217812\n",
      "Iteration 810, loss = 0.64179725\n",
      "Iteration 811, loss = 0.64141690\n",
      "Iteration 812, loss = 0.64103691\n",
      "Iteration 813, loss = 0.64065719\n",
      "Iteration 814, loss = 0.64027775\n",
      "Iteration 815, loss = 0.63989883\n",
      "Iteration 816, loss = 0.63952045\n",
      "Iteration 817, loss = 0.63914232\n",
      "Iteration 818, loss = 0.63876507\n",
      "Iteration 819, loss = 0.63838852\n",
      "Iteration 820, loss = 0.63801231\n",
      "Iteration 821, loss = 0.63763663\n",
      "Iteration 822, loss = 0.63726204\n",
      "Iteration 823, loss = 0.63688783\n",
      "Iteration 824, loss = 0.63651435\n",
      "Iteration 825, loss = 0.63614149\n",
      "Iteration 826, loss = 0.63576912\n",
      "Iteration 827, loss = 0.63539750\n",
      "Iteration 828, loss = 0.63502685\n",
      "Iteration 829, loss = 0.63465700\n",
      "Iteration 830, loss = 0.63428722\n",
      "Iteration 831, loss = 0.63391871\n",
      "Iteration 832, loss = 0.63355064\n",
      "Iteration 833, loss = 0.63318286\n",
      "Iteration 834, loss = 0.63281547\n",
      "Iteration 835, loss = 0.63244905\n",
      "Iteration 836, loss = 0.63208294\n",
      "Iteration 837, loss = 0.63171725\n",
      "Iteration 838, loss = 0.63135216\n",
      "Iteration 839, loss = 0.63098735\n",
      "Iteration 840, loss = 0.63062332\n",
      "Iteration 841, loss = 0.63025930\n",
      "Iteration 842, loss = 0.62989643\n",
      "Iteration 843, loss = 0.62953414\n",
      "Iteration 844, loss = 0.62917192\n",
      "Iteration 845, loss = 0.62881037\n",
      "Iteration 846, loss = 0.62844908\n",
      "Iteration 847, loss = 0.62808826\n",
      "Iteration 848, loss = 0.62772817\n",
      "Iteration 849, loss = 0.62736855\n",
      "Iteration 850, loss = 0.62700951\n",
      "Iteration 851, loss = 0.62665102\n",
      "Iteration 852, loss = 0.62629301\n",
      "Iteration 853, loss = 0.62593581\n",
      "Iteration 854, loss = 0.62557889\n",
      "Iteration 855, loss = 0.62522237\n",
      "Iteration 856, loss = 0.62486649\n",
      "Iteration 857, loss = 0.62451224\n",
      "Iteration 858, loss = 0.62415878\n",
      "Iteration 859, loss = 0.62380568\n",
      "Iteration 860, loss = 0.62345254\n",
      "Iteration 861, loss = 0.62309991\n",
      "Iteration 862, loss = 0.62274723\n",
      "Iteration 863, loss = 0.62239506\n",
      "Iteration 864, loss = 0.62204357\n",
      "Iteration 865, loss = 0.62169226\n",
      "Iteration 866, loss = 0.62134133\n",
      "Iteration 867, loss = 0.62099129\n",
      "Iteration 868, loss = 0.62064157\n",
      "Iteration 869, loss = 0.62029202\n",
      "Iteration 870, loss = 0.61994348\n",
      "Iteration 871, loss = 0.61959534\n",
      "Iteration 872, loss = 0.61924751\n",
      "Iteration 873, loss = 0.61890046\n",
      "Iteration 874, loss = 0.61855399\n",
      "Iteration 875, loss = 0.61820766\n",
      "Iteration 876, loss = 0.61786198\n",
      "Iteration 877, loss = 0.61751705\n",
      "Iteration 878, loss = 0.61717223\n",
      "Iteration 879, loss = 0.61682835\n",
      "Iteration 880, loss = 0.61648457\n",
      "Iteration 881, loss = 0.61614107\n",
      "Iteration 882, loss = 0.61579832\n",
      "Iteration 883, loss = 0.61545626\n",
      "Iteration 884, loss = 0.61511496\n",
      "Iteration 885, loss = 0.61477295\n",
      "Iteration 886, loss = 0.61443148\n",
      "Iteration 887, loss = 0.61409064\n",
      "Iteration 888, loss = 0.61375037\n",
      "Iteration 889, loss = 0.61341057\n",
      "Iteration 890, loss = 0.61307115\n",
      "Iteration 891, loss = 0.61273223\n",
      "Iteration 892, loss = 0.61239361\n",
      "Iteration 893, loss = 0.61205547\n",
      "Iteration 894, loss = 0.61171818\n",
      "Iteration 895, loss = 0.61138128\n",
      "Iteration 896, loss = 0.61104448\n",
      "Iteration 897, loss = 0.61070847\n",
      "Iteration 898, loss = 0.61037335\n",
      "Iteration 899, loss = 0.61003823\n",
      "Iteration 900, loss = 0.60970403\n",
      "Iteration 901, loss = 0.60937062\n",
      "Iteration 902, loss = 0.60903767\n",
      "Iteration 903, loss = 0.60870507\n",
      "Iteration 904, loss = 0.60837218\n",
      "Iteration 905, loss = 0.60803937\n",
      "Iteration 906, loss = 0.60770751\n",
      "Iteration 907, loss = 0.60737608\n",
      "Iteration 908, loss = 0.60704504\n",
      "Iteration 909, loss = 0.60671447\n",
      "Iteration 910, loss = 0.60638411\n",
      "Iteration 911, loss = 0.60605440\n",
      "Iteration 912, loss = 0.60572520\n",
      "Iteration 913, loss = 0.60539625\n",
      "Iteration 914, loss = 0.60506694\n",
      "Iteration 915, loss = 0.60473738\n",
      "Iteration 916, loss = 0.60440810\n",
      "Iteration 917, loss = 0.60407855\n",
      "Iteration 918, loss = 0.60374925\n",
      "Iteration 919, loss = 0.60342003\n",
      "Iteration 920, loss = 0.60309073\n",
      "Iteration 921, loss = 0.60276158\n",
      "Iteration 922, loss = 0.60243239\n",
      "Iteration 923, loss = 0.60210311\n",
      "Iteration 924, loss = 0.60177371\n",
      "Iteration 925, loss = 0.60144441\n",
      "Iteration 926, loss = 0.60111468\n",
      "Iteration 927, loss = 0.60078476\n",
      "Iteration 928, loss = 0.60045488\n",
      "Iteration 929, loss = 0.60012428\n",
      "Iteration 930, loss = 0.59979384\n",
      "Iteration 931, loss = 0.59946484\n",
      "Iteration 932, loss = 0.59913598\n",
      "Iteration 933, loss = 0.59880720\n",
      "Iteration 934, loss = 0.59847861\n",
      "Iteration 935, loss = 0.59815077\n",
      "Iteration 936, loss = 0.59782304\n",
      "Iteration 937, loss = 0.59749470\n",
      "Iteration 938, loss = 0.59716515\n",
      "Iteration 939, loss = 0.59683553\n",
      "Iteration 940, loss = 0.59650615\n",
      "Iteration 941, loss = 0.59617820\n",
      "Iteration 942, loss = 0.59584996\n",
      "Iteration 943, loss = 0.59552188\n",
      "Iteration 944, loss = 0.59519437\n",
      "Iteration 945, loss = 0.59486783\n",
      "Iteration 946, loss = 0.59454250\n",
      "Iteration 947, loss = 0.59421754\n",
      "Iteration 948, loss = 0.59389319\n",
      "Iteration 949, loss = 0.59356777\n",
      "Iteration 950, loss = 0.59324122\n",
      "Iteration 951, loss = 0.59291488\n",
      "Iteration 952, loss = 0.59258852\n",
      "Iteration 953, loss = 0.59226222\n",
      "Iteration 954, loss = 0.59193597\n",
      "Iteration 955, loss = 0.59161242\n",
      "Iteration 956, loss = 0.59129001\n",
      "Iteration 957, loss = 0.59096806\n",
      "Iteration 958, loss = 0.59064645\n",
      "Iteration 959, loss = 0.59032516\n",
      "Iteration 960, loss = 0.59000425\n",
      "Iteration 961, loss = 0.58968456\n",
      "Iteration 962, loss = 0.58936486\n",
      "Iteration 963, loss = 0.58904592\n",
      "Iteration 964, loss = 0.58872803\n",
      "Iteration 965, loss = 0.58841111\n",
      "Iteration 966, loss = 0.58809465\n",
      "Iteration 967, loss = 0.58777874\n",
      "Iteration 968, loss = 0.58746336\n",
      "Iteration 969, loss = 0.58714805\n",
      "Iteration 970, loss = 0.58683334\n",
      "Iteration 971, loss = 0.58651918\n",
      "Iteration 972, loss = 0.58620556\n",
      "Iteration 973, loss = 0.58589246\n",
      "Iteration 974, loss = 0.58558018\n",
      "Iteration 975, loss = 0.58526822\n",
      "Iteration 976, loss = 0.58495673\n",
      "Iteration 977, loss = 0.58464563\n",
      "Iteration 978, loss = 0.58433604\n",
      "Iteration 979, loss = 0.58402708\n",
      "Iteration 980, loss = 0.58371848\n",
      "Iteration 981, loss = 0.58341067\n",
      "Iteration 982, loss = 0.58310311\n",
      "Iteration 983, loss = 0.58279607\n",
      "Iteration 984, loss = 0.58248971\n",
      "Iteration 985, loss = 0.58218370\n",
      "Iteration 986, loss = 0.58187825\n",
      "Iteration 987, loss = 0.58157324\n",
      "Iteration 988, loss = 0.58126881\n",
      "Iteration 989, loss = 0.58096467\n",
      "Iteration 990, loss = 0.58066122\n",
      "Iteration 991, loss = 0.58035807\n",
      "Iteration 992, loss = 0.58005531\n",
      "Iteration 993, loss = 0.57975215\n",
      "Iteration 994, loss = 0.57944817\n",
      "Iteration 995, loss = 0.57914432\n",
      "Iteration 996, loss = 0.57884010\n",
      "Iteration 997, loss = 0.57853629\n",
      "Iteration 998, loss = 0.57823260\n",
      "Iteration 999, loss = 0.57792872\n",
      "Iteration 1000, loss = 0.57762531\n",
      "Iteration 1001, loss = 0.57732204\n",
      "Iteration 1002, loss = 0.57701881\n",
      "Iteration 1003, loss = 0.57671569\n",
      "Iteration 1004, loss = 0.57641166\n",
      "Iteration 1005, loss = 0.57610784\n",
      "Iteration 1006, loss = 0.57580441\n",
      "Iteration 1007, loss = 0.57550103\n",
      "Iteration 1008, loss = 0.57519766\n",
      "Iteration 1009, loss = 0.57489452\n",
      "Iteration 1010, loss = 0.57459194\n",
      "Iteration 1011, loss = 0.57428941\n",
      "Iteration 1012, loss = 0.57398732\n",
      "Iteration 1013, loss = 0.57368515\n",
      "Iteration 1014, loss = 0.57338358\n",
      "Iteration 1015, loss = 0.57308231\n",
      "Iteration 1016, loss = 0.57278153\n",
      "Iteration 1017, loss = 0.57248092\n",
      "Iteration 1018, loss = 0.57218170\n",
      "Iteration 1019, loss = 0.57188455\n",
      "Iteration 1020, loss = 0.57158810\n",
      "Iteration 1021, loss = 0.57129208\n",
      "Iteration 1022, loss = 0.57099661\n",
      "Iteration 1023, loss = 0.57070135\n",
      "Iteration 1024, loss = 0.57040663\n",
      "Iteration 1025, loss = 0.57011258\n",
      "Iteration 1026, loss = 0.56981887\n",
      "Iteration 1027, loss = 0.56952593\n",
      "Iteration 1028, loss = 0.56923354\n",
      "Iteration 1029, loss = 0.56894119\n",
      "Iteration 1030, loss = 0.56864948\n",
      "Iteration 1031, loss = 0.56835829\n",
      "Iteration 1032, loss = 0.56806751\n",
      "Iteration 1033, loss = 0.56777737\n",
      "Iteration 1034, loss = 0.56748770\n",
      "Iteration 1035, loss = 0.56719857\n",
      "Iteration 1036, loss = 0.56690935\n",
      "Iteration 1037, loss = 0.56661995\n",
      "Iteration 1038, loss = 0.56633050\n",
      "Iteration 1039, loss = 0.56604126\n",
      "Iteration 1040, loss = 0.56575246\n",
      "Iteration 1041, loss = 0.56546394\n",
      "Iteration 1042, loss = 0.56517547\n",
      "Iteration 1043, loss = 0.56488732\n",
      "Iteration 1044, loss = 0.56459966\n",
      "Iteration 1045, loss = 0.56431237\n",
      "Iteration 1046, loss = 0.56402678\n",
      "Iteration 1047, loss = 0.56374278\n",
      "Iteration 1048, loss = 0.56345886\n",
      "Iteration 1049, loss = 0.56317495\n",
      "Iteration 1050, loss = 0.56289199\n",
      "Iteration 1051, loss = 0.56260930\n",
      "Iteration 1052, loss = 0.56232750\n",
      "Iteration 1053, loss = 0.56204584\n",
      "Iteration 1054, loss = 0.56176429\n",
      "Iteration 1055, loss = 0.56148127\n",
      "Iteration 1056, loss = 0.56119834\n",
      "Iteration 1057, loss = 0.56091519\n",
      "Iteration 1058, loss = 0.56063075\n",
      "Iteration 1059, loss = 0.56034563\n",
      "Iteration 1060, loss = 0.56006079\n",
      "Iteration 1061, loss = 0.55977580\n",
      "Iteration 1062, loss = 0.55949064\n",
      "Iteration 1063, loss = 0.55920553\n",
      "Iteration 1064, loss = 0.55892181\n",
      "Iteration 1065, loss = 0.55863890\n",
      "Iteration 1066, loss = 0.55835616\n",
      "Iteration 1067, loss = 0.55807369\n",
      "Iteration 1068, loss = 0.55779199\n",
      "Iteration 1069, loss = 0.55751059\n",
      "Iteration 1070, loss = 0.55722921\n",
      "Iteration 1071, loss = 0.55694815\n",
      "Iteration 1072, loss = 0.55666766\n",
      "Iteration 1073, loss = 0.55638757\n",
      "Iteration 1074, loss = 0.55610753\n",
      "Iteration 1075, loss = 0.55582802\n",
      "Iteration 1076, loss = 0.55554880\n",
      "Iteration 1077, loss = 0.55527010\n",
      "Iteration 1078, loss = 0.55499174\n",
      "Iteration 1079, loss = 0.55471338\n",
      "Iteration 1080, loss = 0.55443537\n",
      "Iteration 1081, loss = 0.55415761\n",
      "Iteration 1082, loss = 0.55388028\n",
      "Iteration 1083, loss = 0.55360297\n",
      "Iteration 1084, loss = 0.55332623\n",
      "Iteration 1085, loss = 0.55305006\n",
      "Iteration 1086, loss = 0.55277422\n",
      "Iteration 1087, loss = 0.55249856\n",
      "Iteration 1088, loss = 0.55222347\n",
      "Iteration 1089, loss = 0.55194868\n",
      "Iteration 1090, loss = 0.55167474\n",
      "Iteration 1091, loss = 0.55140174\n",
      "Iteration 1092, loss = 0.55112916\n",
      "Iteration 1093, loss = 0.55085713\n",
      "Iteration 1094, loss = 0.55058525\n",
      "Iteration 1095, loss = 0.55031380\n",
      "Iteration 1096, loss = 0.55004291\n",
      "Iteration 1097, loss = 0.54977218\n",
      "Iteration 1098, loss = 0.54950165\n",
      "Iteration 1099, loss = 0.54923177\n",
      "Iteration 1100, loss = 0.54896230\n",
      "Iteration 1101, loss = 0.54869294\n",
      "Iteration 1102, loss = 0.54842358\n",
      "Iteration 1103, loss = 0.54815449\n",
      "Iteration 1104, loss = 0.54788589\n",
      "Iteration 1105, loss = 0.54761753\n",
      "Iteration 1106, loss = 0.54734921\n",
      "Iteration 1107, loss = 0.54708126\n",
      "Iteration 1108, loss = 0.54681413\n",
      "Iteration 1109, loss = 0.54654713\n",
      "Iteration 1110, loss = 0.54628014\n",
      "Iteration 1111, loss = 0.54601343\n",
      "Iteration 1112, loss = 0.54574719\n",
      "Iteration 1113, loss = 0.54548169\n",
      "Iteration 1114, loss = 0.54521656\n",
      "Iteration 1115, loss = 0.54495129\n",
      "Iteration 1116, loss = 0.54468657\n",
      "Iteration 1117, loss = 0.54442234\n",
      "Iteration 1118, loss = 0.54415839\n",
      "Iteration 1119, loss = 0.54389461\n",
      "Iteration 1120, loss = 0.54363086\n",
      "Iteration 1121, loss = 0.54336799\n",
      "Iteration 1122, loss = 0.54310567\n",
      "Iteration 1123, loss = 0.54284377\n",
      "Iteration 1124, loss = 0.54258221\n",
      "Iteration 1125, loss = 0.54232113\n",
      "Iteration 1126, loss = 0.54206060\n",
      "Iteration 1127, loss = 0.54180050\n",
      "Iteration 1128, loss = 0.54154063\n",
      "Iteration 1129, loss = 0.54128124\n",
      "Iteration 1130, loss = 0.54102202\n",
      "Iteration 1131, loss = 0.54076362\n",
      "Iteration 1132, loss = 0.54050575\n",
      "Iteration 1133, loss = 0.54024800\n",
      "Iteration 1134, loss = 0.53999057\n",
      "Iteration 1135, loss = 0.53973361\n",
      "Iteration 1136, loss = 0.53947694\n",
      "Iteration 1137, loss = 0.53922114\n",
      "Iteration 1138, loss = 0.53896571\n",
      "Iteration 1139, loss = 0.53871052\n",
      "Iteration 1140, loss = 0.53845560\n",
      "Iteration 1141, loss = 0.53820112\n",
      "Iteration 1142, loss = 0.53794687\n",
      "Iteration 1143, loss = 0.53769282\n",
      "Iteration 1144, loss = 0.53743909\n",
      "Iteration 1145, loss = 0.53718569\n",
      "Iteration 1146, loss = 0.53693246\n",
      "Iteration 1147, loss = 0.53667956\n",
      "Iteration 1148, loss = 0.53642733\n",
      "Iteration 1149, loss = 0.53617557\n",
      "Iteration 1150, loss = 0.53592382\n",
      "Iteration 1151, loss = 0.53567247\n",
      "Iteration 1152, loss = 0.53542155\n",
      "Iteration 1153, loss = 0.53517083\n",
      "Iteration 1154, loss = 0.53492032\n",
      "Iteration 1155, loss = 0.53466990\n",
      "Iteration 1156, loss = 0.53441945\n",
      "Iteration 1157, loss = 0.53416890\n",
      "Iteration 1158, loss = 0.53391884\n",
      "Iteration 1159, loss = 0.53366912\n",
      "Iteration 1160, loss = 0.53341998\n",
      "Iteration 1161, loss = 0.53317121\n",
      "Iteration 1162, loss = 0.53292434\n",
      "Iteration 1163, loss = 0.53267856\n",
      "Iteration 1164, loss = 0.53243323\n",
      "Iteration 1165, loss = 0.53218819\n",
      "Iteration 1166, loss = 0.53194355\n",
      "Iteration 1167, loss = 0.53169891\n",
      "Iteration 1168, loss = 0.53145435\n",
      "Iteration 1169, loss = 0.53120990\n",
      "Iteration 1170, loss = 0.53096588\n",
      "Iteration 1171, loss = 0.53072225\n",
      "Iteration 1172, loss = 0.53047858\n",
      "Iteration 1173, loss = 0.53023531\n",
      "Iteration 1174, loss = 0.52999233\n",
      "Iteration 1175, loss = 0.52975006\n",
      "Iteration 1176, loss = 0.52950870\n",
      "Iteration 1177, loss = 0.52926720\n",
      "Iteration 1178, loss = 0.52902551\n",
      "Iteration 1179, loss = 0.52878429\n",
      "Iteration 1180, loss = 0.52854337\n",
      "Iteration 1181, loss = 0.52830259\n",
      "Iteration 1182, loss = 0.52806207\n",
      "Iteration 1183, loss = 0.52782176\n",
      "Iteration 1184, loss = 0.52758098\n",
      "Iteration 1185, loss = 0.52734027\n",
      "Iteration 1186, loss = 0.52709981\n",
      "Iteration 1187, loss = 0.52685942\n",
      "Iteration 1188, loss = 0.52661907\n",
      "Iteration 1189, loss = 0.52637923\n",
      "Iteration 1190, loss = 0.52613943\n",
      "Iteration 1191, loss = 0.52589982\n",
      "Iteration 1192, loss = 0.52566065\n",
      "Iteration 1193, loss = 0.52542156\n",
      "Iteration 1194, loss = 0.52518268\n",
      "Iteration 1195, loss = 0.52494415\n",
      "Iteration 1196, loss = 0.52470577\n",
      "Iteration 1197, loss = 0.52446782\n",
      "Iteration 1198, loss = 0.52423006\n",
      "Iteration 1199, loss = 0.52399258\n",
      "Iteration 1200, loss = 0.52375597\n",
      "Iteration 1201, loss = 0.52351958\n",
      "Iteration 1202, loss = 0.52328323\n",
      "Iteration 1203, loss = 0.52304707\n",
      "Iteration 1204, loss = 0.52281080\n",
      "Iteration 1205, loss = 0.52257501\n",
      "Iteration 1206, loss = 0.52233937\n",
      "Iteration 1207, loss = 0.52210363\n",
      "Iteration 1208, loss = 0.52186808\n",
      "Iteration 1209, loss = 0.52163281\n",
      "Iteration 1210, loss = 0.52139776\n",
      "Iteration 1211, loss = 0.52116286\n",
      "Iteration 1212, loss = 0.52092818\n",
      "Iteration 1213, loss = 0.52069395\n",
      "Iteration 1214, loss = 0.52045992\n",
      "Iteration 1215, loss = 0.52022594\n",
      "Iteration 1216, loss = 0.51999220\n",
      "Iteration 1217, loss = 0.51975890\n",
      "Iteration 1218, loss = 0.51952585\n",
      "Iteration 1219, loss = 0.51929305\n",
      "Iteration 1220, loss = 0.51906053\n",
      "Iteration 1221, loss = 0.51882842\n",
      "Iteration 1222, loss = 0.51859656\n",
      "Iteration 1223, loss = 0.51836491\n",
      "Iteration 1224, loss = 0.51813334\n",
      "Iteration 1225, loss = 0.51790216\n",
      "Iteration 1226, loss = 0.51767113\n",
      "Iteration 1227, loss = 0.51744134\n",
      "Iteration 1228, loss = 0.51721207\n",
      "Iteration 1229, loss = 0.51698310\n",
      "Iteration 1230, loss = 0.51675426\n",
      "Iteration 1231, loss = 0.51652566\n",
      "Iteration 1232, loss = 0.51629705\n",
      "Iteration 1233, loss = 0.51606867\n",
      "Iteration 1234, loss = 0.51584046\n",
      "Iteration 1235, loss = 0.51561242\n",
      "Iteration 1236, loss = 0.51538457\n",
      "Iteration 1237, loss = 0.51515679\n",
      "Iteration 1238, loss = 0.51492968\n",
      "Iteration 1239, loss = 0.51470224\n",
      "Iteration 1240, loss = 0.51447509\n",
      "Iteration 1241, loss = 0.51424803\n",
      "Iteration 1242, loss = 0.51402067\n",
      "Iteration 1243, loss = 0.51379335\n",
      "Iteration 1244, loss = 0.51356614\n",
      "Iteration 1245, loss = 0.51333902\n",
      "Iteration 1246, loss = 0.51311208\n",
      "Iteration 1247, loss = 0.51288532\n",
      "Iteration 1248, loss = 0.51265851\n",
      "Iteration 1249, loss = 0.51243178\n",
      "Iteration 1250, loss = 0.51220547\n",
      "Iteration 1251, loss = 0.51197920\n",
      "Iteration 1252, loss = 0.51175341\n",
      "Iteration 1253, loss = 0.51152783\n",
      "Iteration 1254, loss = 0.51130234\n",
      "Iteration 1255, loss = 0.51107712\n",
      "Iteration 1256, loss = 0.51085202\n",
      "Iteration 1257, loss = 0.51062719\n",
      "Iteration 1258, loss = 0.51040252\n",
      "Iteration 1259, loss = 0.51017817\n",
      "Iteration 1260, loss = 0.50995411\n",
      "Iteration 1261, loss = 0.50973025\n",
      "Iteration 1262, loss = 0.50950640\n",
      "Iteration 1263, loss = 0.50928296\n",
      "Iteration 1264, loss = 0.50906010\n",
      "Iteration 1265, loss = 0.50883730\n",
      "Iteration 1266, loss = 0.50861442\n",
      "Iteration 1267, loss = 0.50839201\n",
      "Iteration 1268, loss = 0.50816991\n",
      "Iteration 1269, loss = 0.50794790\n",
      "Iteration 1270, loss = 0.50772624\n",
      "Iteration 1271, loss = 0.50750495\n",
      "Iteration 1272, loss = 0.50728377\n",
      "Iteration 1273, loss = 0.50706287\n",
      "Iteration 1274, loss = 0.50684204\n",
      "Iteration 1275, loss = 0.50662142\n",
      "Iteration 1276, loss = 0.50640134\n",
      "Iteration 1277, loss = 0.50618140\n",
      "Iteration 1278, loss = 0.50596183\n",
      "Iteration 1279, loss = 0.50574230\n",
      "Iteration 1280, loss = 0.50552294\n",
      "Iteration 1281, loss = 0.50530388\n",
      "Iteration 1282, loss = 0.50508495\n",
      "Iteration 1283, loss = 0.50486611\n",
      "Iteration 1284, loss = 0.50464744\n",
      "Iteration 1285, loss = 0.50442902\n",
      "Iteration 1286, loss = 0.50421089\n",
      "Iteration 1287, loss = 0.50399273\n",
      "Iteration 1288, loss = 0.50377461\n",
      "Iteration 1289, loss = 0.50355660\n",
      "Iteration 1290, loss = 0.50333920\n",
      "Iteration 1291, loss = 0.50312175\n",
      "Iteration 1292, loss = 0.50290425\n",
      "Iteration 1293, loss = 0.50268711\n",
      "Iteration 1294, loss = 0.50247040\n",
      "Iteration 1295, loss = 0.50225388\n",
      "Iteration 1296, loss = 0.50203750\n",
      "Iteration 1297, loss = 0.50182134\n",
      "Iteration 1298, loss = 0.50160529\n",
      "Iteration 1299, loss = 0.50138944\n",
      "Iteration 1300, loss = 0.50117398\n",
      "Iteration 1301, loss = 0.50095865\n",
      "Iteration 1302, loss = 0.50074336\n",
      "Iteration 1303, loss = 0.50052806\n",
      "Iteration 1304, loss = 0.50031231\n",
      "Iteration 1305, loss = 0.50009662\n",
      "Iteration 1306, loss = 0.49988090\n",
      "Iteration 1307, loss = 0.49966521\n",
      "Iteration 1308, loss = 0.49944956\n",
      "Iteration 1309, loss = 0.49923396\n",
      "Iteration 1310, loss = 0.49901827\n",
      "Iteration 1311, loss = 0.49880280\n",
      "Iteration 1312, loss = 0.49858737\n",
      "Iteration 1313, loss = 0.49837315\n",
      "Iteration 1314, loss = 0.49815925\n",
      "Iteration 1315, loss = 0.49794542\n",
      "Iteration 1316, loss = 0.49773175\n",
      "Iteration 1317, loss = 0.49751823\n",
      "Iteration 1318, loss = 0.49730513\n",
      "Iteration 1319, loss = 0.49709234\n",
      "Iteration 1320, loss = 0.49687974\n",
      "Iteration 1321, loss = 0.49666750\n",
      "Iteration 1322, loss = 0.49645547\n",
      "Iteration 1323, loss = 0.49624375\n",
      "Iteration 1324, loss = 0.49603225\n",
      "Iteration 1325, loss = 0.49582085\n",
      "Iteration 1326, loss = 0.49560986\n",
      "Iteration 1327, loss = 0.49539909\n",
      "Iteration 1328, loss = 0.49518876\n",
      "Iteration 1329, loss = 0.49497860\n",
      "Iteration 1330, loss = 0.49476857\n",
      "Iteration 1331, loss = 0.49455901\n",
      "Iteration 1332, loss = 0.49434955\n",
      "Iteration 1333, loss = 0.49414000\n",
      "Iteration 1334, loss = 0.49392975\n",
      "Iteration 1335, loss = 0.49371888\n",
      "Iteration 1336, loss = 0.49350768\n",
      "Iteration 1337, loss = 0.49329630\n",
      "Iteration 1338, loss = 0.49308485\n",
      "Iteration 1339, loss = 0.49287358\n",
      "Iteration 1340, loss = 0.49266213\n",
      "Iteration 1341, loss = 0.49245070\n",
      "Iteration 1342, loss = 0.49223964\n",
      "Iteration 1343, loss = 0.49202848\n",
      "Iteration 1344, loss = 0.49181722\n",
      "Iteration 1345, loss = 0.49160671\n",
      "Iteration 1346, loss = 0.49139761\n",
      "Iteration 1347, loss = 0.49118880\n",
      "Iteration 1348, loss = 0.49098007\n",
      "Iteration 1349, loss = 0.49077105\n",
      "Iteration 1350, loss = 0.49056201\n",
      "Iteration 1351, loss = 0.49035280\n",
      "Iteration 1352, loss = 0.49014378\n",
      "Iteration 1353, loss = 0.48993456\n",
      "Iteration 1354, loss = 0.48972540\n",
      "Iteration 1355, loss = 0.48951620\n",
      "Iteration 1356, loss = 0.48930702\n",
      "Iteration 1357, loss = 0.48909773\n",
      "Iteration 1358, loss = 0.48888921\n",
      "Iteration 1359, loss = 0.48868163\n",
      "Iteration 1360, loss = 0.48847435\n",
      "Iteration 1361, loss = 0.48826712\n",
      "Iteration 1362, loss = 0.48806005\n",
      "Iteration 1363, loss = 0.48785290\n",
      "Iteration 1364, loss = 0.48764576\n",
      "Iteration 1365, loss = 0.48743872\n",
      "Iteration 1366, loss = 0.48723168\n",
      "Iteration 1367, loss = 0.48702481\n",
      "Iteration 1368, loss = 0.48681769\n",
      "Iteration 1369, loss = 0.48661086\n",
      "Iteration 1370, loss = 0.48640399\n",
      "Iteration 1371, loss = 0.48619754\n",
      "Iteration 1372, loss = 0.48599122\n",
      "Iteration 1373, loss = 0.48578493\n",
      "Iteration 1374, loss = 0.48557884\n",
      "Iteration 1375, loss = 0.48537366\n",
      "Iteration 1376, loss = 0.48516861\n",
      "Iteration 1377, loss = 0.48496377\n",
      "Iteration 1378, loss = 0.48475905\n",
      "Iteration 1379, loss = 0.48455449\n",
      "Iteration 1380, loss = 0.48435019\n",
      "Iteration 1381, loss = 0.48414612\n",
      "Iteration 1382, loss = 0.48394227\n",
      "Iteration 1383, loss = 0.48373853\n",
      "Iteration 1384, loss = 0.48353502\n",
      "Iteration 1385, loss = 0.48333152\n",
      "Iteration 1386, loss = 0.48312842\n",
      "Iteration 1387, loss = 0.48292543\n",
      "Iteration 1388, loss = 0.48272273\n",
      "Iteration 1389, loss = 0.48252023\n",
      "Iteration 1390, loss = 0.48231752\n",
      "Iteration 1391, loss = 0.48211442\n",
      "Iteration 1392, loss = 0.48191143\n",
      "Iteration 1393, loss = 0.48170831\n",
      "Iteration 1394, loss = 0.48150541\n",
      "Iteration 1395, loss = 0.48130317\n",
      "Iteration 1396, loss = 0.48110111\n",
      "Iteration 1397, loss = 0.48089915\n",
      "Iteration 1398, loss = 0.48069723\n",
      "Iteration 1399, loss = 0.48049534\n",
      "Iteration 1400, loss = 0.48029361\n",
      "Iteration 1401, loss = 0.48009244\n",
      "Iteration 1402, loss = 0.47989125\n",
      "Iteration 1403, loss = 0.47969051\n",
      "Iteration 1404, loss = 0.47949003\n",
      "Iteration 1405, loss = 0.47928998\n",
      "Iteration 1406, loss = 0.47909014\n",
      "Iteration 1407, loss = 0.47889034\n",
      "Iteration 1408, loss = 0.47869058\n",
      "Iteration 1409, loss = 0.47849115\n",
      "Iteration 1410, loss = 0.47829219\n",
      "Iteration 1411, loss = 0.47809335\n",
      "Iteration 1412, loss = 0.47789462\n",
      "Iteration 1413, loss = 0.47769595\n",
      "Iteration 1414, loss = 0.47749755\n",
      "Iteration 1415, loss = 0.47729940\n",
      "Iteration 1416, loss = 0.47710134\n",
      "Iteration 1417, loss = 0.47690352\n",
      "Iteration 1418, loss = 0.47670591\n",
      "Iteration 1419, loss = 0.47650844\n",
      "Iteration 1420, loss = 0.47631153\n",
      "Iteration 1421, loss = 0.47611497\n",
      "Iteration 1422, loss = 0.47591836\n",
      "Iteration 1423, loss = 0.47572187\n",
      "Iteration 1424, loss = 0.47552578\n",
      "Iteration 1425, loss = 0.47532981\n",
      "Iteration 1426, loss = 0.47513388\n",
      "Iteration 1427, loss = 0.47493847\n",
      "Iteration 1428, loss = 0.47474320\n",
      "Iteration 1429, loss = 0.47454806\n",
      "Iteration 1430, loss = 0.47435128\n",
      "Iteration 1431, loss = 0.47415445\n",
      "Iteration 1432, loss = 0.47395764\n",
      "Iteration 1433, loss = 0.47376058\n",
      "Iteration 1434, loss = 0.47356335\n",
      "Iteration 1435, loss = 0.47336640\n",
      "Iteration 1436, loss = 0.47316940\n",
      "Iteration 1437, loss = 0.47297247\n",
      "Iteration 1438, loss = 0.47277545\n",
      "Iteration 1439, loss = 0.47257856\n",
      "Iteration 1440, loss = 0.47238149\n",
      "Iteration 1441, loss = 0.47218412\n",
      "Iteration 1442, loss = 0.47198686\n",
      "Iteration 1443, loss = 0.47178969\n",
      "Iteration 1444, loss = 0.47159241\n",
      "Iteration 1445, loss = 0.47139529\n",
      "Iteration 1446, loss = 0.47119830\n",
      "Iteration 1447, loss = 0.47100130\n",
      "Iteration 1448, loss = 0.47080436\n",
      "Iteration 1449, loss = 0.47060776\n",
      "Iteration 1450, loss = 0.47041114\n",
      "Iteration 1451, loss = 0.47021465\n",
      "Iteration 1452, loss = 0.47001854\n",
      "Iteration 1453, loss = 0.46982260\n",
      "Iteration 1454, loss = 0.46962673\n",
      "Iteration 1455, loss = 0.46943109\n",
      "Iteration 1456, loss = 0.46923566\n",
      "Iteration 1457, loss = 0.46904041\n",
      "Iteration 1458, loss = 0.46884544\n",
      "Iteration 1459, loss = 0.46865053\n",
      "Iteration 1460, loss = 0.46845590\n",
      "Iteration 1461, loss = 0.46826151\n",
      "Iteration 1462, loss = 0.46806735\n",
      "Iteration 1463, loss = 0.46787334\n",
      "Iteration 1464, loss = 0.46767970\n",
      "Iteration 1465, loss = 0.46748610\n",
      "Iteration 1466, loss = 0.46729275\n",
      "Iteration 1467, loss = 0.46709959\n",
      "Iteration 1468, loss = 0.46690675\n",
      "Iteration 1469, loss = 0.46671414\n",
      "Iteration 1470, loss = 0.46652165\n",
      "Iteration 1471, loss = 0.46632962\n",
      "Iteration 1472, loss = 0.46613768\n",
      "Iteration 1473, loss = 0.46594569\n",
      "Iteration 1474, loss = 0.46575439\n",
      "Iteration 1475, loss = 0.46556329\n",
      "Iteration 1476, loss = 0.46537228\n",
      "Iteration 1477, loss = 0.46518174\n",
      "Iteration 1478, loss = 0.46499135\n",
      "Iteration 1479, loss = 0.46480113\n",
      "Iteration 1480, loss = 0.46461116\n",
      "Iteration 1481, loss = 0.46442148\n",
      "Iteration 1482, loss = 0.46423197\n",
      "Iteration 1483, loss = 0.46404254\n",
      "Iteration 1484, loss = 0.46385362\n",
      "Iteration 1485, loss = 0.46366467\n",
      "Iteration 1486, loss = 0.46347614\n",
      "Iteration 1487, loss = 0.46328770\n",
      "Iteration 1488, loss = 0.46309959\n",
      "Iteration 1489, loss = 0.46291158\n",
      "Iteration 1490, loss = 0.46272386\n",
      "Iteration 1491, loss = 0.46253630\n",
      "Iteration 1492, loss = 0.46234902\n",
      "Iteration 1493, loss = 0.46216194\n",
      "Iteration 1494, loss = 0.46197509\n",
      "Iteration 1495, loss = 0.46178850\n",
      "Iteration 1496, loss = 0.46160212\n",
      "Iteration 1497, loss = 0.46141601\n",
      "Iteration 1498, loss = 0.46122986\n",
      "Iteration 1499, loss = 0.46104411\n",
      "Iteration 1500, loss = 0.46085871\n",
      "Iteration 1501, loss = 0.46067332\n",
      "Iteration 1502, loss = 0.46048816\n",
      "Iteration 1503, loss = 0.46030330\n",
      "Iteration 1504, loss = 0.46011865\n",
      "Iteration 1505, loss = 0.45993413\n",
      "Iteration 1506, loss = 0.45975000\n",
      "Iteration 1507, loss = 0.45956609\n",
      "Iteration 1508, loss = 0.45938245\n",
      "Iteration 1509, loss = 0.45919893\n",
      "Iteration 1510, loss = 0.45901559\n",
      "Iteration 1511, loss = 0.45883237\n",
      "Iteration 1512, loss = 0.45864943\n",
      "Iteration 1513, loss = 0.45846667\n",
      "Iteration 1514, loss = 0.45828396\n",
      "Iteration 1515, loss = 0.45810174\n",
      "Iteration 1516, loss = 0.45791928\n",
      "Iteration 1517, loss = 0.45773721\n",
      "Iteration 1518, loss = 0.45755533\n",
      "Iteration 1519, loss = 0.45737344\n",
      "Iteration 1520, loss = 0.45719189\n",
      "Iteration 1521, loss = 0.45701052\n",
      "Iteration 1522, loss = 0.45682935\n",
      "Iteration 1523, loss = 0.45664837\n",
      "Iteration 1524, loss = 0.45646749\n",
      "Iteration 1525, loss = 0.45628705\n",
      "Iteration 1526, loss = 0.45610677\n",
      "Iteration 1527, loss = 0.45592645\n",
      "Iteration 1528, loss = 0.45574634\n",
      "Iteration 1529, loss = 0.45556652\n",
      "Iteration 1530, loss = 0.45538661\n",
      "Iteration 1531, loss = 0.45520689\n",
      "Iteration 1532, loss = 0.45502732\n",
      "Iteration 1533, loss = 0.45484796\n",
      "Iteration 1534, loss = 0.45466829\n",
      "Iteration 1535, loss = 0.45448856\n",
      "Iteration 1536, loss = 0.45430927\n",
      "Iteration 1537, loss = 0.45413004\n",
      "Iteration 1538, loss = 0.45395080\n",
      "Iteration 1539, loss = 0.45377171\n",
      "Iteration 1540, loss = 0.45359290\n",
      "Iteration 1541, loss = 0.45341410\n",
      "Iteration 1542, loss = 0.45323523\n",
      "Iteration 1543, loss = 0.45305623\n",
      "Iteration 1544, loss = 0.45287745\n",
      "Iteration 1545, loss = 0.45269886\n",
      "Iteration 1546, loss = 0.45252015\n",
      "Iteration 1547, loss = 0.45234161\n",
      "Iteration 1548, loss = 0.45216324\n",
      "Iteration 1549, loss = 0.45198501\n",
      "Iteration 1550, loss = 0.45180678\n",
      "Iteration 1551, loss = 0.45162844\n",
      "Iteration 1552, loss = 0.45145010\n",
      "Iteration 1553, loss = 0.45127218\n",
      "Iteration 1554, loss = 0.45109436\n",
      "Iteration 1555, loss = 0.45091643\n",
      "Iteration 1556, loss = 0.45073883\n",
      "Iteration 1557, loss = 0.45056149\n",
      "Iteration 1558, loss = 0.45038421\n",
      "Iteration 1559, loss = 0.45020692\n",
      "Iteration 1560, loss = 0.45002976\n",
      "Iteration 1561, loss = 0.44985270\n",
      "Iteration 1562, loss = 0.44967626\n",
      "Iteration 1563, loss = 0.44949981\n",
      "Iteration 1564, loss = 0.44932350\n",
      "Iteration 1565, loss = 0.44914732\n",
      "Iteration 1566, loss = 0.44897117\n",
      "Iteration 1567, loss = 0.44879498\n",
      "Iteration 1568, loss = 0.44861870\n",
      "Iteration 1569, loss = 0.44844268\n",
      "Iteration 1570, loss = 0.44826672\n",
      "Iteration 1571, loss = 0.44809103\n",
      "Iteration 1572, loss = 0.44791535\n",
      "Iteration 1573, loss = 0.44773981\n",
      "Iteration 1574, loss = 0.44756451\n",
      "Iteration 1575, loss = 0.44738926\n",
      "Iteration 1576, loss = 0.44721413\n",
      "Iteration 1577, loss = 0.44703944\n",
      "Iteration 1578, loss = 0.44686486\n",
      "Iteration 1579, loss = 0.44669029\n",
      "Iteration 1580, loss = 0.44651594\n",
      "Iteration 1581, loss = 0.44634120\n",
      "Iteration 1582, loss = 0.44616654\n",
      "Iteration 1583, loss = 0.44599195\n",
      "Iteration 1584, loss = 0.44581776\n",
      "Iteration 1585, loss = 0.44564367\n",
      "Iteration 1586, loss = 0.44546991\n",
      "Iteration 1587, loss = 0.44529608\n",
      "Iteration 1588, loss = 0.44512260\n",
      "Iteration 1589, loss = 0.44494926\n",
      "Iteration 1590, loss = 0.44477592\n",
      "Iteration 1591, loss = 0.44460269\n",
      "Iteration 1592, loss = 0.44442993\n",
      "Iteration 1593, loss = 0.44425790\n",
      "Iteration 1594, loss = 0.44408617\n",
      "Iteration 1595, loss = 0.44391473\n",
      "Iteration 1596, loss = 0.44374365\n",
      "Iteration 1597, loss = 0.44357257\n",
      "Iteration 1598, loss = 0.44340173\n",
      "Iteration 1599, loss = 0.44323111\n",
      "Iteration 1600, loss = 0.44306075\n",
      "Iteration 1601, loss = 0.44289065\n",
      "Iteration 1602, loss = 0.44272056\n",
      "Iteration 1603, loss = 0.44255080\n",
      "Iteration 1604, loss = 0.44238061\n",
      "Iteration 1605, loss = 0.44221054\n",
      "Iteration 1606, loss = 0.44204068\n",
      "Iteration 1607, loss = 0.44187092\n",
      "Iteration 1608, loss = 0.44170141\n",
      "Iteration 1609, loss = 0.44153275\n",
      "Iteration 1610, loss = 0.44136427\n",
      "Iteration 1611, loss = 0.44119583\n",
      "Iteration 1612, loss = 0.44102794\n",
      "Iteration 1613, loss = 0.44086002\n",
      "Iteration 1614, loss = 0.44069223\n",
      "Iteration 1615, loss = 0.44052441\n",
      "Iteration 1616, loss = 0.44035675\n",
      "Iteration 1617, loss = 0.44018911\n",
      "Iteration 1618, loss = 0.44002172\n",
      "Iteration 1619, loss = 0.43985446\n",
      "Iteration 1620, loss = 0.43968724\n",
      "Iteration 1621, loss = 0.43952034\n",
      "Iteration 1622, loss = 0.43935353\n",
      "Iteration 1623, loss = 0.43918665\n",
      "Iteration 1624, loss = 0.43902028\n",
      "Iteration 1625, loss = 0.43885399\n",
      "Iteration 1626, loss = 0.43868797\n",
      "Iteration 1627, loss = 0.43852219\n",
      "Iteration 1628, loss = 0.43835657\n",
      "Iteration 1629, loss = 0.43819100\n",
      "Iteration 1630, loss = 0.43802575\n",
      "Iteration 1631, loss = 0.43786063\n",
      "Iteration 1632, loss = 0.43769580\n",
      "Iteration 1633, loss = 0.43753097\n",
      "Iteration 1634, loss = 0.43736644\n",
      "Iteration 1635, loss = 0.43720192\n",
      "Iteration 1636, loss = 0.43703779\n",
      "Iteration 1637, loss = 0.43687385\n",
      "Iteration 1638, loss = 0.43671042\n",
      "Iteration 1639, loss = 0.43654696\n",
      "Iteration 1640, loss = 0.43638367\n",
      "Iteration 1641, loss = 0.43622066\n",
      "Iteration 1642, loss = 0.43605783\n",
      "Iteration 1643, loss = 0.43589517\n",
      "Iteration 1644, loss = 0.43573276\n",
      "Iteration 1645, loss = 0.43557058\n",
      "Iteration 1646, loss = 0.43540860\n",
      "Iteration 1647, loss = 0.43524657\n",
      "Iteration 1648, loss = 0.43508495\n",
      "Iteration 1649, loss = 0.43492318\n",
      "Iteration 1650, loss = 0.43476136\n",
      "Iteration 1651, loss = 0.43459994\n",
      "Iteration 1652, loss = 0.43443872\n",
      "Iteration 1653, loss = 0.43427761\n",
      "Iteration 1654, loss = 0.43411679\n",
      "Iteration 1655, loss = 0.43395602\n",
      "Iteration 1656, loss = 0.43379542\n",
      "Iteration 1657, loss = 0.43363503\n",
      "Iteration 1658, loss = 0.43347497\n",
      "Iteration 1659, loss = 0.43331514\n",
      "Iteration 1660, loss = 0.43315540\n",
      "Iteration 1661, loss = 0.43299599\n",
      "Iteration 1662, loss = 0.43283660\n",
      "Iteration 1663, loss = 0.43267738\n",
      "Iteration 1664, loss = 0.43251852\n",
      "Iteration 1665, loss = 0.43235976\n",
      "Iteration 1666, loss = 0.43220113\n",
      "Iteration 1667, loss = 0.43204242\n",
      "Iteration 1668, loss = 0.43188377\n",
      "Iteration 1669, loss = 0.43172540\n",
      "Iteration 1670, loss = 0.43156704\n",
      "Iteration 1671, loss = 0.43140882\n",
      "Iteration 1672, loss = 0.43125076\n",
      "Iteration 1673, loss = 0.43109293\n",
      "Iteration 1674, loss = 0.43093544\n",
      "Iteration 1675, loss = 0.43077795\n",
      "Iteration 1676, loss = 0.43062039\n",
      "Iteration 1677, loss = 0.43046249\n",
      "Iteration 1678, loss = 0.43030501\n",
      "Iteration 1679, loss = 0.43014741\n",
      "Iteration 1680, loss = 0.42999005\n",
      "Iteration 1681, loss = 0.42983299\n",
      "Iteration 1682, loss = 0.42967581\n",
      "Iteration 1683, loss = 0.42951900\n",
      "Iteration 1684, loss = 0.42936232\n",
      "Iteration 1685, loss = 0.42920594\n",
      "Iteration 1686, loss = 0.42905024\n",
      "Iteration 1687, loss = 0.42889464\n",
      "Iteration 1688, loss = 0.42873885\n",
      "Iteration 1689, loss = 0.42858257\n",
      "Iteration 1690, loss = 0.42842650\n",
      "Iteration 1691, loss = 0.42827078\n",
      "Iteration 1692, loss = 0.42811537\n",
      "Iteration 1693, loss = 0.42796028\n",
      "Iteration 1694, loss = 0.42780530\n",
      "Iteration 1695, loss = 0.42765061\n",
      "Iteration 1696, loss = 0.42749612\n",
      "Iteration 1697, loss = 0.42734198\n",
      "Iteration 1698, loss = 0.42718765\n",
      "Iteration 1699, loss = 0.42703353\n",
      "Iteration 1700, loss = 0.42687983\n",
      "Iteration 1701, loss = 0.42672621\n",
      "Iteration 1702, loss = 0.42657255\n",
      "Iteration 1703, loss = 0.42641910\n",
      "Iteration 1704, loss = 0.42626581\n",
      "Iteration 1705, loss = 0.42611328\n",
      "Iteration 1706, loss = 0.42596042\n",
      "Iteration 1707, loss = 0.42580753\n",
      "Iteration 1708, loss = 0.42565467\n",
      "Iteration 1709, loss = 0.42550262\n",
      "Iteration 1710, loss = 0.42535060\n",
      "Iteration 1711, loss = 0.42519851\n",
      "Iteration 1712, loss = 0.42504691\n",
      "Iteration 1713, loss = 0.42489538\n",
      "Iteration 1714, loss = 0.42474393\n",
      "Iteration 1715, loss = 0.42459260\n",
      "Iteration 1716, loss = 0.42444154\n",
      "Iteration 1717, loss = 0.42429036\n",
      "Iteration 1718, loss = 0.42413983\n",
      "Iteration 1719, loss = 0.42398910\n",
      "Iteration 1720, loss = 0.42383833\n",
      "Iteration 1721, loss = 0.42368769\n",
      "Iteration 1722, loss = 0.42353673\n",
      "Iteration 1723, loss = 0.42338651\n",
      "Iteration 1724, loss = 0.42323657\n",
      "Iteration 1725, loss = 0.42308651\n",
      "Iteration 1726, loss = 0.42293670\n",
      "Iteration 1727, loss = 0.42278676\n",
      "Iteration 1728, loss = 0.42263727\n",
      "Iteration 1729, loss = 0.42248812\n",
      "Iteration 1730, loss = 0.42233874\n",
      "Iteration 1731, loss = 0.42218949\n",
      "Iteration 1732, loss = 0.42204051\n",
      "Iteration 1733, loss = 0.42189184\n",
      "Iteration 1734, loss = 0.42174328\n",
      "Iteration 1735, loss = 0.42159487\n",
      "Iteration 1736, loss = 0.42144655\n",
      "Iteration 1737, loss = 0.42129840\n",
      "Iteration 1738, loss = 0.42115059\n",
      "Iteration 1739, loss = 0.42100281\n",
      "Iteration 1740, loss = 0.42085537\n",
      "Iteration 1741, loss = 0.42070798\n",
      "Iteration 1742, loss = 0.42056062\n",
      "Iteration 1743, loss = 0.42041363\n",
      "Iteration 1744, loss = 0.42026697\n",
      "Iteration 1745, loss = 0.42012048\n",
      "Iteration 1746, loss = 0.41997399\n",
      "Iteration 1747, loss = 0.41982752\n",
      "Iteration 1748, loss = 0.41968129\n",
      "Iteration 1749, loss = 0.41953536\n",
      "Iteration 1750, loss = 0.41938920\n",
      "Iteration 1751, loss = 0.41924290\n",
      "Iteration 1752, loss = 0.41909701\n",
      "Iteration 1753, loss = 0.41894994\n",
      "Iteration 1754, loss = 0.41880286\n",
      "Iteration 1755, loss = 0.41865743\n",
      "Iteration 1756, loss = 0.41851268\n",
      "Iteration 1757, loss = 0.41836825\n",
      "Iteration 1758, loss = 0.41822525\n",
      "Iteration 1759, loss = 0.41808201\n",
      "Iteration 1760, loss = 0.41793855\n",
      "Iteration 1761, loss = 0.41779523\n",
      "Iteration 1762, loss = 0.41765161\n",
      "Iteration 1763, loss = 0.41750786\n",
      "Iteration 1764, loss = 0.41736392\n",
      "Iteration 1765, loss = 0.41721977\n",
      "Iteration 1766, loss = 0.41707574\n",
      "Iteration 1767, loss = 0.41693233\n",
      "Iteration 1768, loss = 0.41678976\n",
      "Iteration 1769, loss = 0.41664736\n",
      "Iteration 1770, loss = 0.41650520\n",
      "Iteration 1771, loss = 0.41636315\n",
      "Iteration 1772, loss = 0.41622120\n",
      "Iteration 1773, loss = 0.41607917\n",
      "Iteration 1774, loss = 0.41593736\n",
      "Iteration 1775, loss = 0.41579548\n",
      "Iteration 1776, loss = 0.41565401\n",
      "Iteration 1777, loss = 0.41551269\n",
      "Iteration 1778, loss = 0.41537172\n",
      "Iteration 1779, loss = 0.41523077\n",
      "Iteration 1780, loss = 0.41508956\n",
      "Iteration 1781, loss = 0.41494865\n",
      "Iteration 1782, loss = 0.41480779\n",
      "Iteration 1783, loss = 0.41466711\n",
      "Iteration 1784, loss = 0.41452659\n",
      "Iteration 1785, loss = 0.41438626\n",
      "Iteration 1786, loss = 0.41424586\n",
      "Iteration 1787, loss = 0.41410586\n",
      "Iteration 1788, loss = 0.41396618\n",
      "Iteration 1789, loss = 0.41382614\n",
      "Iteration 1790, loss = 0.41368583\n",
      "Iteration 1791, loss = 0.41354521\n",
      "Iteration 1792, loss = 0.41340477\n",
      "Iteration 1793, loss = 0.41326435\n",
      "Iteration 1794, loss = 0.41312385\n",
      "Iteration 1795, loss = 0.41298333\n",
      "Iteration 1796, loss = 0.41284296\n",
      "Iteration 1797, loss = 0.41270308\n",
      "Iteration 1798, loss = 0.41256368\n",
      "Iteration 1799, loss = 0.41242396\n",
      "Iteration 1800, loss = 0.41228449\n",
      "Iteration 1801, loss = 0.41214513\n",
      "Iteration 1802, loss = 0.41200676\n",
      "Iteration 1803, loss = 0.41186849\n",
      "Iteration 1804, loss = 0.41173042\n",
      "Iteration 1805, loss = 0.41159256\n",
      "Iteration 1806, loss = 0.41145476\n",
      "Iteration 1807, loss = 0.41131722\n",
      "Iteration 1808, loss = 0.41117990\n",
      "Iteration 1809, loss = 0.41104253\n",
      "Iteration 1810, loss = 0.41090519\n",
      "Iteration 1811, loss = 0.41076823\n",
      "Iteration 1812, loss = 0.41063159\n",
      "Iteration 1813, loss = 0.41049510\n",
      "Iteration 1814, loss = 0.41035829\n",
      "Iteration 1815, loss = 0.41022170\n",
      "Iteration 1816, loss = 0.41008530\n",
      "Iteration 1817, loss = 0.40994873\n",
      "Iteration 1818, loss = 0.40981238\n",
      "Iteration 1819, loss = 0.40967590\n",
      "Iteration 1820, loss = 0.40953919\n",
      "Iteration 1821, loss = 0.40940271\n",
      "Iteration 1822, loss = 0.40926683\n",
      "Iteration 1823, loss = 0.40913076\n",
      "Iteration 1824, loss = 0.40899452\n",
      "Iteration 1825, loss = 0.40885851\n",
      "Iteration 1826, loss = 0.40872282\n",
      "Iteration 1827, loss = 0.40858720\n",
      "Iteration 1828, loss = 0.40845188\n",
      "Iteration 1829, loss = 0.40831657\n",
      "Iteration 1830, loss = 0.40818111\n",
      "Iteration 1831, loss = 0.40804597\n",
      "Iteration 1832, loss = 0.40791136\n",
      "Iteration 1833, loss = 0.40777674\n",
      "Iteration 1834, loss = 0.40764233\n",
      "Iteration 1835, loss = 0.40750761\n",
      "Iteration 1836, loss = 0.40737231\n",
      "Iteration 1837, loss = 0.40723716\n",
      "Iteration 1838, loss = 0.40710229\n",
      "Iteration 1839, loss = 0.40696758\n",
      "Iteration 1840, loss = 0.40683363\n",
      "Iteration 1841, loss = 0.40670039\n",
      "Iteration 1842, loss = 0.40656731\n",
      "Iteration 1843, loss = 0.40643409\n",
      "Iteration 1844, loss = 0.40630013\n",
      "Iteration 1845, loss = 0.40616630\n",
      "Iteration 1846, loss = 0.40603289\n",
      "Iteration 1847, loss = 0.40589932\n",
      "Iteration 1848, loss = 0.40576631\n",
      "Iteration 1849, loss = 0.40563546\n",
      "Iteration 1850, loss = 0.40550580\n",
      "Iteration 1851, loss = 0.40537529\n",
      "Iteration 1852, loss = 0.40524506\n",
      "Iteration 1853, loss = 0.40511503\n",
      "Iteration 1854, loss = 0.40498489\n",
      "Iteration 1855, loss = 0.40485449\n",
      "Iteration 1856, loss = 0.40472396\n",
      "Iteration 1857, loss = 0.40459367\n",
      "Iteration 1858, loss = 0.40446382\n",
      "Iteration 1859, loss = 0.40433435\n",
      "Iteration 1860, loss = 0.40420505\n",
      "Iteration 1861, loss = 0.40407579\n",
      "Iteration 1862, loss = 0.40394656\n",
      "Iteration 1863, loss = 0.40381724\n",
      "Iteration 1864, loss = 0.40368835\n",
      "Iteration 1865, loss = 0.40355942\n",
      "Iteration 1866, loss = 0.40343061\n",
      "Iteration 1867, loss = 0.40330175\n",
      "Iteration 1868, loss = 0.40317252\n",
      "Iteration 1869, loss = 0.40304345\n",
      "Iteration 1870, loss = 0.40291551\n",
      "Iteration 1871, loss = 0.40278773\n",
      "Iteration 1872, loss = 0.40266032\n",
      "Iteration 1873, loss = 0.40253284\n",
      "Iteration 1874, loss = 0.40240530\n",
      "Iteration 1875, loss = 0.40227802\n",
      "Iteration 1876, loss = 0.40215069\n",
      "Iteration 1877, loss = 0.40202341\n",
      "Iteration 1878, loss = 0.40189620\n",
      "Iteration 1879, loss = 0.40176917\n",
      "Iteration 1880, loss = 0.40164239\n",
      "Iteration 1881, loss = 0.40151527\n",
      "Iteration 1882, loss = 0.40138841\n",
      "Iteration 1883, loss = 0.40126137\n",
      "Iteration 1884, loss = 0.40113448\n",
      "Iteration 1885, loss = 0.40100770\n",
      "Iteration 1886, loss = 0.40088078\n",
      "Iteration 1887, loss = 0.40075371\n",
      "Iteration 1888, loss = 0.40062606\n",
      "Iteration 1889, loss = 0.40049762\n",
      "Iteration 1890, loss = 0.40036867\n",
      "Iteration 1891, loss = 0.40023926\n",
      "Iteration 1892, loss = 0.40010944\n",
      "Iteration 1893, loss = 0.39997918\n",
      "Iteration 1894, loss = 0.39984901\n",
      "Iteration 1895, loss = 0.39971861\n",
      "Iteration 1896, loss = 0.39958785\n",
      "Iteration 1897, loss = 0.39945890\n",
      "Iteration 1898, loss = 0.39933220\n",
      "Iteration 1899, loss = 0.39920576\n",
      "Iteration 1900, loss = 0.39907955\n",
      "Iteration 1901, loss = 0.39895364\n",
      "Iteration 1902, loss = 0.39882781\n",
      "Iteration 1903, loss = 0.39870232\n",
      "Iteration 1904, loss = 0.39857700\n",
      "Iteration 1905, loss = 0.39845210\n",
      "Iteration 1906, loss = 0.39832719\n",
      "Iteration 1907, loss = 0.39820229\n",
      "Iteration 1908, loss = 0.39807773\n",
      "Iteration 1909, loss = 0.39795318\n",
      "Iteration 1910, loss = 0.39782876\n",
      "Iteration 1911, loss = 0.39770476\n",
      "Iteration 1912, loss = 0.39758071\n",
      "Iteration 1913, loss = 0.39745689\n",
      "Iteration 1914, loss = 0.39733331\n",
      "Iteration 1915, loss = 0.39720994\n",
      "Iteration 1916, loss = 0.39708653\n",
      "Iteration 1917, loss = 0.39696331\n",
      "Iteration 1918, loss = 0.39684053\n",
      "Iteration 1919, loss = 0.39671757\n",
      "Iteration 1920, loss = 0.39659444\n",
      "Iteration 1921, loss = 0.39647156\n",
      "Iteration 1922, loss = 0.39634894\n",
      "Iteration 1923, loss = 0.39622627\n",
      "Iteration 1924, loss = 0.39610384\n",
      "Iteration 1925, loss = 0.39598147\n",
      "Iteration 1926, loss = 0.39585918\n",
      "Iteration 1927, loss = 0.39573696\n",
      "Iteration 1928, loss = 0.39561465\n",
      "Iteration 1929, loss = 0.39549261\n",
      "Iteration 1930, loss = 0.39537098\n",
      "Iteration 1931, loss = 0.39524920\n",
      "Iteration 1932, loss = 0.39512749\n",
      "Iteration 1933, loss = 0.39500587\n",
      "Iteration 1934, loss = 0.39488454\n",
      "Iteration 1935, loss = 0.39476356\n",
      "Iteration 1936, loss = 0.39464324\n",
      "Iteration 1937, loss = 0.39452282\n",
      "Iteration 1938, loss = 0.39440282\n",
      "Iteration 1939, loss = 0.39428291\n",
      "Iteration 1940, loss = 0.39416284\n",
      "Iteration 1941, loss = 0.39404314\n",
      "Iteration 1942, loss = 0.39392344\n",
      "Iteration 1943, loss = 0.39380384\n",
      "Iteration 1944, loss = 0.39368441\n",
      "Iteration 1945, loss = 0.39356500\n",
      "Iteration 1946, loss = 0.39344593\n",
      "Iteration 1947, loss = 0.39332710\n",
      "Iteration 1948, loss = 0.39320818\n",
      "Iteration 1949, loss = 0.39308926\n",
      "Iteration 1950, loss = 0.39297034\n",
      "Iteration 1951, loss = 0.39285196\n",
      "Iteration 1952, loss = 0.39273375\n",
      "Iteration 1953, loss = 0.39261571\n",
      "Iteration 1954, loss = 0.39249746\n",
      "Iteration 1955, loss = 0.39237939\n",
      "Iteration 1956, loss = 0.39226155\n",
      "Iteration 1957, loss = 0.39214373\n",
      "Iteration 1958, loss = 0.39202612\n",
      "Iteration 1959, loss = 0.39190881\n",
      "Iteration 1960, loss = 0.39179153\n",
      "Iteration 1961, loss = 0.39167440\n",
      "Iteration 1962, loss = 0.39155774\n",
      "Iteration 1963, loss = 0.39144103\n",
      "Iteration 1964, loss = 0.39132431\n",
      "Iteration 1965, loss = 0.39120822\n",
      "Iteration 1966, loss = 0.39109188\n",
      "Iteration 1967, loss = 0.39097578\n",
      "Iteration 1968, loss = 0.39085953\n",
      "Iteration 1969, loss = 0.39074328\n",
      "Iteration 1970, loss = 0.39062753\n",
      "Iteration 1971, loss = 0.39051200\n",
      "Iteration 1972, loss = 0.39039626\n",
      "Iteration 1973, loss = 0.39028047\n",
      "Iteration 1974, loss = 0.39016477\n",
      "Iteration 1975, loss = 0.39004960\n",
      "Iteration 1976, loss = 0.38993448\n",
      "Iteration 1977, loss = 0.38981939\n",
      "Iteration 1978, loss = 0.38970441\n",
      "Iteration 1979, loss = 0.38958950\n",
      "Iteration 1980, loss = 0.38947468\n",
      "Iteration 1981, loss = 0.38935991\n",
      "Iteration 1982, loss = 0.38924556\n",
      "Iteration 1983, loss = 0.38913134\n",
      "Iteration 1984, loss = 0.38901712\n",
      "Iteration 1985, loss = 0.38890294\n",
      "Iteration 1986, loss = 0.38878886\n",
      "Iteration 1987, loss = 0.38867537\n",
      "Iteration 1988, loss = 0.38856186\n",
      "Iteration 1989, loss = 0.38844849\n",
      "Iteration 1990, loss = 0.38833562\n",
      "Iteration 1991, loss = 0.38822280\n",
      "Iteration 1992, loss = 0.38811004\n",
      "Iteration 1993, loss = 0.38799816\n",
      "Iteration 1994, loss = 0.38788631\n",
      "Iteration 1995, loss = 0.38777447\n",
      "Iteration 1996, loss = 0.38766274\n",
      "Iteration 1997, loss = 0.38755091\n",
      "Iteration 1998, loss = 0.38743946\n",
      "Iteration 1999, loss = 0.38732822\n",
      "Iteration 2000, loss = 0.38721690\n",
      "Iteration 2001, loss = 0.38710588\n",
      "Iteration 2002, loss = 0.38699508\n",
      "Iteration 2003, loss = 0.38688372\n",
      "Iteration 2004, loss = 0.38677333\n",
      "Iteration 2005, loss = 0.38666288\n",
      "Iteration 2006, loss = 0.38655222\n",
      "Iteration 2007, loss = 0.38644153\n",
      "Iteration 2008, loss = 0.38633148\n",
      "Iteration 2009, loss = 0.38622119\n",
      "Iteration 2010, loss = 0.38611086\n",
      "Iteration 2011, loss = 0.38600033\n",
      "Iteration 2012, loss = 0.38588998\n",
      "Iteration 2013, loss = 0.38577995\n",
      "Iteration 2014, loss = 0.38567021\n",
      "Iteration 2015, loss = 0.38556010\n",
      "Iteration 2016, loss = 0.38545020\n",
      "Iteration 2017, loss = 0.38534071\n",
      "Iteration 2018, loss = 0.38523121\n",
      "Iteration 2019, loss = 0.38512185\n",
      "Iteration 2020, loss = 0.38501250\n",
      "Iteration 2021, loss = 0.38490322\n",
      "Iteration 2022, loss = 0.38479421\n",
      "Iteration 2023, loss = 0.38468519\n",
      "Iteration 2024, loss = 0.38457617\n",
      "Iteration 2025, loss = 0.38446765\n",
      "Iteration 2026, loss = 0.38435904\n",
      "Iteration 2027, loss = 0.38425042\n",
      "Iteration 2028, loss = 0.38414184\n",
      "Iteration 2029, loss = 0.38403362\n",
      "Iteration 2030, loss = 0.38392580\n",
      "Iteration 2031, loss = 0.38381750\n",
      "Iteration 2032, loss = 0.38370985\n",
      "Iteration 2033, loss = 0.38360347\n",
      "Iteration 2034, loss = 0.38349711\n",
      "Iteration 2035, loss = 0.38339055\n",
      "Iteration 2036, loss = 0.38328388\n",
      "Iteration 2037, loss = 0.38317730\n",
      "Iteration 2038, loss = 0.38307083\n",
      "Iteration 2039, loss = 0.38296427\n",
      "Iteration 2040, loss = 0.38285770\n",
      "Iteration 2041, loss = 0.38275128\n",
      "Iteration 2042, loss = 0.38264473\n",
      "Iteration 2043, loss = 0.38253865\n",
      "Iteration 2044, loss = 0.38243254\n",
      "Iteration 2045, loss = 0.38232667\n",
      "Iteration 2046, loss = 0.38222079\n",
      "Iteration 2047, loss = 0.38211502\n",
      "Iteration 2048, loss = 0.38200938\n",
      "Iteration 2049, loss = 0.38190395\n",
      "Iteration 2050, loss = 0.38179855\n",
      "Iteration 2051, loss = 0.38169327\n",
      "Iteration 2052, loss = 0.38158902\n",
      "Iteration 2053, loss = 0.38148514\n",
      "Iteration 2054, loss = 0.38138125\n",
      "Iteration 2055, loss = 0.38127722\n",
      "Iteration 2056, loss = 0.38117311\n",
      "Iteration 2057, loss = 0.38106925\n",
      "Iteration 2058, loss = 0.38096566\n",
      "Iteration 2059, loss = 0.38086219\n",
      "Iteration 2060, loss = 0.38075851\n",
      "Iteration 2061, loss = 0.38065552\n",
      "Iteration 2062, loss = 0.38055286\n",
      "Iteration 2063, loss = 0.38045020\n",
      "Iteration 2064, loss = 0.38034769\n",
      "Iteration 2065, loss = 0.38024524\n",
      "Iteration 2066, loss = 0.38014288\n",
      "Iteration 2067, loss = 0.38004062\n",
      "Iteration 2068, loss = 0.37993845\n",
      "Iteration 2069, loss = 0.37983670\n",
      "Iteration 2070, loss = 0.37973473\n",
      "Iteration 2071, loss = 0.37963275\n",
      "Iteration 2072, loss = 0.37953114\n",
      "Iteration 2073, loss = 0.37942945\n",
      "Iteration 2074, loss = 0.37932783\n",
      "Iteration 2075, loss = 0.37922639\n",
      "Iteration 2076, loss = 0.37912529\n",
      "Iteration 2077, loss = 0.37902400\n",
      "Iteration 2078, loss = 0.37892289\n",
      "Iteration 2079, loss = 0.37882170\n",
      "Iteration 2080, loss = 0.37872084\n",
      "Iteration 2081, loss = 0.37861992\n",
      "Iteration 2082, loss = 0.37851938\n",
      "Iteration 2083, loss = 0.37841872\n",
      "Iteration 2084, loss = 0.37831849\n",
      "Iteration 2085, loss = 0.37821811\n",
      "Iteration 2086, loss = 0.37811783\n",
      "Iteration 2087, loss = 0.37801779\n",
      "Iteration 2088, loss = 0.37791790\n",
      "Iteration 2089, loss = 0.37781776\n",
      "Iteration 2090, loss = 0.37771802\n",
      "Iteration 2091, loss = 0.37761833\n",
      "Iteration 2092, loss = 0.37751882\n",
      "Iteration 2093, loss = 0.37741941\n",
      "Iteration 2094, loss = 0.37731980\n",
      "Iteration 2095, loss = 0.37722064\n",
      "Iteration 2096, loss = 0.37712141\n",
      "Iteration 2097, loss = 0.37702235\n",
      "Iteration 2098, loss = 0.37692339\n",
      "Iteration 2099, loss = 0.37682479\n",
      "Iteration 2100, loss = 0.37672603\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34512639\n",
      "Iteration 2, loss = 1.34286860\n",
      "Iteration 3, loss = 1.34062196\n",
      "Iteration 4, loss = 1.33838841\n",
      "Iteration 5, loss = 1.33617555\n",
      "Iteration 6, loss = 1.33397106\n",
      "Iteration 7, loss = 1.33177124\n",
      "Iteration 8, loss = 1.32957755\n",
      "Iteration 9, loss = 1.32739390\n",
      "Iteration 10, loss = 1.32521782\n",
      "Iteration 11, loss = 1.32305971\n",
      "Iteration 12, loss = 1.32091282\n",
      "Iteration 13, loss = 1.31877003\n",
      "Iteration 14, loss = 1.31662516\n",
      "Iteration 15, loss = 1.31448345\n",
      "Iteration 16, loss = 1.31234640\n",
      "Iteration 17, loss = 1.31022345\n",
      "Iteration 18, loss = 1.30810794\n",
      "Iteration 19, loss = 1.30600442\n",
      "Iteration 20, loss = 1.30391379\n",
      "Iteration 21, loss = 1.30183241\n",
      "Iteration 22, loss = 1.29975988\n",
      "Iteration 23, loss = 1.29769641\n",
      "Iteration 24, loss = 1.29564322\n",
      "Iteration 25, loss = 1.29360511\n",
      "Iteration 26, loss = 1.29158708\n",
      "Iteration 27, loss = 1.28957914\n",
      "Iteration 28, loss = 1.28758240\n",
      "Iteration 29, loss = 1.28559958\n",
      "Iteration 30, loss = 1.28363434\n",
      "Iteration 31, loss = 1.28168463\n",
      "Iteration 32, loss = 1.27976022\n",
      "Iteration 33, loss = 1.27785053\n",
      "Iteration 34, loss = 1.27595119\n",
      "Iteration 35, loss = 1.27406130\n",
      "Iteration 36, loss = 1.27218160\n",
      "Iteration 37, loss = 1.27031662\n",
      "Iteration 38, loss = 1.26846031\n",
      "Iteration 39, loss = 1.26660864\n",
      "Iteration 40, loss = 1.26476281\n",
      "Iteration 41, loss = 1.26291798\n",
      "Iteration 42, loss = 1.26108068\n",
      "Iteration 43, loss = 1.25924905\n",
      "Iteration 44, loss = 1.25742261\n",
      "Iteration 45, loss = 1.25560698\n",
      "Iteration 46, loss = 1.25378513\n",
      "Iteration 47, loss = 1.25196669\n",
      "Iteration 48, loss = 1.25015342\n",
      "Iteration 49, loss = 1.24834113\n",
      "Iteration 50, loss = 1.24653777\n",
      "Iteration 51, loss = 1.24473935\n",
      "Iteration 52, loss = 1.24294243\n",
      "Iteration 53, loss = 1.24113880\n",
      "Iteration 54, loss = 1.23934220\n",
      "Iteration 55, loss = 1.23755631\n",
      "Iteration 56, loss = 1.23577577\n",
      "Iteration 57, loss = 1.23400061\n",
      "Iteration 58, loss = 1.23223284\n",
      "Iteration 59, loss = 1.23047379\n",
      "Iteration 60, loss = 1.22872701\n",
      "Iteration 61, loss = 1.22699566\n",
      "Iteration 62, loss = 1.22527419\n",
      "Iteration 63, loss = 1.22356058\n",
      "Iteration 64, loss = 1.22186202\n",
      "Iteration 65, loss = 1.22018742\n",
      "Iteration 66, loss = 1.21852128\n",
      "Iteration 67, loss = 1.21686713\n",
      "Iteration 68, loss = 1.21522033\n",
      "Iteration 69, loss = 1.21358042\n",
      "Iteration 70, loss = 1.21194882\n",
      "Iteration 71, loss = 1.21032192\n",
      "Iteration 72, loss = 1.20869998\n",
      "Iteration 73, loss = 1.20708472\n",
      "Iteration 74, loss = 1.20547637\n",
      "Iteration 75, loss = 1.20387816\n",
      "Iteration 76, loss = 1.20228665\n",
      "Iteration 77, loss = 1.20070295\n",
      "Iteration 78, loss = 1.19912603\n",
      "Iteration 79, loss = 1.19755718\n",
      "Iteration 80, loss = 1.19599607\n",
      "Iteration 81, loss = 1.19444111\n",
      "Iteration 82, loss = 1.19289200\n",
      "Iteration 83, loss = 1.19135337\n",
      "Iteration 84, loss = 1.18982209\n",
      "Iteration 85, loss = 1.18829697\n",
      "Iteration 86, loss = 1.18677821\n",
      "Iteration 87, loss = 1.18526749\n",
      "Iteration 88, loss = 1.18376750\n",
      "Iteration 89, loss = 1.18227707\n",
      "Iteration 90, loss = 1.18079193\n",
      "Iteration 91, loss = 1.17931181\n",
      "Iteration 92, loss = 1.17783981\n",
      "Iteration 93, loss = 1.17637594\n",
      "Iteration 94, loss = 1.17491766\n",
      "Iteration 95, loss = 1.17348159\n",
      "Iteration 96, loss = 1.17204577\n",
      "Iteration 97, loss = 1.17061660\n",
      "Iteration 98, loss = 1.16919245\n",
      "Iteration 99, loss = 1.16777374\n",
      "Iteration 100, loss = 1.16636110\n",
      "Iteration 101, loss = 1.16495670\n",
      "Iteration 102, loss = 1.16355941\n",
      "Iteration 103, loss = 1.16216884\n",
      "Iteration 104, loss = 1.16079162\n",
      "Iteration 105, loss = 1.15941693\n",
      "Iteration 106, loss = 1.15804884\n",
      "Iteration 107, loss = 1.15668481\n",
      "Iteration 108, loss = 1.15532748\n",
      "Iteration 109, loss = 1.15397427\n",
      "Iteration 110, loss = 1.15262520\n",
      "Iteration 111, loss = 1.15128295\n",
      "Iteration 112, loss = 1.14995242\n",
      "Iteration 113, loss = 1.14862278\n",
      "Iteration 114, loss = 1.14730175\n",
      "Iteration 115, loss = 1.14598473\n",
      "Iteration 116, loss = 1.14467261\n",
      "Iteration 117, loss = 1.14336576\n",
      "Iteration 118, loss = 1.14206193\n",
      "Iteration 119, loss = 1.14076218\n",
      "Iteration 120, loss = 1.13946805\n",
      "Iteration 121, loss = 1.13817846\n",
      "Iteration 122, loss = 1.13689554\n",
      "Iteration 123, loss = 1.13561706\n",
      "Iteration 124, loss = 1.13434221\n",
      "Iteration 125, loss = 1.13307255\n",
      "Iteration 126, loss = 1.13181606\n",
      "Iteration 127, loss = 1.13056060\n",
      "Iteration 128, loss = 1.12931033\n",
      "Iteration 129, loss = 1.12806615\n",
      "Iteration 130, loss = 1.12682336\n",
      "Iteration 131, loss = 1.12558656\n",
      "Iteration 132, loss = 1.12435402\n",
      "Iteration 133, loss = 1.12312379\n",
      "Iteration 134, loss = 1.12189682\n",
      "Iteration 135, loss = 1.12067291\n",
      "Iteration 136, loss = 1.11945838\n",
      "Iteration 137, loss = 1.11824620\n",
      "Iteration 138, loss = 1.11704009\n",
      "Iteration 139, loss = 1.11583779\n",
      "Iteration 140, loss = 1.11463925\n",
      "Iteration 141, loss = 1.11344313\n",
      "Iteration 142, loss = 1.11224706\n",
      "Iteration 143, loss = 1.11105699\n",
      "Iteration 144, loss = 1.10986656\n",
      "Iteration 145, loss = 1.10867902\n",
      "Iteration 146, loss = 1.10749911\n",
      "Iteration 147, loss = 1.10632183\n",
      "Iteration 148, loss = 1.10514820\n",
      "Iteration 149, loss = 1.10397895\n",
      "Iteration 150, loss = 1.10281540\n",
      "Iteration 151, loss = 1.10165916\n",
      "Iteration 152, loss = 1.10050366\n",
      "Iteration 153, loss = 1.09934802\n",
      "Iteration 154, loss = 1.09819473\n",
      "Iteration 155, loss = 1.09704542\n",
      "Iteration 156, loss = 1.09589992\n",
      "Iteration 157, loss = 1.09475521\n",
      "Iteration 158, loss = 1.09361233\n",
      "Iteration 159, loss = 1.09247295\n",
      "Iteration 160, loss = 1.09133499\n",
      "Iteration 161, loss = 1.09020758\n",
      "Iteration 162, loss = 1.08908168\n",
      "Iteration 163, loss = 1.08795790\n",
      "Iteration 164, loss = 1.08683616\n",
      "Iteration 165, loss = 1.08571882\n",
      "Iteration 166, loss = 1.08460219\n",
      "Iteration 167, loss = 1.08348813\n",
      "Iteration 168, loss = 1.08237796\n",
      "Iteration 169, loss = 1.08126914\n",
      "Iteration 170, loss = 1.08016211\n",
      "Iteration 171, loss = 1.07905777\n",
      "Iteration 172, loss = 1.07796562\n",
      "Iteration 173, loss = 1.07687800\n",
      "Iteration 174, loss = 1.07578816\n",
      "Iteration 175, loss = 1.07470062\n",
      "Iteration 176, loss = 1.07361692\n",
      "Iteration 177, loss = 1.07253364\n",
      "Iteration 178, loss = 1.07145041\n",
      "Iteration 179, loss = 1.07036765\n",
      "Iteration 180, loss = 1.06928706\n",
      "Iteration 181, loss = 1.06820875\n",
      "Iteration 182, loss = 1.06713767\n",
      "Iteration 183, loss = 1.06606805\n",
      "Iteration 184, loss = 1.06500165\n",
      "Iteration 185, loss = 1.06393718\n",
      "Iteration 186, loss = 1.06287604\n",
      "Iteration 187, loss = 1.06181807\n",
      "Iteration 188, loss = 1.06076421\n",
      "Iteration 189, loss = 1.05971204\n",
      "Iteration 190, loss = 1.05866361\n",
      "Iteration 191, loss = 1.05761765\n",
      "Iteration 192, loss = 1.05657059\n",
      "Iteration 193, loss = 1.05551780\n",
      "Iteration 194, loss = 1.05446779\n",
      "Iteration 195, loss = 1.05342059\n",
      "Iteration 196, loss = 1.05237279\n",
      "Iteration 197, loss = 1.05132832\n",
      "Iteration 198, loss = 1.05028583\n",
      "Iteration 199, loss = 1.04924405\n",
      "Iteration 200, loss = 1.04820479\n",
      "Iteration 201, loss = 1.04716824\n",
      "Iteration 202, loss = 1.04613285\n",
      "Iteration 203, loss = 1.04510189\n",
      "Iteration 204, loss = 1.04407129\n",
      "Iteration 205, loss = 1.04304349\n",
      "Iteration 206, loss = 1.04201506\n",
      "Iteration 207, loss = 1.04098417\n",
      "Iteration 208, loss = 1.03995682\n",
      "Iteration 209, loss = 1.03893049\n",
      "Iteration 210, loss = 1.03790779\n",
      "Iteration 211, loss = 1.03689127\n",
      "Iteration 212, loss = 1.03587663\n",
      "Iteration 213, loss = 1.03486466\n",
      "Iteration 214, loss = 1.03385783\n",
      "Iteration 215, loss = 1.03285313\n",
      "Iteration 216, loss = 1.03184741\n",
      "Iteration 217, loss = 1.03084266\n",
      "Iteration 218, loss = 1.02984108\n",
      "Iteration 219, loss = 1.02883974\n",
      "Iteration 220, loss = 1.02783619\n",
      "Iteration 221, loss = 1.02683274\n",
      "Iteration 222, loss = 1.02583087\n",
      "Iteration 223, loss = 1.02482947\n",
      "Iteration 224, loss = 1.02383125\n",
      "Iteration 225, loss = 1.02283389\n",
      "Iteration 226, loss = 1.02183709\n",
      "Iteration 227, loss = 1.02084296\n",
      "Iteration 228, loss = 1.01985200\n",
      "Iteration 229, loss = 1.01886290\n",
      "Iteration 230, loss = 1.01787713\n",
      "Iteration 231, loss = 1.01689491\n",
      "Iteration 232, loss = 1.01591245\n",
      "Iteration 233, loss = 1.01493630\n",
      "Iteration 234, loss = 1.01396239\n",
      "Iteration 235, loss = 1.01299124\n",
      "Iteration 236, loss = 1.01202275\n",
      "Iteration 237, loss = 1.01105818\n",
      "Iteration 238, loss = 1.01010212\n",
      "Iteration 239, loss = 1.00914775\n",
      "Iteration 240, loss = 1.00818304\n",
      "Iteration 241, loss = 1.00721665\n",
      "Iteration 242, loss = 1.00625279\n",
      "Iteration 243, loss = 1.00529135\n",
      "Iteration 244, loss = 1.00432955\n",
      "Iteration 245, loss = 1.00336780\n",
      "Iteration 246, loss = 1.00241039\n",
      "Iteration 247, loss = 1.00145667\n",
      "Iteration 248, loss = 1.00050327\n",
      "Iteration 249, loss = 0.99955365\n",
      "Iteration 250, loss = 0.99860376\n",
      "Iteration 251, loss = 0.99765538\n",
      "Iteration 252, loss = 0.99670864\n",
      "Iteration 253, loss = 0.99576653\n",
      "Iteration 254, loss = 0.99482881\n",
      "Iteration 255, loss = 0.99389177\n",
      "Iteration 256, loss = 0.99295463\n",
      "Iteration 257, loss = 0.99201801\n",
      "Iteration 258, loss = 0.99108618\n",
      "Iteration 259, loss = 0.99015558\n",
      "Iteration 260, loss = 0.98922717\n",
      "Iteration 261, loss = 0.98830192\n",
      "Iteration 262, loss = 0.98737749\n",
      "Iteration 263, loss = 0.98645322\n",
      "Iteration 264, loss = 0.98553041\n",
      "Iteration 265, loss = 0.98461064\n",
      "Iteration 266, loss = 0.98369160\n",
      "Iteration 267, loss = 0.98277223\n",
      "Iteration 268, loss = 0.98185088\n",
      "Iteration 269, loss = 0.98093593\n",
      "Iteration 270, loss = 0.98002071\n",
      "Iteration 271, loss = 0.97910474\n",
      "Iteration 272, loss = 0.97819204\n",
      "Iteration 273, loss = 0.97728211\n",
      "Iteration 274, loss = 0.97637493\n",
      "Iteration 275, loss = 0.97546869\n",
      "Iteration 276, loss = 0.97456298\n",
      "Iteration 277, loss = 0.97365488\n",
      "Iteration 278, loss = 0.97275180\n",
      "Iteration 279, loss = 0.97185189\n",
      "Iteration 280, loss = 0.97094867\n",
      "Iteration 281, loss = 0.97005006\n",
      "Iteration 282, loss = 0.96915152\n",
      "Iteration 283, loss = 0.96825225\n",
      "Iteration 284, loss = 0.96735916\n",
      "Iteration 285, loss = 0.96646883\n",
      "Iteration 286, loss = 0.96557821\n",
      "Iteration 287, loss = 0.96468927\n",
      "Iteration 288, loss = 0.96380334\n",
      "Iteration 289, loss = 0.96291640\n",
      "Iteration 290, loss = 0.96203084\n",
      "Iteration 291, loss = 0.96114748\n",
      "Iteration 292, loss = 0.96026468\n",
      "Iteration 293, loss = 0.95938257\n",
      "Iteration 294, loss = 0.95849617\n",
      "Iteration 295, loss = 0.95761375\n",
      "Iteration 296, loss = 0.95673262\n",
      "Iteration 297, loss = 0.95585536\n",
      "Iteration 298, loss = 0.95497652\n",
      "Iteration 299, loss = 0.95409867\n",
      "Iteration 300, loss = 0.95322263\n",
      "Iteration 301, loss = 0.95234866\n",
      "Iteration 302, loss = 0.95147497\n",
      "Iteration 303, loss = 0.95060045\n",
      "Iteration 304, loss = 0.94972667\n",
      "Iteration 305, loss = 0.94885608\n",
      "Iteration 306, loss = 0.94798693\n",
      "Iteration 307, loss = 0.94711840\n",
      "Iteration 308, loss = 0.94625079\n",
      "Iteration 309, loss = 0.94538793\n",
      "Iteration 310, loss = 0.94452272\n",
      "Iteration 311, loss = 0.94365705\n",
      "Iteration 312, loss = 0.94279565\n",
      "Iteration 313, loss = 0.94194003\n",
      "Iteration 314, loss = 0.94108403\n",
      "Iteration 315, loss = 0.94022929\n",
      "Iteration 316, loss = 0.93937360\n",
      "Iteration 317, loss = 0.93851870\n",
      "Iteration 318, loss = 0.93766384\n",
      "Iteration 319, loss = 0.93681094\n",
      "Iteration 320, loss = 0.93595877\n",
      "Iteration 321, loss = 0.93510590\n",
      "Iteration 322, loss = 0.93424771\n",
      "Iteration 323, loss = 0.93338960\n",
      "Iteration 324, loss = 0.93253093\n",
      "Iteration 325, loss = 0.93167399\n",
      "Iteration 326, loss = 0.93081617\n",
      "Iteration 327, loss = 0.92995735\n",
      "Iteration 328, loss = 0.92909990\n",
      "Iteration 329, loss = 0.92824381\n",
      "Iteration 330, loss = 0.92739109\n",
      "Iteration 331, loss = 0.92653672\n",
      "Iteration 332, loss = 0.92568131\n",
      "Iteration 333, loss = 0.92482742\n",
      "Iteration 334, loss = 0.92397367\n",
      "Iteration 335, loss = 0.92312043\n",
      "Iteration 336, loss = 0.92227102\n",
      "Iteration 337, loss = 0.92142229\n",
      "Iteration 338, loss = 0.92057426\n",
      "Iteration 339, loss = 0.91972916\n",
      "Iteration 340, loss = 0.91888459\n",
      "Iteration 341, loss = 0.91804015\n",
      "Iteration 342, loss = 0.91719931\n",
      "Iteration 343, loss = 0.91636302\n",
      "Iteration 344, loss = 0.91553615\n",
      "Iteration 345, loss = 0.91471107\n",
      "Iteration 346, loss = 0.91388722\n",
      "Iteration 347, loss = 0.91306476\n",
      "Iteration 348, loss = 0.91224302\n",
      "Iteration 349, loss = 0.91142227\n",
      "Iteration 350, loss = 0.91060415\n",
      "Iteration 351, loss = 0.90978517\n",
      "Iteration 352, loss = 0.90896923\n",
      "Iteration 353, loss = 0.90815480\n",
      "Iteration 354, loss = 0.90734225\n",
      "Iteration 355, loss = 0.90653274\n",
      "Iteration 356, loss = 0.90572484\n",
      "Iteration 357, loss = 0.90491360\n",
      "Iteration 358, loss = 0.90410313\n",
      "Iteration 359, loss = 0.90329083\n",
      "Iteration 360, loss = 0.90247749\n",
      "Iteration 361, loss = 0.90166649\n",
      "Iteration 362, loss = 0.90085870\n",
      "Iteration 363, loss = 0.90005300\n",
      "Iteration 364, loss = 0.89924996\n",
      "Iteration 365, loss = 0.89844687\n",
      "Iteration 366, loss = 0.89764779\n",
      "Iteration 367, loss = 0.89685093\n",
      "Iteration 368, loss = 0.89605645\n",
      "Iteration 369, loss = 0.89526193\n",
      "Iteration 370, loss = 0.89446747\n",
      "Iteration 371, loss = 0.89367393\n",
      "Iteration 372, loss = 0.89288459\n",
      "Iteration 373, loss = 0.89209793\n",
      "Iteration 374, loss = 0.89131365\n",
      "Iteration 375, loss = 0.89053128\n",
      "Iteration 376, loss = 0.88974955\n",
      "Iteration 377, loss = 0.88897224\n",
      "Iteration 378, loss = 0.88819474\n",
      "Iteration 379, loss = 0.88741998\n",
      "Iteration 380, loss = 0.88664492\n",
      "Iteration 381, loss = 0.88587067\n",
      "Iteration 382, loss = 0.88509556\n",
      "Iteration 383, loss = 0.88432253\n",
      "Iteration 384, loss = 0.88355667\n",
      "Iteration 385, loss = 0.88279723\n",
      "Iteration 386, loss = 0.88203753\n",
      "Iteration 387, loss = 0.88128100\n",
      "Iteration 388, loss = 0.88052332\n",
      "Iteration 389, loss = 0.87976585\n",
      "Iteration 390, loss = 0.87900903\n",
      "Iteration 391, loss = 0.87825361\n",
      "Iteration 392, loss = 0.87750020\n",
      "Iteration 393, loss = 0.87674784\n",
      "Iteration 394, loss = 0.87599757\n",
      "Iteration 395, loss = 0.87524836\n",
      "Iteration 396, loss = 0.87449991\n",
      "Iteration 397, loss = 0.87375416\n",
      "Iteration 398, loss = 0.87300822\n",
      "Iteration 399, loss = 0.87226381\n",
      "Iteration 400, loss = 0.87152418\n",
      "Iteration 401, loss = 0.87078407\n",
      "Iteration 402, loss = 0.87004319\n",
      "Iteration 403, loss = 0.86930286\n",
      "Iteration 404, loss = 0.86856237\n",
      "Iteration 405, loss = 0.86782239\n",
      "Iteration 406, loss = 0.86708110\n",
      "Iteration 407, loss = 0.86634091\n",
      "Iteration 408, loss = 0.86560070\n",
      "Iteration 409, loss = 0.86486395\n",
      "Iteration 410, loss = 0.86412541\n",
      "Iteration 411, loss = 0.86338702\n",
      "Iteration 412, loss = 0.86264877\n",
      "Iteration 413, loss = 0.86191201\n",
      "Iteration 414, loss = 0.86117780\n",
      "Iteration 415, loss = 0.86044383\n",
      "Iteration 416, loss = 0.85970969\n",
      "Iteration 417, loss = 0.85897937\n",
      "Iteration 418, loss = 0.85824989\n",
      "Iteration 419, loss = 0.85752334\n",
      "Iteration 420, loss = 0.85679740\n",
      "Iteration 421, loss = 0.85607442\n",
      "Iteration 422, loss = 0.85535241\n",
      "Iteration 423, loss = 0.85463339\n",
      "Iteration 424, loss = 0.85391431\n",
      "Iteration 425, loss = 0.85319653\n",
      "Iteration 426, loss = 0.85248220\n",
      "Iteration 427, loss = 0.85176922\n",
      "Iteration 428, loss = 0.85105749\n",
      "Iteration 429, loss = 0.85034643\n",
      "Iteration 430, loss = 0.84963897\n",
      "Iteration 431, loss = 0.84893257\n",
      "Iteration 432, loss = 0.84822657\n",
      "Iteration 433, loss = 0.84752151\n",
      "Iteration 434, loss = 0.84681702\n",
      "Iteration 435, loss = 0.84611168\n",
      "Iteration 436, loss = 0.84541108\n",
      "Iteration 437, loss = 0.84471080\n",
      "Iteration 438, loss = 0.84401044\n",
      "Iteration 439, loss = 0.84331260\n",
      "Iteration 440, loss = 0.84261582\n",
      "Iteration 441, loss = 0.84192079\n",
      "Iteration 442, loss = 0.84122853\n",
      "Iteration 443, loss = 0.84053869\n",
      "Iteration 444, loss = 0.83985155\n",
      "Iteration 445, loss = 0.83916501\n",
      "Iteration 446, loss = 0.83848195\n",
      "Iteration 447, loss = 0.83780028\n",
      "Iteration 448, loss = 0.83711852\n",
      "Iteration 449, loss = 0.83643781\n",
      "Iteration 450, loss = 0.83575565\n",
      "Iteration 451, loss = 0.83506958\n",
      "Iteration 452, loss = 0.83438351\n",
      "Iteration 453, loss = 0.83369637\n",
      "Iteration 454, loss = 0.83300871\n",
      "Iteration 455, loss = 0.83232160\n",
      "Iteration 456, loss = 0.83163533\n",
      "Iteration 457, loss = 0.83094887\n",
      "Iteration 458, loss = 0.83026136\n",
      "Iteration 459, loss = 0.82957519\n",
      "Iteration 460, loss = 0.82888886\n",
      "Iteration 461, loss = 0.82820295\n",
      "Iteration 462, loss = 0.82751834\n",
      "Iteration 463, loss = 0.82683384\n",
      "Iteration 464, loss = 0.82615094\n",
      "Iteration 465, loss = 0.82546846\n",
      "Iteration 466, loss = 0.82478842\n",
      "Iteration 467, loss = 0.82410969\n",
      "Iteration 468, loss = 0.82343286\n",
      "Iteration 469, loss = 0.82275720\n",
      "Iteration 470, loss = 0.82208349\n",
      "Iteration 471, loss = 0.82141213\n",
      "Iteration 472, loss = 0.82074146\n",
      "Iteration 473, loss = 0.82007219\n",
      "Iteration 474, loss = 0.81940636\n",
      "Iteration 475, loss = 0.81874366\n",
      "Iteration 476, loss = 0.81808244\n",
      "Iteration 477, loss = 0.81742188\n",
      "Iteration 478, loss = 0.81676203\n",
      "Iteration 479, loss = 0.81610280\n",
      "Iteration 480, loss = 0.81544658\n",
      "Iteration 481, loss = 0.81479210\n",
      "Iteration 482, loss = 0.81413834\n",
      "Iteration 483, loss = 0.81348522\n",
      "Iteration 484, loss = 0.81283464\n",
      "Iteration 485, loss = 0.81218560\n",
      "Iteration 486, loss = 0.81153760\n",
      "Iteration 487, loss = 0.81089088\n",
      "Iteration 488, loss = 0.81024555\n",
      "Iteration 489, loss = 0.80960332\n",
      "Iteration 490, loss = 0.80896326\n",
      "Iteration 491, loss = 0.80832275\n",
      "Iteration 492, loss = 0.80768482\n",
      "Iteration 493, loss = 0.80704812\n",
      "Iteration 494, loss = 0.80641264\n",
      "Iteration 495, loss = 0.80577864\n",
      "Iteration 496, loss = 0.80514663\n",
      "Iteration 497, loss = 0.80451588\n",
      "Iteration 498, loss = 0.80388688\n",
      "Iteration 499, loss = 0.80325932\n",
      "Iteration 500, loss = 0.80263298\n",
      "Iteration 501, loss = 0.80200844\n",
      "Iteration 502, loss = 0.80138563\n",
      "Iteration 503, loss = 0.80076377\n",
      "Iteration 504, loss = 0.80014353\n",
      "Iteration 505, loss = 0.79952483\n",
      "Iteration 506, loss = 0.79890743\n",
      "Iteration 507, loss = 0.79829054\n",
      "Iteration 508, loss = 0.79767789\n",
      "Iteration 509, loss = 0.79706491\n",
      "Iteration 510, loss = 0.79645112\n",
      "Iteration 511, loss = 0.79583813\n",
      "Iteration 512, loss = 0.79522477\n",
      "Iteration 513, loss = 0.79461105\n",
      "Iteration 514, loss = 0.79399805\n",
      "Iteration 515, loss = 0.79338493\n",
      "Iteration 516, loss = 0.79277331\n",
      "Iteration 517, loss = 0.79216155\n",
      "Iteration 518, loss = 0.79155043\n",
      "Iteration 519, loss = 0.79094026\n",
      "Iteration 520, loss = 0.79033086\n",
      "Iteration 521, loss = 0.78972226\n",
      "Iteration 522, loss = 0.78911483\n",
      "Iteration 523, loss = 0.78850777\n",
      "Iteration 524, loss = 0.78790212\n",
      "Iteration 525, loss = 0.78729738\n",
      "Iteration 526, loss = 0.78669585\n",
      "Iteration 527, loss = 0.78609488\n",
      "Iteration 528, loss = 0.78549400\n",
      "Iteration 529, loss = 0.78489454\n",
      "Iteration 530, loss = 0.78429692\n",
      "Iteration 531, loss = 0.78370026\n",
      "Iteration 532, loss = 0.78310414\n",
      "Iteration 533, loss = 0.78250863\n",
      "Iteration 534, loss = 0.78191451\n",
      "Iteration 535, loss = 0.78132279\n",
      "Iteration 536, loss = 0.78073299\n",
      "Iteration 537, loss = 0.78014516\n",
      "Iteration 538, loss = 0.77955801\n",
      "Iteration 539, loss = 0.77897238\n",
      "Iteration 540, loss = 0.77838788\n",
      "Iteration 541, loss = 0.77780541\n",
      "Iteration 542, loss = 0.77722307\n",
      "Iteration 543, loss = 0.77664029\n",
      "Iteration 544, loss = 0.77605759\n",
      "Iteration 545, loss = 0.77547542\n",
      "Iteration 546, loss = 0.77489294\n",
      "Iteration 547, loss = 0.77430910\n",
      "Iteration 548, loss = 0.77372608\n",
      "Iteration 549, loss = 0.77314238\n",
      "Iteration 550, loss = 0.77255954\n",
      "Iteration 551, loss = 0.77197709\n",
      "Iteration 552, loss = 0.77139548\n",
      "Iteration 553, loss = 0.77081253\n",
      "Iteration 554, loss = 0.77023045\n",
      "Iteration 555, loss = 0.76964944\n",
      "Iteration 556, loss = 0.76906822\n",
      "Iteration 557, loss = 0.76848851\n",
      "Iteration 558, loss = 0.76790947\n",
      "Iteration 559, loss = 0.76733137\n",
      "Iteration 560, loss = 0.76675397\n",
      "Iteration 561, loss = 0.76617761\n",
      "Iteration 562, loss = 0.76560100\n",
      "Iteration 563, loss = 0.76502472\n",
      "Iteration 564, loss = 0.76444932\n",
      "Iteration 565, loss = 0.76387509\n",
      "Iteration 566, loss = 0.76330259\n",
      "Iteration 567, loss = 0.76273080\n",
      "Iteration 568, loss = 0.76216019\n",
      "Iteration 569, loss = 0.76159067\n",
      "Iteration 570, loss = 0.76102227\n",
      "Iteration 571, loss = 0.76045349\n",
      "Iteration 572, loss = 0.75988372\n",
      "Iteration 573, loss = 0.75931470\n",
      "Iteration 574, loss = 0.75874540\n",
      "Iteration 575, loss = 0.75817774\n",
      "Iteration 576, loss = 0.75761071\n",
      "Iteration 577, loss = 0.75704510\n",
      "Iteration 578, loss = 0.75648146\n",
      "Iteration 579, loss = 0.75591880\n",
      "Iteration 580, loss = 0.75535591\n",
      "Iteration 581, loss = 0.75479497\n",
      "Iteration 582, loss = 0.75423536\n",
      "Iteration 583, loss = 0.75367646\n",
      "Iteration 584, loss = 0.75311890\n",
      "Iteration 585, loss = 0.75256017\n",
      "Iteration 586, loss = 0.75200150\n",
      "Iteration 587, loss = 0.75144248\n",
      "Iteration 588, loss = 0.75088592\n",
      "Iteration 589, loss = 0.75032925\n",
      "Iteration 590, loss = 0.74977328\n",
      "Iteration 591, loss = 0.74921786\n",
      "Iteration 592, loss = 0.74866285\n",
      "Iteration 593, loss = 0.74810818\n",
      "Iteration 594, loss = 0.74755607\n",
      "Iteration 595, loss = 0.74700490\n",
      "Iteration 596, loss = 0.74645465\n",
      "Iteration 597, loss = 0.74590557\n",
      "Iteration 598, loss = 0.74535683\n",
      "Iteration 599, loss = 0.74480586\n",
      "Iteration 600, loss = 0.74425542\n",
      "Iteration 601, loss = 0.74370505\n",
      "Iteration 602, loss = 0.74315782\n",
      "Iteration 603, loss = 0.74261030\n",
      "Iteration 604, loss = 0.74206202\n",
      "Iteration 605, loss = 0.74151236\n",
      "Iteration 606, loss = 0.74096429\n",
      "Iteration 607, loss = 0.74042070\n",
      "Iteration 608, loss = 0.73988043\n",
      "Iteration 609, loss = 0.73934436\n",
      "Iteration 610, loss = 0.73881099\n",
      "Iteration 611, loss = 0.73827903\n",
      "Iteration 612, loss = 0.73774917\n",
      "Iteration 613, loss = 0.73722003\n",
      "Iteration 614, loss = 0.73669288\n",
      "Iteration 615, loss = 0.73616907\n",
      "Iteration 616, loss = 0.73564705\n",
      "Iteration 617, loss = 0.73512474\n",
      "Iteration 618, loss = 0.73460284\n",
      "Iteration 619, loss = 0.73408265\n",
      "Iteration 620, loss = 0.73356357\n",
      "Iteration 621, loss = 0.73304621\n",
      "Iteration 622, loss = 0.73253132\n",
      "Iteration 623, loss = 0.73201736\n",
      "Iteration 624, loss = 0.73150518\n",
      "Iteration 625, loss = 0.73099344\n",
      "Iteration 626, loss = 0.73048276\n",
      "Iteration 627, loss = 0.72997245\n",
      "Iteration 628, loss = 0.72946338\n",
      "Iteration 629, loss = 0.72895498\n",
      "Iteration 630, loss = 0.72844920\n",
      "Iteration 631, loss = 0.72794523\n",
      "Iteration 632, loss = 0.72744169\n",
      "Iteration 633, loss = 0.72693916\n",
      "Iteration 634, loss = 0.72643746\n",
      "Iteration 635, loss = 0.72593668\n",
      "Iteration 636, loss = 0.72543669\n",
      "Iteration 637, loss = 0.72493827\n",
      "Iteration 638, loss = 0.72444185\n",
      "Iteration 639, loss = 0.72394374\n",
      "Iteration 640, loss = 0.72344657\n",
      "Iteration 641, loss = 0.72295016\n",
      "Iteration 642, loss = 0.72245315\n",
      "Iteration 643, loss = 0.72195535\n",
      "Iteration 644, loss = 0.72145849\n",
      "Iteration 645, loss = 0.72096230\n",
      "Iteration 646, loss = 0.72046687\n",
      "Iteration 647, loss = 0.71997015\n",
      "Iteration 648, loss = 0.71947274\n",
      "Iteration 649, loss = 0.71897573\n",
      "Iteration 650, loss = 0.71847946\n",
      "Iteration 651, loss = 0.71798357\n",
      "Iteration 652, loss = 0.71748900\n",
      "Iteration 653, loss = 0.71699479\n",
      "Iteration 654, loss = 0.71650064\n",
      "Iteration 655, loss = 0.71600710\n",
      "Iteration 656, loss = 0.71551422\n",
      "Iteration 657, loss = 0.71502117\n",
      "Iteration 658, loss = 0.71452866\n",
      "Iteration 659, loss = 0.71403724\n",
      "Iteration 660, loss = 0.71354667\n",
      "Iteration 661, loss = 0.71305646\n",
      "Iteration 662, loss = 0.71256761\n",
      "Iteration 663, loss = 0.71207947\n",
      "Iteration 664, loss = 0.71159264\n",
      "Iteration 665, loss = 0.71110635\n",
      "Iteration 666, loss = 0.71062187\n",
      "Iteration 667, loss = 0.71013787\n",
      "Iteration 668, loss = 0.70965485\n",
      "Iteration 669, loss = 0.70917364\n",
      "Iteration 670, loss = 0.70869256\n",
      "Iteration 671, loss = 0.70821294\n",
      "Iteration 672, loss = 0.70773408\n",
      "Iteration 673, loss = 0.70725626\n",
      "Iteration 674, loss = 0.70677952\n",
      "Iteration 675, loss = 0.70630352\n",
      "Iteration 676, loss = 0.70582884\n",
      "Iteration 677, loss = 0.70535384\n",
      "Iteration 678, loss = 0.70487805\n",
      "Iteration 679, loss = 0.70440249\n",
      "Iteration 680, loss = 0.70392701\n",
      "Iteration 681, loss = 0.70345230\n",
      "Iteration 682, loss = 0.70297852\n",
      "Iteration 683, loss = 0.70250681\n",
      "Iteration 684, loss = 0.70203635\n",
      "Iteration 685, loss = 0.70156611\n",
      "Iteration 686, loss = 0.70109655\n",
      "Iteration 687, loss = 0.70062830\n",
      "Iteration 688, loss = 0.70016183\n",
      "Iteration 689, loss = 0.69969582\n",
      "Iteration 690, loss = 0.69923029\n",
      "Iteration 691, loss = 0.69876537\n",
      "Iteration 692, loss = 0.69830109\n",
      "Iteration 693, loss = 0.69783895\n",
      "Iteration 694, loss = 0.69737834\n",
      "Iteration 695, loss = 0.69691756\n",
      "Iteration 696, loss = 0.69645736\n",
      "Iteration 697, loss = 0.69599840\n",
      "Iteration 698, loss = 0.69554017\n",
      "Iteration 699, loss = 0.69508224\n",
      "Iteration 700, loss = 0.69462467\n",
      "Iteration 701, loss = 0.69416743\n",
      "Iteration 702, loss = 0.69371089\n",
      "Iteration 703, loss = 0.69325461\n",
      "Iteration 704, loss = 0.69279901\n",
      "Iteration 705, loss = 0.69234392\n",
      "Iteration 706, loss = 0.69189002\n",
      "Iteration 707, loss = 0.69143697\n",
      "Iteration 708, loss = 0.69098383\n",
      "Iteration 709, loss = 0.69053008\n",
      "Iteration 710, loss = 0.69007611\n",
      "Iteration 711, loss = 0.68962352\n",
      "Iteration 712, loss = 0.68917110\n",
      "Iteration 713, loss = 0.68871859\n",
      "Iteration 714, loss = 0.68826762\n",
      "Iteration 715, loss = 0.68782005\n",
      "Iteration 716, loss = 0.68737325\n",
      "Iteration 717, loss = 0.68692707\n",
      "Iteration 718, loss = 0.68648220\n",
      "Iteration 719, loss = 0.68603914\n",
      "Iteration 720, loss = 0.68559864\n",
      "Iteration 721, loss = 0.68515794\n",
      "Iteration 722, loss = 0.68471794\n",
      "Iteration 723, loss = 0.68427822\n",
      "Iteration 724, loss = 0.68383811\n",
      "Iteration 725, loss = 0.68339867\n",
      "Iteration 726, loss = 0.68295920\n",
      "Iteration 727, loss = 0.68252143\n",
      "Iteration 728, loss = 0.68208489\n",
      "Iteration 729, loss = 0.68164946\n",
      "Iteration 730, loss = 0.68121473\n",
      "Iteration 731, loss = 0.68078058\n",
      "Iteration 732, loss = 0.68034704\n",
      "Iteration 733, loss = 0.67991564\n",
      "Iteration 734, loss = 0.67948515\n",
      "Iteration 735, loss = 0.67905580\n",
      "Iteration 736, loss = 0.67862707\n",
      "Iteration 737, loss = 0.67819898\n",
      "Iteration 738, loss = 0.67777244\n",
      "Iteration 739, loss = 0.67734712\n",
      "Iteration 740, loss = 0.67692185\n",
      "Iteration 741, loss = 0.67649756\n",
      "Iteration 742, loss = 0.67607393\n",
      "Iteration 743, loss = 0.67565122\n",
      "Iteration 744, loss = 0.67522941\n",
      "Iteration 745, loss = 0.67480786\n",
      "Iteration 746, loss = 0.67438767\n",
      "Iteration 747, loss = 0.67396818\n",
      "Iteration 748, loss = 0.67354914\n",
      "Iteration 749, loss = 0.67312907\n",
      "Iteration 750, loss = 0.67271021\n",
      "Iteration 751, loss = 0.67229226\n",
      "Iteration 752, loss = 0.67187379\n",
      "Iteration 753, loss = 0.67145502\n",
      "Iteration 754, loss = 0.67103723\n",
      "Iteration 755, loss = 0.67062088\n",
      "Iteration 756, loss = 0.67020534\n",
      "Iteration 757, loss = 0.66978999\n",
      "Iteration 758, loss = 0.66937533\n",
      "Iteration 759, loss = 0.66896156\n",
      "Iteration 760, loss = 0.66854801\n",
      "Iteration 761, loss = 0.66813602\n",
      "Iteration 762, loss = 0.66772500\n",
      "Iteration 763, loss = 0.66731553\n",
      "Iteration 764, loss = 0.66690686\n",
      "Iteration 765, loss = 0.66649878\n",
      "Iteration 766, loss = 0.66609159\n",
      "Iteration 767, loss = 0.66568444\n",
      "Iteration 768, loss = 0.66527803\n",
      "Iteration 769, loss = 0.66487226\n",
      "Iteration 770, loss = 0.66446733\n",
      "Iteration 771, loss = 0.66406037\n",
      "Iteration 772, loss = 0.66365180\n",
      "Iteration 773, loss = 0.66324327\n",
      "Iteration 774, loss = 0.66283451\n",
      "Iteration 775, loss = 0.66242504\n",
      "Iteration 776, loss = 0.66201597\n",
      "Iteration 777, loss = 0.66160636\n",
      "Iteration 778, loss = 0.66119724\n",
      "Iteration 779, loss = 0.66078878\n",
      "Iteration 780, loss = 0.66038095\n",
      "Iteration 781, loss = 0.65997355\n",
      "Iteration 782, loss = 0.65956636\n",
      "Iteration 783, loss = 0.65915930\n",
      "Iteration 784, loss = 0.65875348\n",
      "Iteration 785, loss = 0.65834820\n",
      "Iteration 786, loss = 0.65794276\n",
      "Iteration 787, loss = 0.65753812\n",
      "Iteration 788, loss = 0.65713431\n",
      "Iteration 789, loss = 0.65673015\n",
      "Iteration 790, loss = 0.65632522\n",
      "Iteration 791, loss = 0.65592037\n",
      "Iteration 792, loss = 0.65551627\n",
      "Iteration 793, loss = 0.65511262\n",
      "Iteration 794, loss = 0.65470916\n",
      "Iteration 795, loss = 0.65430684\n",
      "Iteration 796, loss = 0.65390482\n",
      "Iteration 797, loss = 0.65350324\n",
      "Iteration 798, loss = 0.65310260\n",
      "Iteration 799, loss = 0.65270170\n",
      "Iteration 800, loss = 0.65230210\n",
      "Iteration 801, loss = 0.65190295\n",
      "Iteration 802, loss = 0.65150503\n",
      "Iteration 803, loss = 0.65110785\n",
      "Iteration 804, loss = 0.65071095\n",
      "Iteration 805, loss = 0.65031500\n",
      "Iteration 806, loss = 0.64991788\n",
      "Iteration 807, loss = 0.64951954\n",
      "Iteration 808, loss = 0.64912011\n",
      "Iteration 809, loss = 0.64871986\n",
      "Iteration 810, loss = 0.64831946\n",
      "Iteration 811, loss = 0.64791989\n",
      "Iteration 812, loss = 0.64752032\n",
      "Iteration 813, loss = 0.64712073\n",
      "Iteration 814, loss = 0.64672124\n",
      "Iteration 815, loss = 0.64632189\n",
      "Iteration 816, loss = 0.64592330\n",
      "Iteration 817, loss = 0.64552479\n",
      "Iteration 818, loss = 0.64512750\n",
      "Iteration 819, loss = 0.64473051\n",
      "Iteration 820, loss = 0.64433324\n",
      "Iteration 821, loss = 0.64393751\n",
      "Iteration 822, loss = 0.64354226\n",
      "Iteration 823, loss = 0.64314721\n",
      "Iteration 824, loss = 0.64275258\n",
      "Iteration 825, loss = 0.64235806\n",
      "Iteration 826, loss = 0.64196390\n",
      "Iteration 827, loss = 0.64157012\n",
      "Iteration 828, loss = 0.64117684\n",
      "Iteration 829, loss = 0.64078455\n",
      "Iteration 830, loss = 0.64039336\n",
      "Iteration 831, loss = 0.64000546\n",
      "Iteration 832, loss = 0.63961960\n",
      "Iteration 833, loss = 0.63923437\n",
      "Iteration 834, loss = 0.63885017\n",
      "Iteration 835, loss = 0.63846639\n",
      "Iteration 836, loss = 0.63808329\n",
      "Iteration 837, loss = 0.63770088\n",
      "Iteration 838, loss = 0.63731988\n",
      "Iteration 839, loss = 0.63693974\n",
      "Iteration 840, loss = 0.63656031\n",
      "Iteration 841, loss = 0.63618159\n",
      "Iteration 842, loss = 0.63580337\n",
      "Iteration 843, loss = 0.63542628\n",
      "Iteration 844, loss = 0.63504779\n",
      "Iteration 845, loss = 0.63466729\n",
      "Iteration 846, loss = 0.63428726\n",
      "Iteration 847, loss = 0.63390770\n",
      "Iteration 848, loss = 0.63352860\n",
      "Iteration 849, loss = 0.63314955\n",
      "Iteration 850, loss = 0.63277043\n",
      "Iteration 851, loss = 0.63239172\n",
      "Iteration 852, loss = 0.63200954\n",
      "Iteration 853, loss = 0.63162719\n",
      "Iteration 854, loss = 0.63124499\n",
      "Iteration 855, loss = 0.63086345\n",
      "Iteration 856, loss = 0.63048214\n",
      "Iteration 857, loss = 0.63010113\n",
      "Iteration 858, loss = 0.62972056\n",
      "Iteration 859, loss = 0.62934040\n",
      "Iteration 860, loss = 0.62895997\n",
      "Iteration 861, loss = 0.62857969\n",
      "Iteration 862, loss = 0.62819992\n",
      "Iteration 863, loss = 0.62782037\n",
      "Iteration 864, loss = 0.62744139\n",
      "Iteration 865, loss = 0.62706298\n",
      "Iteration 866, loss = 0.62668491\n",
      "Iteration 867, loss = 0.62630724\n",
      "Iteration 868, loss = 0.62593043\n",
      "Iteration 869, loss = 0.62555296\n",
      "Iteration 870, loss = 0.62517557\n",
      "Iteration 871, loss = 0.62479846\n",
      "Iteration 872, loss = 0.62442197\n",
      "Iteration 873, loss = 0.62404602\n",
      "Iteration 874, loss = 0.62367046\n",
      "Iteration 875, loss = 0.62329484\n",
      "Iteration 876, loss = 0.62292220\n",
      "Iteration 877, loss = 0.62255092\n",
      "Iteration 878, loss = 0.62217976\n",
      "Iteration 879, loss = 0.62180887\n",
      "Iteration 880, loss = 0.62143877\n",
      "Iteration 881, loss = 0.62106912\n",
      "Iteration 882, loss = 0.62070000\n",
      "Iteration 883, loss = 0.62033193\n",
      "Iteration 884, loss = 0.61996524\n",
      "Iteration 885, loss = 0.61959966\n",
      "Iteration 886, loss = 0.61923441\n",
      "Iteration 887, loss = 0.61887034\n",
      "Iteration 888, loss = 0.61850659\n",
      "Iteration 889, loss = 0.61814419\n",
      "Iteration 890, loss = 0.61778265\n",
      "Iteration 891, loss = 0.61742150\n",
      "Iteration 892, loss = 0.61706132\n",
      "Iteration 893, loss = 0.61670116\n",
      "Iteration 894, loss = 0.61634135\n",
      "Iteration 895, loss = 0.61598205\n",
      "Iteration 896, loss = 0.61562317\n",
      "Iteration 897, loss = 0.61526409\n",
      "Iteration 898, loss = 0.61490530\n",
      "Iteration 899, loss = 0.61454894\n",
      "Iteration 900, loss = 0.61419349\n",
      "Iteration 901, loss = 0.61383859\n",
      "Iteration 902, loss = 0.61348382\n",
      "Iteration 903, loss = 0.61312938\n",
      "Iteration 904, loss = 0.61277654\n",
      "Iteration 905, loss = 0.61242533\n",
      "Iteration 906, loss = 0.61207513\n",
      "Iteration 907, loss = 0.61172571\n",
      "Iteration 908, loss = 0.61137820\n",
      "Iteration 909, loss = 0.61103156\n",
      "Iteration 910, loss = 0.61068623\n",
      "Iteration 911, loss = 0.61034224\n",
      "Iteration 912, loss = 0.60999898\n",
      "Iteration 913, loss = 0.60965600\n",
      "Iteration 914, loss = 0.60931384\n",
      "Iteration 915, loss = 0.60897289\n",
      "Iteration 916, loss = 0.60863268\n",
      "Iteration 917, loss = 0.60829294\n",
      "Iteration 918, loss = 0.60795378\n",
      "Iteration 919, loss = 0.60761550\n",
      "Iteration 920, loss = 0.60727760\n",
      "Iteration 921, loss = 0.60694035\n",
      "Iteration 922, loss = 0.60660381\n",
      "Iteration 923, loss = 0.60626752\n",
      "Iteration 924, loss = 0.60593178\n",
      "Iteration 925, loss = 0.60559658\n",
      "Iteration 926, loss = 0.60526170\n",
      "Iteration 927, loss = 0.60492704\n",
      "Iteration 928, loss = 0.60459377\n",
      "Iteration 929, loss = 0.60426122\n",
      "Iteration 930, loss = 0.60392951\n",
      "Iteration 931, loss = 0.60359850\n",
      "Iteration 932, loss = 0.60326793\n",
      "Iteration 933, loss = 0.60293752\n",
      "Iteration 934, loss = 0.60260792\n",
      "Iteration 935, loss = 0.60227855\n",
      "Iteration 936, loss = 0.60194981\n",
      "Iteration 937, loss = 0.60162147\n",
      "Iteration 938, loss = 0.60129369\n",
      "Iteration 939, loss = 0.60096629\n",
      "Iteration 940, loss = 0.60063961\n",
      "Iteration 941, loss = 0.60031389\n",
      "Iteration 942, loss = 0.59998858\n",
      "Iteration 943, loss = 0.59966349\n",
      "Iteration 944, loss = 0.59933915\n",
      "Iteration 945, loss = 0.59901508\n",
      "Iteration 946, loss = 0.59869183\n",
      "Iteration 947, loss = 0.59836880\n",
      "Iteration 948, loss = 0.59804583\n",
      "Iteration 949, loss = 0.59772339\n",
      "Iteration 950, loss = 0.59740133\n",
      "Iteration 951, loss = 0.59707968\n",
      "Iteration 952, loss = 0.59675839\n",
      "Iteration 953, loss = 0.59643778\n",
      "Iteration 954, loss = 0.59611750\n",
      "Iteration 955, loss = 0.59579780\n",
      "Iteration 956, loss = 0.59547862\n",
      "Iteration 957, loss = 0.59516002\n",
      "Iteration 958, loss = 0.59484188\n",
      "Iteration 959, loss = 0.59452421\n",
      "Iteration 960, loss = 0.59420706\n",
      "Iteration 961, loss = 0.59389035\n",
      "Iteration 962, loss = 0.59357390\n",
      "Iteration 963, loss = 0.59325804\n",
      "Iteration 964, loss = 0.59294276\n",
      "Iteration 965, loss = 0.59262782\n",
      "Iteration 966, loss = 0.59231341\n",
      "Iteration 967, loss = 0.59199923\n",
      "Iteration 968, loss = 0.59168557\n",
      "Iteration 969, loss = 0.59137272\n",
      "Iteration 970, loss = 0.59106014\n",
      "Iteration 971, loss = 0.59074808\n",
      "Iteration 972, loss = 0.59043652\n",
      "Iteration 973, loss = 0.59012551\n",
      "Iteration 974, loss = 0.58981472\n",
      "Iteration 975, loss = 0.58950453\n",
      "Iteration 976, loss = 0.58919469\n",
      "Iteration 977, loss = 0.58888511\n",
      "Iteration 978, loss = 0.58857586\n",
      "Iteration 979, loss = 0.58826696\n",
      "Iteration 980, loss = 0.58795842\n",
      "Iteration 981, loss = 0.58765037\n",
      "Iteration 982, loss = 0.58734184\n",
      "Iteration 983, loss = 0.58703347\n",
      "Iteration 984, loss = 0.58672556\n",
      "Iteration 985, loss = 0.58641787\n",
      "Iteration 986, loss = 0.58611041\n",
      "Iteration 987, loss = 0.58580333\n",
      "Iteration 988, loss = 0.58549654\n",
      "Iteration 989, loss = 0.58518975\n",
      "Iteration 990, loss = 0.58488324\n",
      "Iteration 991, loss = 0.58457726\n",
      "Iteration 992, loss = 0.58427163\n",
      "Iteration 993, loss = 0.58396630\n",
      "Iteration 994, loss = 0.58366128\n",
      "Iteration 995, loss = 0.58335672\n",
      "Iteration 996, loss = 0.58305256\n",
      "Iteration 997, loss = 0.58274845\n",
      "Iteration 998, loss = 0.58244429\n",
      "Iteration 999, loss = 0.58214067\n",
      "Iteration 1000, loss = 0.58183728\n",
      "Iteration 1001, loss = 0.58153443\n",
      "Iteration 1002, loss = 0.58123183\n",
      "Iteration 1003, loss = 0.58092972\n",
      "Iteration 1004, loss = 0.58062793\n",
      "Iteration 1005, loss = 0.58032641\n",
      "Iteration 1006, loss = 0.58002532\n",
      "Iteration 1007, loss = 0.57972464\n",
      "Iteration 1008, loss = 0.57942445\n",
      "Iteration 1009, loss = 0.57912467\n",
      "Iteration 1010, loss = 0.57882532\n",
      "Iteration 1011, loss = 0.57852614\n",
      "Iteration 1012, loss = 0.57822760\n",
      "Iteration 1013, loss = 0.57792965\n",
      "Iteration 1014, loss = 0.57763168\n",
      "Iteration 1015, loss = 0.57733431\n",
      "Iteration 1016, loss = 0.57703731\n",
      "Iteration 1017, loss = 0.57674072\n",
      "Iteration 1018, loss = 0.57644415\n",
      "Iteration 1019, loss = 0.57614821\n",
      "Iteration 1020, loss = 0.57585175\n",
      "Iteration 1021, loss = 0.57555537\n",
      "Iteration 1022, loss = 0.57525935\n",
      "Iteration 1023, loss = 0.57496348\n",
      "Iteration 1024, loss = 0.57466786\n",
      "Iteration 1025, loss = 0.57437264\n",
      "Iteration 1026, loss = 0.57407790\n",
      "Iteration 1027, loss = 0.57378347\n",
      "Iteration 1028, loss = 0.57348932\n",
      "Iteration 1029, loss = 0.57319507\n",
      "Iteration 1030, loss = 0.57290109\n",
      "Iteration 1031, loss = 0.57260870\n",
      "Iteration 1032, loss = 0.57231780\n",
      "Iteration 1033, loss = 0.57202643\n",
      "Iteration 1034, loss = 0.57173530\n",
      "Iteration 1035, loss = 0.57144470\n",
      "Iteration 1036, loss = 0.57115472\n",
      "Iteration 1037, loss = 0.57086495\n",
      "Iteration 1038, loss = 0.57057565\n",
      "Iteration 1039, loss = 0.57028661\n",
      "Iteration 1040, loss = 0.56999778\n",
      "Iteration 1041, loss = 0.56970923\n",
      "Iteration 1042, loss = 0.56942130\n",
      "Iteration 1043, loss = 0.56913364\n",
      "Iteration 1044, loss = 0.56884626\n",
      "Iteration 1045, loss = 0.56855947\n",
      "Iteration 1046, loss = 0.56827335\n",
      "Iteration 1047, loss = 0.56798751\n",
      "Iteration 1048, loss = 0.56770211\n",
      "Iteration 1049, loss = 0.56741647\n",
      "Iteration 1050, loss = 0.56713149\n",
      "Iteration 1051, loss = 0.56684700\n",
      "Iteration 1052, loss = 0.56656254\n",
      "Iteration 1053, loss = 0.56627838\n",
      "Iteration 1054, loss = 0.56599486\n",
      "Iteration 1055, loss = 0.56571179\n",
      "Iteration 1056, loss = 0.56542892\n",
      "Iteration 1057, loss = 0.56514797\n",
      "Iteration 1058, loss = 0.56486767\n",
      "Iteration 1059, loss = 0.56458776\n",
      "Iteration 1060, loss = 0.56430805\n",
      "Iteration 1061, loss = 0.56402879\n",
      "Iteration 1062, loss = 0.56375007\n",
      "Iteration 1063, loss = 0.56347170\n",
      "Iteration 1064, loss = 0.56319367\n",
      "Iteration 1065, loss = 0.56291538\n",
      "Iteration 1066, loss = 0.56263733\n",
      "Iteration 1067, loss = 0.56235974\n",
      "Iteration 1068, loss = 0.56208267\n",
      "Iteration 1069, loss = 0.56180551\n",
      "Iteration 1070, loss = 0.56152903\n",
      "Iteration 1071, loss = 0.56125300\n",
      "Iteration 1072, loss = 0.56097709\n",
      "Iteration 1073, loss = 0.56070160\n",
      "Iteration 1074, loss = 0.56042628\n",
      "Iteration 1075, loss = 0.56015158\n",
      "Iteration 1076, loss = 0.55987722\n",
      "Iteration 1077, loss = 0.55960256\n",
      "Iteration 1078, loss = 0.55932822\n",
      "Iteration 1079, loss = 0.55905400\n",
      "Iteration 1080, loss = 0.55877994\n",
      "Iteration 1081, loss = 0.55850685\n",
      "Iteration 1082, loss = 0.55823411\n",
      "Iteration 1083, loss = 0.55796147\n",
      "Iteration 1084, loss = 0.55768915\n",
      "Iteration 1085, loss = 0.55741729\n",
      "Iteration 1086, loss = 0.55714561\n",
      "Iteration 1087, loss = 0.55687420\n",
      "Iteration 1088, loss = 0.55660309\n",
      "Iteration 1089, loss = 0.55633222\n",
      "Iteration 1090, loss = 0.55606174\n",
      "Iteration 1091, loss = 0.55579143\n",
      "Iteration 1092, loss = 0.55552136\n",
      "Iteration 1093, loss = 0.55525147\n",
      "Iteration 1094, loss = 0.55498148\n",
      "Iteration 1095, loss = 0.55471149\n",
      "Iteration 1096, loss = 0.55444111\n",
      "Iteration 1097, loss = 0.55417075\n",
      "Iteration 1098, loss = 0.55390060\n",
      "Iteration 1099, loss = 0.55363050\n",
      "Iteration 1100, loss = 0.55336093\n",
      "Iteration 1101, loss = 0.55309287\n",
      "Iteration 1102, loss = 0.55282508\n",
      "Iteration 1103, loss = 0.55255796\n",
      "Iteration 1104, loss = 0.55229083\n",
      "Iteration 1105, loss = 0.55202393\n",
      "Iteration 1106, loss = 0.55175763\n",
      "Iteration 1107, loss = 0.55149192\n",
      "Iteration 1108, loss = 0.55122646\n",
      "Iteration 1109, loss = 0.55096143\n",
      "Iteration 1110, loss = 0.55069685\n",
      "Iteration 1111, loss = 0.55043282\n",
      "Iteration 1112, loss = 0.55016918\n",
      "Iteration 1113, loss = 0.54990566\n",
      "Iteration 1114, loss = 0.54964248\n",
      "Iteration 1115, loss = 0.54937973\n",
      "Iteration 1116, loss = 0.54911729\n",
      "Iteration 1117, loss = 0.54885523\n",
      "Iteration 1118, loss = 0.54859341\n",
      "Iteration 1119, loss = 0.54833198\n",
      "Iteration 1120, loss = 0.54807029\n",
      "Iteration 1121, loss = 0.54780839\n",
      "Iteration 1122, loss = 0.54754656\n",
      "Iteration 1123, loss = 0.54728487\n",
      "Iteration 1124, loss = 0.54702351\n",
      "Iteration 1125, loss = 0.54676208\n",
      "Iteration 1126, loss = 0.54650096\n",
      "Iteration 1127, loss = 0.54624022\n",
      "Iteration 1128, loss = 0.54597964\n",
      "Iteration 1129, loss = 0.54571912\n",
      "Iteration 1130, loss = 0.54545886\n",
      "Iteration 1131, loss = 0.54519888\n",
      "Iteration 1132, loss = 0.54493905\n",
      "Iteration 1133, loss = 0.54467944\n",
      "Iteration 1134, loss = 0.54442035\n",
      "Iteration 1135, loss = 0.54416254\n",
      "Iteration 1136, loss = 0.54390619\n",
      "Iteration 1137, loss = 0.54364987\n",
      "Iteration 1138, loss = 0.54339393\n",
      "Iteration 1139, loss = 0.54313844\n",
      "Iteration 1140, loss = 0.54288317\n",
      "Iteration 1141, loss = 0.54262821\n",
      "Iteration 1142, loss = 0.54237330\n",
      "Iteration 1143, loss = 0.54211838\n",
      "Iteration 1144, loss = 0.54186399\n",
      "Iteration 1145, loss = 0.54160967\n",
      "Iteration 1146, loss = 0.54135534\n",
      "Iteration 1147, loss = 0.54110142\n",
      "Iteration 1148, loss = 0.54084806\n",
      "Iteration 1149, loss = 0.54059500\n",
      "Iteration 1150, loss = 0.54034221\n",
      "Iteration 1151, loss = 0.54008978\n",
      "Iteration 1152, loss = 0.53983761\n",
      "Iteration 1153, loss = 0.53958569\n",
      "Iteration 1154, loss = 0.53933386\n",
      "Iteration 1155, loss = 0.53908242\n",
      "Iteration 1156, loss = 0.53883128\n",
      "Iteration 1157, loss = 0.53858043\n",
      "Iteration 1158, loss = 0.53832960\n",
      "Iteration 1159, loss = 0.53807931\n",
      "Iteration 1160, loss = 0.53782933\n",
      "Iteration 1161, loss = 0.53757960\n",
      "Iteration 1162, loss = 0.53733016\n",
      "Iteration 1163, loss = 0.53708108\n",
      "Iteration 1164, loss = 0.53683205\n",
      "Iteration 1165, loss = 0.53658291\n",
      "Iteration 1166, loss = 0.53633363\n",
      "Iteration 1167, loss = 0.53608488\n",
      "Iteration 1168, loss = 0.53583604\n",
      "Iteration 1169, loss = 0.53558729\n",
      "Iteration 1170, loss = 0.53533874\n",
      "Iteration 1171, loss = 0.53509054\n",
      "Iteration 1172, loss = 0.53484270\n",
      "Iteration 1173, loss = 0.53459513\n",
      "Iteration 1174, loss = 0.53434778\n",
      "Iteration 1175, loss = 0.53410079\n",
      "Iteration 1176, loss = 0.53385395\n",
      "Iteration 1177, loss = 0.53360687\n",
      "Iteration 1178, loss = 0.53335987\n",
      "Iteration 1179, loss = 0.53311287\n",
      "Iteration 1180, loss = 0.53286588\n",
      "Iteration 1181, loss = 0.53261913\n",
      "Iteration 1182, loss = 0.53237254\n",
      "Iteration 1183, loss = 0.53212623\n",
      "Iteration 1184, loss = 0.53188010\n",
      "Iteration 1185, loss = 0.53163440\n",
      "Iteration 1186, loss = 0.53138875\n",
      "Iteration 1187, loss = 0.53114326\n",
      "Iteration 1188, loss = 0.53089792\n",
      "Iteration 1189, loss = 0.53065273\n",
      "Iteration 1190, loss = 0.53040804\n",
      "Iteration 1191, loss = 0.53016331\n",
      "Iteration 1192, loss = 0.52991898\n",
      "Iteration 1193, loss = 0.52967496\n",
      "Iteration 1194, loss = 0.52943114\n",
      "Iteration 1195, loss = 0.52918810\n",
      "Iteration 1196, loss = 0.52894509\n",
      "Iteration 1197, loss = 0.52870225\n",
      "Iteration 1198, loss = 0.52845991\n",
      "Iteration 1199, loss = 0.52821788\n",
      "Iteration 1200, loss = 0.52797588\n",
      "Iteration 1201, loss = 0.52773429\n",
      "Iteration 1202, loss = 0.52749298\n",
      "Iteration 1203, loss = 0.52725179\n",
      "Iteration 1204, loss = 0.52701080\n",
      "Iteration 1205, loss = 0.52677002\n",
      "Iteration 1206, loss = 0.52652959\n",
      "Iteration 1207, loss = 0.52628929\n",
      "Iteration 1208, loss = 0.52604904\n",
      "Iteration 1209, loss = 0.52580883\n",
      "Iteration 1210, loss = 0.52556862\n",
      "Iteration 1211, loss = 0.52532818\n",
      "Iteration 1212, loss = 0.52508796\n",
      "Iteration 1213, loss = 0.52484814\n",
      "Iteration 1214, loss = 0.52460836\n",
      "Iteration 1215, loss = 0.52436856\n",
      "Iteration 1216, loss = 0.52412879\n",
      "Iteration 1217, loss = 0.52388918\n",
      "Iteration 1218, loss = 0.52365000\n",
      "Iteration 1219, loss = 0.52341086\n",
      "Iteration 1220, loss = 0.52317195\n",
      "Iteration 1221, loss = 0.52293323\n",
      "Iteration 1222, loss = 0.52269471\n",
      "Iteration 1223, loss = 0.52245616\n",
      "Iteration 1224, loss = 0.52221783\n",
      "Iteration 1225, loss = 0.52198001\n",
      "Iteration 1226, loss = 0.52174260\n",
      "Iteration 1227, loss = 0.52150559\n",
      "Iteration 1228, loss = 0.52126887\n",
      "Iteration 1229, loss = 0.52103219\n",
      "Iteration 1230, loss = 0.52079582\n",
      "Iteration 1231, loss = 0.52055957\n",
      "Iteration 1232, loss = 0.52032375\n",
      "Iteration 1233, loss = 0.52008816\n",
      "Iteration 1234, loss = 0.51985285\n",
      "Iteration 1235, loss = 0.51961771\n",
      "Iteration 1236, loss = 0.51938298\n",
      "Iteration 1237, loss = 0.51914858\n",
      "Iteration 1238, loss = 0.51891425\n",
      "Iteration 1239, loss = 0.51868000\n",
      "Iteration 1240, loss = 0.51844623\n",
      "Iteration 1241, loss = 0.51821271\n",
      "Iteration 1242, loss = 0.51797935\n",
      "Iteration 1243, loss = 0.51774635\n",
      "Iteration 1244, loss = 0.51751356\n",
      "Iteration 1245, loss = 0.51728094\n",
      "Iteration 1246, loss = 0.51704851\n",
      "Iteration 1247, loss = 0.51681642\n",
      "Iteration 1248, loss = 0.51658442\n",
      "Iteration 1249, loss = 0.51635273\n",
      "Iteration 1250, loss = 0.51612141\n",
      "Iteration 1251, loss = 0.51588997\n",
      "Iteration 1252, loss = 0.51565878\n",
      "Iteration 1253, loss = 0.51542831\n",
      "Iteration 1254, loss = 0.51519796\n",
      "Iteration 1255, loss = 0.51496748\n",
      "Iteration 1256, loss = 0.51473722\n",
      "Iteration 1257, loss = 0.51450762\n",
      "Iteration 1258, loss = 0.51427799\n",
      "Iteration 1259, loss = 0.51404852\n",
      "Iteration 1260, loss = 0.51381938\n",
      "Iteration 1261, loss = 0.51359073\n",
      "Iteration 1262, loss = 0.51336188\n",
      "Iteration 1263, loss = 0.51313317\n",
      "Iteration 1264, loss = 0.51290459\n",
      "Iteration 1265, loss = 0.51267616\n",
      "Iteration 1266, loss = 0.51244777\n",
      "Iteration 1267, loss = 0.51221967\n",
      "Iteration 1268, loss = 0.51199157\n",
      "Iteration 1269, loss = 0.51176351\n",
      "Iteration 1270, loss = 0.51153552\n",
      "Iteration 1271, loss = 0.51130788\n",
      "Iteration 1272, loss = 0.51108048\n",
      "Iteration 1273, loss = 0.51085315\n",
      "Iteration 1274, loss = 0.51062600\n",
      "Iteration 1275, loss = 0.51039919\n",
      "Iteration 1276, loss = 0.51017247\n",
      "Iteration 1277, loss = 0.50994577\n",
      "Iteration 1278, loss = 0.50971930\n",
      "Iteration 1279, loss = 0.50949317\n",
      "Iteration 1280, loss = 0.50926687\n",
      "Iteration 1281, loss = 0.50904072\n",
      "Iteration 1282, loss = 0.50881482\n",
      "Iteration 1283, loss = 0.50858915\n",
      "Iteration 1284, loss = 0.50836378\n",
      "Iteration 1285, loss = 0.50813851\n",
      "Iteration 1286, loss = 0.50791324\n",
      "Iteration 1287, loss = 0.50768818\n",
      "Iteration 1288, loss = 0.50746333\n",
      "Iteration 1289, loss = 0.50723886\n",
      "Iteration 1290, loss = 0.50701453\n",
      "Iteration 1291, loss = 0.50679028\n",
      "Iteration 1292, loss = 0.50656645\n",
      "Iteration 1293, loss = 0.50634276\n",
      "Iteration 1294, loss = 0.50611933\n",
      "Iteration 1295, loss = 0.50589592\n",
      "Iteration 1296, loss = 0.50567304\n",
      "Iteration 1297, loss = 0.50545036\n",
      "Iteration 1298, loss = 0.50522784\n",
      "Iteration 1299, loss = 0.50500553\n",
      "Iteration 1300, loss = 0.50478345\n",
      "Iteration 1301, loss = 0.50456170\n",
      "Iteration 1302, loss = 0.50434012\n",
      "Iteration 1303, loss = 0.50411896\n",
      "Iteration 1304, loss = 0.50389810\n",
      "Iteration 1305, loss = 0.50367748\n",
      "Iteration 1306, loss = 0.50345724\n",
      "Iteration 1307, loss = 0.50323709\n",
      "Iteration 1308, loss = 0.50301699\n",
      "Iteration 1309, loss = 0.50279737\n",
      "Iteration 1310, loss = 0.50257814\n",
      "Iteration 1311, loss = 0.50235885\n",
      "Iteration 1312, loss = 0.50213974\n",
      "Iteration 1313, loss = 0.50192111\n",
      "Iteration 1314, loss = 0.50170256\n",
      "Iteration 1315, loss = 0.50148412\n",
      "Iteration 1316, loss = 0.50126614\n",
      "Iteration 1317, loss = 0.50104861\n",
      "Iteration 1318, loss = 0.50083092\n",
      "Iteration 1319, loss = 0.50061346\n",
      "Iteration 1320, loss = 0.50039648\n",
      "Iteration 1321, loss = 0.50017990\n",
      "Iteration 1322, loss = 0.49996327\n",
      "Iteration 1323, loss = 0.49974696\n",
      "Iteration 1324, loss = 0.49953087\n",
      "Iteration 1325, loss = 0.49931513\n",
      "Iteration 1326, loss = 0.49909971\n",
      "Iteration 1327, loss = 0.49888456\n",
      "Iteration 1328, loss = 0.49866940\n",
      "Iteration 1329, loss = 0.49845447\n",
      "Iteration 1330, loss = 0.49823991\n",
      "Iteration 1331, loss = 0.49802572\n",
      "Iteration 1332, loss = 0.49781166\n",
      "Iteration 1333, loss = 0.49759765\n",
      "Iteration 1334, loss = 0.49738385\n",
      "Iteration 1335, loss = 0.49717029\n",
      "Iteration 1336, loss = 0.49695691\n",
      "Iteration 1337, loss = 0.49674376\n",
      "Iteration 1338, loss = 0.49653103\n",
      "Iteration 1339, loss = 0.49631853\n",
      "Iteration 1340, loss = 0.49610603\n",
      "Iteration 1341, loss = 0.49589407\n",
      "Iteration 1342, loss = 0.49568219\n",
      "Iteration 1343, loss = 0.49547032\n",
      "Iteration 1344, loss = 0.49525865\n",
      "Iteration 1345, loss = 0.49504720\n",
      "Iteration 1346, loss = 0.49483610\n",
      "Iteration 1347, loss = 0.49462549\n",
      "Iteration 1348, loss = 0.49441488\n",
      "Iteration 1349, loss = 0.49420450\n",
      "Iteration 1350, loss = 0.49399419\n",
      "Iteration 1351, loss = 0.49378405\n",
      "Iteration 1352, loss = 0.49357423\n",
      "Iteration 1353, loss = 0.49336470\n",
      "Iteration 1354, loss = 0.49315514\n",
      "Iteration 1355, loss = 0.49294567\n",
      "Iteration 1356, loss = 0.49273598\n",
      "Iteration 1357, loss = 0.49252660\n",
      "Iteration 1358, loss = 0.49231713\n",
      "Iteration 1359, loss = 0.49210804\n",
      "Iteration 1360, loss = 0.49189904\n",
      "Iteration 1361, loss = 0.49169008\n",
      "Iteration 1362, loss = 0.49148109\n",
      "Iteration 1363, loss = 0.49127248\n",
      "Iteration 1364, loss = 0.49106416\n",
      "Iteration 1365, loss = 0.49085611\n",
      "Iteration 1366, loss = 0.49064807\n",
      "Iteration 1367, loss = 0.49043994\n",
      "Iteration 1368, loss = 0.49023232\n",
      "Iteration 1369, loss = 0.49002491\n",
      "Iteration 1370, loss = 0.48981705\n",
      "Iteration 1371, loss = 0.48960933\n",
      "Iteration 1372, loss = 0.48940155\n",
      "Iteration 1373, loss = 0.48919374\n",
      "Iteration 1374, loss = 0.48898603\n",
      "Iteration 1375, loss = 0.48877873\n",
      "Iteration 1376, loss = 0.48857147\n",
      "Iteration 1377, loss = 0.48836416\n",
      "Iteration 1378, loss = 0.48815698\n",
      "Iteration 1379, loss = 0.48794955\n",
      "Iteration 1380, loss = 0.48774220\n",
      "Iteration 1381, loss = 0.48753491\n",
      "Iteration 1382, loss = 0.48732779\n",
      "Iteration 1383, loss = 0.48712071\n",
      "Iteration 1384, loss = 0.48691366\n",
      "Iteration 1385, loss = 0.48670686\n",
      "Iteration 1386, loss = 0.48650021\n",
      "Iteration 1387, loss = 0.48629337\n",
      "Iteration 1388, loss = 0.48608673\n",
      "Iteration 1389, loss = 0.48588043\n",
      "Iteration 1390, loss = 0.48567423\n",
      "Iteration 1391, loss = 0.48546808\n",
      "Iteration 1392, loss = 0.48526208\n",
      "Iteration 1393, loss = 0.48505626\n",
      "Iteration 1394, loss = 0.48485051\n",
      "Iteration 1395, loss = 0.48464515\n",
      "Iteration 1396, loss = 0.48443991\n",
      "Iteration 1397, loss = 0.48423476\n",
      "Iteration 1398, loss = 0.48402978\n",
      "Iteration 1399, loss = 0.48382503\n",
      "Iteration 1400, loss = 0.48362034\n",
      "Iteration 1401, loss = 0.48341589\n",
      "Iteration 1402, loss = 0.48321170\n",
      "Iteration 1403, loss = 0.48300760\n",
      "Iteration 1404, loss = 0.48280381\n",
      "Iteration 1405, loss = 0.48260014\n",
      "Iteration 1406, loss = 0.48239682\n",
      "Iteration 1407, loss = 0.48219389\n",
      "Iteration 1408, loss = 0.48199117\n",
      "Iteration 1409, loss = 0.48178841\n",
      "Iteration 1410, loss = 0.48158602\n",
      "Iteration 1411, loss = 0.48138360\n",
      "Iteration 1412, loss = 0.48118126\n",
      "Iteration 1413, loss = 0.48097892\n",
      "Iteration 1414, loss = 0.48077617\n",
      "Iteration 1415, loss = 0.48057359\n",
      "Iteration 1416, loss = 0.48037116\n",
      "Iteration 1417, loss = 0.48016887\n",
      "Iteration 1418, loss = 0.47996651\n",
      "Iteration 1419, loss = 0.47976418\n",
      "Iteration 1420, loss = 0.47956210\n",
      "Iteration 1421, loss = 0.47935990\n",
      "Iteration 1422, loss = 0.47915768\n",
      "Iteration 1423, loss = 0.47895568\n",
      "Iteration 1424, loss = 0.47875365\n",
      "Iteration 1425, loss = 0.47855164\n",
      "Iteration 1426, loss = 0.47835007\n",
      "Iteration 1427, loss = 0.47814834\n",
      "Iteration 1428, loss = 0.47794620\n",
      "Iteration 1429, loss = 0.47774424\n",
      "Iteration 1430, loss = 0.47754230\n",
      "Iteration 1431, loss = 0.47734053\n",
      "Iteration 1432, loss = 0.47713870\n",
      "Iteration 1433, loss = 0.47693707\n",
      "Iteration 1434, loss = 0.47673555\n",
      "Iteration 1435, loss = 0.47653408\n",
      "Iteration 1436, loss = 0.47633267\n",
      "Iteration 1437, loss = 0.47613155\n",
      "Iteration 1438, loss = 0.47593055\n",
      "Iteration 1439, loss = 0.47572980\n",
      "Iteration 1440, loss = 0.47552916\n",
      "Iteration 1441, loss = 0.47532892\n",
      "Iteration 1442, loss = 0.47512881\n",
      "Iteration 1443, loss = 0.47492863\n",
      "Iteration 1444, loss = 0.47472853\n",
      "Iteration 1445, loss = 0.47452855\n",
      "Iteration 1446, loss = 0.47432882\n",
      "Iteration 1447, loss = 0.47412927\n",
      "Iteration 1448, loss = 0.47392981\n",
      "Iteration 1449, loss = 0.47373051\n",
      "Iteration 1450, loss = 0.47353146\n",
      "Iteration 1451, loss = 0.47333245\n",
      "Iteration 1452, loss = 0.47313362\n",
      "Iteration 1453, loss = 0.47293480\n",
      "Iteration 1454, loss = 0.47273615\n",
      "Iteration 1455, loss = 0.47253756\n",
      "Iteration 1456, loss = 0.47233892\n",
      "Iteration 1457, loss = 0.47214040\n",
      "Iteration 1458, loss = 0.47194222\n",
      "Iteration 1459, loss = 0.47174423\n",
      "Iteration 1460, loss = 0.47154625\n",
      "Iteration 1461, loss = 0.47134865\n",
      "Iteration 1462, loss = 0.47115100\n",
      "Iteration 1463, loss = 0.47095335\n",
      "Iteration 1464, loss = 0.47075569\n",
      "Iteration 1465, loss = 0.47055829\n",
      "Iteration 1466, loss = 0.47036097\n",
      "Iteration 1467, loss = 0.47016368\n",
      "Iteration 1468, loss = 0.46996678\n",
      "Iteration 1469, loss = 0.46976999\n",
      "Iteration 1470, loss = 0.46957315\n",
      "Iteration 1471, loss = 0.46937658\n",
      "Iteration 1472, loss = 0.46918027\n",
      "Iteration 1473, loss = 0.46898412\n",
      "Iteration 1474, loss = 0.46878802\n",
      "Iteration 1475, loss = 0.46859216\n",
      "Iteration 1476, loss = 0.46839643\n",
      "Iteration 1477, loss = 0.46820101\n",
      "Iteration 1478, loss = 0.46800576\n",
      "Iteration 1479, loss = 0.46781082\n",
      "Iteration 1480, loss = 0.46761611\n",
      "Iteration 1481, loss = 0.46742133\n",
      "Iteration 1482, loss = 0.46722646\n",
      "Iteration 1483, loss = 0.46703183\n",
      "Iteration 1484, loss = 0.46683744\n",
      "Iteration 1485, loss = 0.46664320\n",
      "Iteration 1486, loss = 0.46644912\n",
      "Iteration 1487, loss = 0.46625529\n",
      "Iteration 1488, loss = 0.46606241\n",
      "Iteration 1489, loss = 0.46586977\n",
      "Iteration 1490, loss = 0.46567720\n",
      "Iteration 1491, loss = 0.46548486\n",
      "Iteration 1492, loss = 0.46529252\n",
      "Iteration 1493, loss = 0.46510037\n",
      "Iteration 1494, loss = 0.46490846\n",
      "Iteration 1495, loss = 0.46471677\n",
      "Iteration 1496, loss = 0.46452502\n",
      "Iteration 1497, loss = 0.46433353\n",
      "Iteration 1498, loss = 0.46414232\n",
      "Iteration 1499, loss = 0.46395111\n",
      "Iteration 1500, loss = 0.46376026\n",
      "Iteration 1501, loss = 0.46356953\n",
      "Iteration 1502, loss = 0.46337918\n",
      "Iteration 1503, loss = 0.46318919\n",
      "Iteration 1504, loss = 0.46299949\n",
      "Iteration 1505, loss = 0.46280989\n",
      "Iteration 1506, loss = 0.46262051\n",
      "Iteration 1507, loss = 0.46243109\n",
      "Iteration 1508, loss = 0.46224204\n",
      "Iteration 1509, loss = 0.46205330\n",
      "Iteration 1510, loss = 0.46186465\n",
      "Iteration 1511, loss = 0.46167608\n",
      "Iteration 1512, loss = 0.46148751\n",
      "Iteration 1513, loss = 0.46129947\n",
      "Iteration 1514, loss = 0.46111157\n",
      "Iteration 1515, loss = 0.46092374\n",
      "Iteration 1516, loss = 0.46073606\n",
      "Iteration 1517, loss = 0.46054867\n",
      "Iteration 1518, loss = 0.46036159\n",
      "Iteration 1519, loss = 0.46017446\n",
      "Iteration 1520, loss = 0.45998768\n",
      "Iteration 1521, loss = 0.45980114\n",
      "Iteration 1522, loss = 0.45961453\n",
      "Iteration 1523, loss = 0.45942791\n",
      "Iteration 1524, loss = 0.45924151\n",
      "Iteration 1525, loss = 0.45905509\n",
      "Iteration 1526, loss = 0.45886904\n",
      "Iteration 1527, loss = 0.45868303\n",
      "Iteration 1528, loss = 0.45849720\n",
      "Iteration 1529, loss = 0.45831139\n",
      "Iteration 1530, loss = 0.45812516\n",
      "Iteration 1531, loss = 0.45793857\n",
      "Iteration 1532, loss = 0.45775212\n",
      "Iteration 1533, loss = 0.45756530\n",
      "Iteration 1534, loss = 0.45737884\n",
      "Iteration 1535, loss = 0.45719233\n",
      "Iteration 1536, loss = 0.45700571\n",
      "Iteration 1537, loss = 0.45681896\n",
      "Iteration 1538, loss = 0.45663242\n",
      "Iteration 1539, loss = 0.45644786\n",
      "Iteration 1540, loss = 0.45626333\n",
      "Iteration 1541, loss = 0.45607893\n",
      "Iteration 1542, loss = 0.45589468\n",
      "Iteration 1543, loss = 0.45570937\n",
      "Iteration 1544, loss = 0.45552362\n",
      "Iteration 1545, loss = 0.45533802\n",
      "Iteration 1546, loss = 0.45515200\n",
      "Iteration 1547, loss = 0.45496648\n",
      "Iteration 1548, loss = 0.45478252\n",
      "Iteration 1549, loss = 0.45459802\n",
      "Iteration 1550, loss = 0.45441210\n",
      "Iteration 1551, loss = 0.45422434\n",
      "Iteration 1552, loss = 0.45403613\n",
      "Iteration 1553, loss = 0.45384901\n",
      "Iteration 1554, loss = 0.45366380\n",
      "Iteration 1555, loss = 0.45347879\n",
      "Iteration 1556, loss = 0.45329385\n",
      "Iteration 1557, loss = 0.45310900\n",
      "Iteration 1558, loss = 0.45292382\n",
      "Iteration 1559, loss = 0.45273919\n",
      "Iteration 1560, loss = 0.45255459\n",
      "Iteration 1561, loss = 0.45236921\n",
      "Iteration 1562, loss = 0.45218295\n",
      "Iteration 1563, loss = 0.45199643\n",
      "Iteration 1564, loss = 0.45181010\n",
      "Iteration 1565, loss = 0.45162353\n",
      "Iteration 1566, loss = 0.45143813\n",
      "Iteration 1567, loss = 0.45125415\n",
      "Iteration 1568, loss = 0.45107051\n",
      "Iteration 1569, loss = 0.45088691\n",
      "Iteration 1570, loss = 0.45070385\n",
      "Iteration 1571, loss = 0.45052104\n",
      "Iteration 1572, loss = 0.45033854\n",
      "Iteration 1573, loss = 0.45015627\n",
      "Iteration 1574, loss = 0.44997390\n",
      "Iteration 1575, loss = 0.44979148\n",
      "Iteration 1576, loss = 0.44960927\n",
      "Iteration 1577, loss = 0.44942723\n",
      "Iteration 1578, loss = 0.44924458\n",
      "Iteration 1579, loss = 0.44906214\n",
      "Iteration 1580, loss = 0.44887822\n",
      "Iteration 1581, loss = 0.44869402\n",
      "Iteration 1582, loss = 0.44850983\n",
      "Iteration 1583, loss = 0.44832667\n",
      "Iteration 1584, loss = 0.44814417\n",
      "Iteration 1585, loss = 0.44796293\n",
      "Iteration 1586, loss = 0.44778184\n",
      "Iteration 1587, loss = 0.44760087\n",
      "Iteration 1588, loss = 0.44742002\n",
      "Iteration 1589, loss = 0.44723929\n",
      "Iteration 1590, loss = 0.44705882\n",
      "Iteration 1591, loss = 0.44687859\n",
      "Iteration 1592, loss = 0.44669845\n",
      "Iteration 1593, loss = 0.44651839\n",
      "Iteration 1594, loss = 0.44633865\n",
      "Iteration 1595, loss = 0.44615909\n",
      "Iteration 1596, loss = 0.44598004\n",
      "Iteration 1597, loss = 0.44580146\n",
      "Iteration 1598, loss = 0.44562329\n",
      "Iteration 1599, loss = 0.44544544\n",
      "Iteration 1600, loss = 0.44526745\n",
      "Iteration 1601, loss = 0.44508966\n",
      "Iteration 1602, loss = 0.44491191\n",
      "Iteration 1603, loss = 0.44473441\n",
      "Iteration 1604, loss = 0.44455711\n",
      "Iteration 1605, loss = 0.44438004\n",
      "Iteration 1606, loss = 0.44420302\n",
      "Iteration 1607, loss = 0.44402605\n",
      "Iteration 1608, loss = 0.44384943\n",
      "Iteration 1609, loss = 0.44367314\n",
      "Iteration 1610, loss = 0.44349695\n",
      "Iteration 1611, loss = 0.44332083\n",
      "Iteration 1612, loss = 0.44314509\n",
      "Iteration 1613, loss = 0.44296972\n",
      "Iteration 1614, loss = 0.44279453\n",
      "Iteration 1615, loss = 0.44261966\n",
      "Iteration 1616, loss = 0.44244498\n",
      "Iteration 1617, loss = 0.44227069\n",
      "Iteration 1618, loss = 0.44209657\n",
      "Iteration 1619, loss = 0.44192256\n",
      "Iteration 1620, loss = 0.44174867\n",
      "Iteration 1621, loss = 0.44157491\n",
      "Iteration 1622, loss = 0.44140115\n",
      "Iteration 1623, loss = 0.44122818\n",
      "Iteration 1624, loss = 0.44105537\n",
      "Iteration 1625, loss = 0.44088267\n",
      "Iteration 1626, loss = 0.44071017\n",
      "Iteration 1627, loss = 0.44053791\n",
      "Iteration 1628, loss = 0.44036550\n",
      "Iteration 1629, loss = 0.44019341\n",
      "Iteration 1630, loss = 0.44002157\n",
      "Iteration 1631, loss = 0.43984980\n",
      "Iteration 1632, loss = 0.43967833\n",
      "Iteration 1633, loss = 0.43950704\n",
      "Iteration 1634, loss = 0.43933561\n",
      "Iteration 1635, loss = 0.43916431\n",
      "Iteration 1636, loss = 0.43899342\n",
      "Iteration 1637, loss = 0.43882240\n",
      "Iteration 1638, loss = 0.43865159\n",
      "Iteration 1639, loss = 0.43848099\n",
      "Iteration 1640, loss = 0.43831053\n",
      "Iteration 1641, loss = 0.43814003\n",
      "Iteration 1642, loss = 0.43796985\n",
      "Iteration 1643, loss = 0.43779987\n",
      "Iteration 1644, loss = 0.43763000\n",
      "Iteration 1645, loss = 0.43746061\n",
      "Iteration 1646, loss = 0.43729147\n",
      "Iteration 1647, loss = 0.43712234\n",
      "Iteration 1648, loss = 0.43695365\n",
      "Iteration 1649, loss = 0.43678496\n",
      "Iteration 1650, loss = 0.43661656\n",
      "Iteration 1651, loss = 0.43644818\n",
      "Iteration 1652, loss = 0.43627988\n",
      "Iteration 1653, loss = 0.43611191\n",
      "Iteration 1654, loss = 0.43594405\n",
      "Iteration 1655, loss = 0.43577624\n",
      "Iteration 1656, loss = 0.43560855\n",
      "Iteration 1657, loss = 0.43544105\n",
      "Iteration 1658, loss = 0.43527365\n",
      "Iteration 1659, loss = 0.43510632\n",
      "Iteration 1660, loss = 0.43493878\n",
      "Iteration 1661, loss = 0.43477159\n",
      "Iteration 1662, loss = 0.43460429\n",
      "Iteration 1663, loss = 0.43443726\n",
      "Iteration 1664, loss = 0.43427036\n",
      "Iteration 1665, loss = 0.43410354\n",
      "Iteration 1666, loss = 0.43393836\n",
      "Iteration 1667, loss = 0.43377347\n",
      "Iteration 1668, loss = 0.43360878\n",
      "Iteration 1669, loss = 0.43344422\n",
      "Iteration 1670, loss = 0.43327983\n",
      "Iteration 1671, loss = 0.43311568\n",
      "Iteration 1672, loss = 0.43295178\n",
      "Iteration 1673, loss = 0.43278776\n",
      "Iteration 1674, loss = 0.43262397\n",
      "Iteration 1675, loss = 0.43246058\n",
      "Iteration 1676, loss = 0.43229738\n",
      "Iteration 1677, loss = 0.43213416\n",
      "Iteration 1678, loss = 0.43197098\n",
      "Iteration 1679, loss = 0.43180793\n",
      "Iteration 1680, loss = 0.43164490\n",
      "Iteration 1681, loss = 0.43148203\n",
      "Iteration 1682, loss = 0.43131943\n",
      "Iteration 1683, loss = 0.43115493\n",
      "Iteration 1684, loss = 0.43099012\n",
      "Iteration 1685, loss = 0.43082502\n",
      "Iteration 1686, loss = 0.43066036\n",
      "Iteration 1687, loss = 0.43049517\n",
      "Iteration 1688, loss = 0.43032977\n",
      "Iteration 1689, loss = 0.43016436\n",
      "Iteration 1690, loss = 0.42999890\n",
      "Iteration 1691, loss = 0.42983328\n",
      "Iteration 1692, loss = 0.42966779\n",
      "Iteration 1693, loss = 0.42950213\n",
      "Iteration 1694, loss = 0.42933657\n",
      "Iteration 1695, loss = 0.42917117\n",
      "Iteration 1696, loss = 0.42900568\n",
      "Iteration 1697, loss = 0.42884056\n",
      "Iteration 1698, loss = 0.42867548\n",
      "Iteration 1699, loss = 0.42851065\n",
      "Iteration 1700, loss = 0.42834576\n",
      "Iteration 1701, loss = 0.42818088\n",
      "Iteration 1702, loss = 0.42801594\n",
      "Iteration 1703, loss = 0.42785098\n",
      "Iteration 1704, loss = 0.42768616\n",
      "Iteration 1705, loss = 0.42752148\n",
      "Iteration 1706, loss = 0.42735673\n",
      "Iteration 1707, loss = 0.42719255\n",
      "Iteration 1708, loss = 0.42702832\n",
      "Iteration 1709, loss = 0.42686427\n",
      "Iteration 1710, loss = 0.42670022\n",
      "Iteration 1711, loss = 0.42653635\n",
      "Iteration 1712, loss = 0.42637283\n",
      "Iteration 1713, loss = 0.42620919\n",
      "Iteration 1714, loss = 0.42604591\n",
      "Iteration 1715, loss = 0.42588302\n",
      "Iteration 1716, loss = 0.42572048\n",
      "Iteration 1717, loss = 0.42555801\n",
      "Iteration 1718, loss = 0.42539562\n",
      "Iteration 1719, loss = 0.42523340\n",
      "Iteration 1720, loss = 0.42507155\n",
      "Iteration 1721, loss = 0.42490976\n",
      "Iteration 1722, loss = 0.42474818\n",
      "Iteration 1723, loss = 0.42458652\n",
      "Iteration 1724, loss = 0.42442515\n",
      "Iteration 1725, loss = 0.42426410\n",
      "Iteration 1726, loss = 0.42410335\n",
      "Iteration 1727, loss = 0.42394277\n",
      "Iteration 1728, loss = 0.42378235\n",
      "Iteration 1729, loss = 0.42362239\n",
      "Iteration 1730, loss = 0.42346255\n",
      "Iteration 1731, loss = 0.42330283\n",
      "Iteration 1732, loss = 0.42314322\n",
      "Iteration 1733, loss = 0.42298369\n",
      "Iteration 1734, loss = 0.42282440\n",
      "Iteration 1735, loss = 0.42266503\n",
      "Iteration 1736, loss = 0.42250599\n",
      "Iteration 1737, loss = 0.42234727\n",
      "Iteration 1738, loss = 0.42218861\n",
      "Iteration 1739, loss = 0.42203017\n",
      "Iteration 1740, loss = 0.42187203\n",
      "Iteration 1741, loss = 0.42171389\n",
      "Iteration 1742, loss = 0.42155590\n",
      "Iteration 1743, loss = 0.42139803\n",
      "Iteration 1744, loss = 0.42124036\n",
      "Iteration 1745, loss = 0.42108298\n",
      "Iteration 1746, loss = 0.42092569\n",
      "Iteration 1747, loss = 0.42076866\n",
      "Iteration 1748, loss = 0.42061156\n",
      "Iteration 1749, loss = 0.42045498\n",
      "Iteration 1750, loss = 0.42029846\n",
      "Iteration 1751, loss = 0.42014202\n",
      "Iteration 1752, loss = 0.41998578\n",
      "Iteration 1753, loss = 0.41982973\n",
      "Iteration 1754, loss = 0.41967404\n",
      "Iteration 1755, loss = 0.41951845\n",
      "Iteration 1756, loss = 0.41936351\n",
      "Iteration 1757, loss = 0.41920871\n",
      "Iteration 1758, loss = 0.41905400\n",
      "Iteration 1759, loss = 0.41889898\n",
      "Iteration 1760, loss = 0.41874404\n",
      "Iteration 1761, loss = 0.41858956\n",
      "Iteration 1762, loss = 0.41843521\n",
      "Iteration 1763, loss = 0.41828079\n",
      "Iteration 1764, loss = 0.41812642\n",
      "Iteration 1765, loss = 0.41797330\n",
      "Iteration 1766, loss = 0.41782043\n",
      "Iteration 1767, loss = 0.41766791\n",
      "Iteration 1768, loss = 0.41751524\n",
      "Iteration 1769, loss = 0.41736228\n",
      "Iteration 1770, loss = 0.41720941\n",
      "Iteration 1771, loss = 0.41705658\n",
      "Iteration 1772, loss = 0.41690383\n",
      "Iteration 1773, loss = 0.41675102\n",
      "Iteration 1774, loss = 0.41659865\n",
      "Iteration 1775, loss = 0.41644698\n",
      "Iteration 1776, loss = 0.41629543\n",
      "Iteration 1777, loss = 0.41614421\n",
      "Iteration 1778, loss = 0.41599276\n",
      "Iteration 1779, loss = 0.41584148\n",
      "Iteration 1780, loss = 0.41569035\n",
      "Iteration 1781, loss = 0.41553930\n",
      "Iteration 1782, loss = 0.41538829\n",
      "Iteration 1783, loss = 0.41523741\n",
      "Iteration 1784, loss = 0.41508653\n",
      "Iteration 1785, loss = 0.41493579\n",
      "Iteration 1786, loss = 0.41478539\n",
      "Iteration 1787, loss = 0.41463562\n",
      "Iteration 1788, loss = 0.41448562\n",
      "Iteration 1789, loss = 0.41433554\n",
      "Iteration 1790, loss = 0.41418581\n",
      "Iteration 1791, loss = 0.41403667\n",
      "Iteration 1792, loss = 0.41388739\n",
      "Iteration 1793, loss = 0.41373826\n",
      "Iteration 1794, loss = 0.41358936\n",
      "Iteration 1795, loss = 0.41344052\n",
      "Iteration 1796, loss = 0.41329225\n",
      "Iteration 1797, loss = 0.41314381\n",
      "Iteration 1798, loss = 0.41299525\n",
      "Iteration 1799, loss = 0.41284729\n",
      "Iteration 1800, loss = 0.41269960\n",
      "Iteration 1801, loss = 0.41255188\n",
      "Iteration 1802, loss = 0.41240441\n",
      "Iteration 1803, loss = 0.41225715\n",
      "Iteration 1804, loss = 0.41210982\n",
      "Iteration 1805, loss = 0.41196281\n",
      "Iteration 1806, loss = 0.41181605\n",
      "Iteration 1807, loss = 0.41166942\n",
      "Iteration 1808, loss = 0.41152317\n",
      "Iteration 1809, loss = 0.41137675\n",
      "Iteration 1810, loss = 0.41123014\n",
      "Iteration 1811, loss = 0.41108346\n",
      "Iteration 1812, loss = 0.41093680\n",
      "Iteration 1813, loss = 0.41079026\n",
      "Iteration 1814, loss = 0.41064399\n",
      "Iteration 1815, loss = 0.41049766\n",
      "Iteration 1816, loss = 0.41035128\n",
      "Iteration 1817, loss = 0.41020484\n",
      "Iteration 1818, loss = 0.41005865\n",
      "Iteration 1819, loss = 0.40991240\n",
      "Iteration 1820, loss = 0.40976608\n",
      "Iteration 1821, loss = 0.40962014\n",
      "Iteration 1822, loss = 0.40947449\n",
      "Iteration 1823, loss = 0.40932874\n",
      "Iteration 1824, loss = 0.40918289\n",
      "Iteration 1825, loss = 0.40903830\n",
      "Iteration 1826, loss = 0.40889442\n",
      "Iteration 1827, loss = 0.40874926\n",
      "Iteration 1828, loss = 0.40860407\n",
      "Iteration 1829, loss = 0.40845881\n",
      "Iteration 1830, loss = 0.40831383\n",
      "Iteration 1831, loss = 0.40816892\n",
      "Iteration 1832, loss = 0.40802423\n",
      "Iteration 1833, loss = 0.40787959\n",
      "Iteration 1834, loss = 0.40773598\n",
      "Iteration 1835, loss = 0.40759258\n",
      "Iteration 1836, loss = 0.40744953\n",
      "Iteration 1837, loss = 0.40730656\n",
      "Iteration 1838, loss = 0.40716374\n",
      "Iteration 1839, loss = 0.40702102\n",
      "Iteration 1840, loss = 0.40687880\n",
      "Iteration 1841, loss = 0.40673669\n",
      "Iteration 1842, loss = 0.40659450\n",
      "Iteration 1843, loss = 0.40645237\n",
      "Iteration 1844, loss = 0.40631066\n",
      "Iteration 1845, loss = 0.40616913\n",
      "Iteration 1846, loss = 0.40602772\n",
      "Iteration 1847, loss = 0.40588603\n",
      "Iteration 1848, loss = 0.40574271\n",
      "Iteration 1849, loss = 0.40559869\n",
      "Iteration 1850, loss = 0.40545555\n",
      "Iteration 1851, loss = 0.40531325\n",
      "Iteration 1852, loss = 0.40517089\n",
      "Iteration 1853, loss = 0.40502836\n",
      "Iteration 1854, loss = 0.40488606\n",
      "Iteration 1855, loss = 0.40474329\n",
      "Iteration 1856, loss = 0.40459931\n",
      "Iteration 1857, loss = 0.40445516\n",
      "Iteration 1858, loss = 0.40431416\n",
      "Iteration 1859, loss = 0.40417299\n",
      "Iteration 1860, loss = 0.40403121\n",
      "Iteration 1861, loss = 0.40389102\n",
      "Iteration 1862, loss = 0.40375067\n",
      "Iteration 1863, loss = 0.40361209\n",
      "Iteration 1864, loss = 0.40347343\n",
      "Iteration 1865, loss = 0.40333506\n",
      "Iteration 1866, loss = 0.40319690\n",
      "Iteration 1867, loss = 0.40305883\n",
      "Iteration 1868, loss = 0.40292096\n",
      "Iteration 1869, loss = 0.40278320\n",
      "Iteration 1870, loss = 0.40264541\n",
      "Iteration 1871, loss = 0.40250796\n",
      "Iteration 1872, loss = 0.40237098\n",
      "Iteration 1873, loss = 0.40223390\n",
      "Iteration 1874, loss = 0.40209712\n",
      "Iteration 1875, loss = 0.40196038\n",
      "Iteration 1876, loss = 0.40182397\n",
      "Iteration 1877, loss = 0.40168760\n",
      "Iteration 1878, loss = 0.40155167\n",
      "Iteration 1879, loss = 0.40141580\n",
      "Iteration 1880, loss = 0.40127986\n",
      "Iteration 1881, loss = 0.40114403\n",
      "Iteration 1882, loss = 0.40100845\n",
      "Iteration 1883, loss = 0.40087304\n",
      "Iteration 1884, loss = 0.40073779\n",
      "Iteration 1885, loss = 0.40060262\n",
      "Iteration 1886, loss = 0.40046770\n",
      "Iteration 1887, loss = 0.40033287\n",
      "Iteration 1888, loss = 0.40019795\n",
      "Iteration 1889, loss = 0.40006334\n",
      "Iteration 1890, loss = 0.39992844\n",
      "Iteration 1891, loss = 0.39979367\n",
      "Iteration 1892, loss = 0.39965912\n",
      "Iteration 1893, loss = 0.39952441\n",
      "Iteration 1894, loss = 0.39938973\n",
      "Iteration 1895, loss = 0.39925526\n",
      "Iteration 1896, loss = 0.39912093\n",
      "Iteration 1897, loss = 0.39898654\n",
      "Iteration 1898, loss = 0.39885253\n",
      "Iteration 1899, loss = 0.39871862\n",
      "Iteration 1900, loss = 0.39858475\n",
      "Iteration 1901, loss = 0.39845095\n",
      "Iteration 1902, loss = 0.39831723\n",
      "Iteration 1903, loss = 0.39818356\n",
      "Iteration 1904, loss = 0.39805017\n",
      "Iteration 1905, loss = 0.39791704\n",
      "Iteration 1906, loss = 0.39778365\n",
      "Iteration 1907, loss = 0.39765044\n",
      "Iteration 1908, loss = 0.39751768\n",
      "Iteration 1909, loss = 0.39738491\n",
      "Iteration 1910, loss = 0.39725224\n",
      "Iteration 1911, loss = 0.39711979\n",
      "Iteration 1912, loss = 0.39698763\n",
      "Iteration 1913, loss = 0.39685553\n",
      "Iteration 1914, loss = 0.39672369\n",
      "Iteration 1915, loss = 0.39659214\n",
      "Iteration 1916, loss = 0.39646064\n",
      "Iteration 1917, loss = 0.39632957\n",
      "Iteration 1918, loss = 0.39619848\n",
      "Iteration 1919, loss = 0.39606735\n",
      "Iteration 1920, loss = 0.39593633\n",
      "Iteration 1921, loss = 0.39580596\n",
      "Iteration 1922, loss = 0.39567549\n",
      "Iteration 1923, loss = 0.39554522\n",
      "Iteration 1924, loss = 0.39541486\n",
      "Iteration 1925, loss = 0.39528454\n",
      "Iteration 1926, loss = 0.39515460\n",
      "Iteration 1927, loss = 0.39502478\n",
      "Iteration 1928, loss = 0.39489494\n",
      "Iteration 1929, loss = 0.39476529\n",
      "Iteration 1930, loss = 0.39463575\n",
      "Iteration 1931, loss = 0.39450649\n",
      "Iteration 1932, loss = 0.39437703\n",
      "Iteration 1933, loss = 0.39424790\n",
      "Iteration 1934, loss = 0.39411888\n",
      "Iteration 1935, loss = 0.39398988\n",
      "Iteration 1936, loss = 0.39386046\n",
      "Iteration 1937, loss = 0.39373084\n",
      "Iteration 1938, loss = 0.39360101\n",
      "Iteration 1939, loss = 0.39347118\n",
      "Iteration 1940, loss = 0.39334238\n",
      "Iteration 1941, loss = 0.39321356\n",
      "Iteration 1942, loss = 0.39308506\n",
      "Iteration 1943, loss = 0.39295691\n",
      "Iteration 1944, loss = 0.39282888\n",
      "Iteration 1945, loss = 0.39270070\n",
      "Iteration 1946, loss = 0.39257186\n",
      "Iteration 1947, loss = 0.39244240\n",
      "Iteration 1948, loss = 0.39231335\n",
      "Iteration 1949, loss = 0.39218470\n",
      "Iteration 1950, loss = 0.39205711\n",
      "Iteration 1951, loss = 0.39192987\n",
      "Iteration 1952, loss = 0.39180284\n",
      "Iteration 1953, loss = 0.39167583\n",
      "Iteration 1954, loss = 0.39154907\n",
      "Iteration 1955, loss = 0.39142259\n",
      "Iteration 1956, loss = 0.39129560\n",
      "Iteration 1957, loss = 0.39116696\n",
      "Iteration 1958, loss = 0.39103777\n",
      "Iteration 1959, loss = 0.39090845\n",
      "Iteration 1960, loss = 0.39077890\n",
      "Iteration 1961, loss = 0.39064887\n",
      "Iteration 1962, loss = 0.39051880\n",
      "Iteration 1963, loss = 0.39038846\n",
      "Iteration 1964, loss = 0.39025808\n",
      "Iteration 1965, loss = 0.39012759\n",
      "Iteration 1966, loss = 0.38999683\n",
      "Iteration 1967, loss = 0.38986862\n",
      "Iteration 1968, loss = 0.38974112\n",
      "Iteration 1969, loss = 0.38961370\n",
      "Iteration 1970, loss = 0.38948659\n",
      "Iteration 1971, loss = 0.38935974\n",
      "Iteration 1972, loss = 0.38923221\n",
      "Iteration 1973, loss = 0.38909936\n",
      "Iteration 1974, loss = 0.38896585\n",
      "Iteration 1975, loss = 0.38883196\n",
      "Iteration 1976, loss = 0.38869974\n",
      "Iteration 1977, loss = 0.38856825\n",
      "Iteration 1978, loss = 0.38843728\n",
      "Iteration 1979, loss = 0.38830961\n",
      "Iteration 1980, loss = 0.38818337\n",
      "Iteration 1981, loss = 0.38805903\n",
      "Iteration 1982, loss = 0.38793470\n",
      "Iteration 1983, loss = 0.38781054\n",
      "Iteration 1984, loss = 0.38768655\n",
      "Iteration 1985, loss = 0.38756275\n",
      "Iteration 1986, loss = 0.38743891\n",
      "Iteration 1987, loss = 0.38731535\n",
      "Iteration 1988, loss = 0.38719178\n",
      "Iteration 1989, loss = 0.38706824\n",
      "Iteration 1990, loss = 0.38694473\n",
      "Iteration 1991, loss = 0.38682145\n",
      "Iteration 1992, loss = 0.38669840\n",
      "Iteration 1993, loss = 0.38657529\n",
      "Iteration 1994, loss = 0.38645227\n",
      "Iteration 1995, loss = 0.38632944\n",
      "Iteration 1996, loss = 0.38620670\n",
      "Iteration 1997, loss = 0.38608416\n",
      "Iteration 1998, loss = 0.38596143\n",
      "Iteration 1999, loss = 0.38583900\n",
      "Iteration 2000, loss = 0.38571674\n",
      "Iteration 2001, loss = 0.38559436\n",
      "Iteration 2002, loss = 0.38547200\n",
      "Iteration 2003, loss = 0.38534999\n",
      "Iteration 2004, loss = 0.38522817\n",
      "Iteration 2005, loss = 0.38510621\n",
      "Iteration 2006, loss = 0.38498441\n",
      "Iteration 2007, loss = 0.38486276\n",
      "Iteration 2008, loss = 0.38474112\n",
      "Iteration 2009, loss = 0.38461957\n",
      "Iteration 2010, loss = 0.38449843\n",
      "Iteration 2011, loss = 0.38437720\n",
      "Iteration 2012, loss = 0.38425621\n",
      "Iteration 2013, loss = 0.38413537\n",
      "Iteration 2014, loss = 0.38401451\n",
      "Iteration 2015, loss = 0.38389382\n",
      "Iteration 2016, loss = 0.38377307\n",
      "Iteration 2017, loss = 0.38365233\n",
      "Iteration 2018, loss = 0.38353185\n",
      "Iteration 2019, loss = 0.38341149\n",
      "Iteration 2020, loss = 0.38329108\n",
      "Iteration 2021, loss = 0.38317062\n",
      "Iteration 2022, loss = 0.38304993\n",
      "Iteration 2023, loss = 0.38292949\n",
      "Iteration 2024, loss = 0.38280927\n",
      "Iteration 2025, loss = 0.38268909\n",
      "Iteration 2026, loss = 0.38256867\n",
      "Iteration 2027, loss = 0.38244843\n",
      "Iteration 2028, loss = 0.38232853\n",
      "Iteration 2029, loss = 0.38220874\n",
      "Iteration 2030, loss = 0.38208901\n",
      "Iteration 2031, loss = 0.38196945\n",
      "Iteration 2032, loss = 0.38185012\n",
      "Iteration 2033, loss = 0.38173068\n",
      "Iteration 2034, loss = 0.38161145\n",
      "Iteration 2035, loss = 0.38149245\n",
      "Iteration 2036, loss = 0.38137339\n",
      "Iteration 2037, loss = 0.38125439\n",
      "Iteration 2038, loss = 0.38113545\n",
      "Iteration 2039, loss = 0.38101662\n",
      "Iteration 2040, loss = 0.38089773\n",
      "Iteration 2041, loss = 0.38077917\n",
      "Iteration 2042, loss = 0.38066073\n",
      "Iteration 2043, loss = 0.38054228\n",
      "Iteration 2044, loss = 0.38042388\n",
      "Iteration 2045, loss = 0.38030510\n",
      "Iteration 2046, loss = 0.38018665\n",
      "Iteration 2047, loss = 0.38006800\n",
      "Iteration 2048, loss = 0.37994918\n",
      "Iteration 2049, loss = 0.37983054\n",
      "Iteration 2050, loss = 0.37971228\n",
      "Iteration 2051, loss = 0.37959520\n",
      "Iteration 2052, loss = 0.37947783\n",
      "Iteration 2053, loss = 0.37936072\n",
      "Iteration 2054, loss = 0.37924373\n",
      "Iteration 2055, loss = 0.37912653\n",
      "Iteration 2056, loss = 0.37900922\n",
      "Iteration 2057, loss = 0.37889147\n",
      "Iteration 2058, loss = 0.37877370\n",
      "Iteration 2059, loss = 0.37865583\n",
      "Iteration 2060, loss = 0.37853807\n",
      "Iteration 2061, loss = 0.37842061\n",
      "Iteration 2062, loss = 0.37830427\n",
      "Iteration 2063, loss = 0.37818791\n",
      "Iteration 2064, loss = 0.37807167\n",
      "Iteration 2065, loss = 0.37795565\n",
      "Iteration 2066, loss = 0.37783980\n",
      "Iteration 2067, loss = 0.37772408\n",
      "Iteration 2068, loss = 0.37760831\n",
      "Iteration 2069, loss = 0.37749266\n",
      "Iteration 2070, loss = 0.37737695\n",
      "Iteration 2071, loss = 0.37726163\n",
      "Iteration 2072, loss = 0.37714654\n",
      "Iteration 2073, loss = 0.37703108\n",
      "Iteration 2074, loss = 0.37691572\n",
      "Iteration 2075, loss = 0.37680098\n",
      "Iteration 2076, loss = 0.37668692\n",
      "Iteration 2077, loss = 0.37657288\n",
      "Iteration 2078, loss = 0.37645891\n",
      "Iteration 2079, loss = 0.37634484\n",
      "Iteration 2080, loss = 0.37623091\n",
      "Iteration 2081, loss = 0.37611725\n",
      "Iteration 2082, loss = 0.37600328\n",
      "Iteration 2083, loss = 0.37588934\n",
      "Iteration 2084, loss = 0.37577539\n",
      "Iteration 2085, loss = 0.37566209\n",
      "Iteration 2086, loss = 0.37554899\n",
      "Iteration 2087, loss = 0.37543595\n",
      "Iteration 2088, loss = 0.37532314\n",
      "Iteration 2089, loss = 0.37521046\n",
      "Iteration 2090, loss = 0.37509778\n",
      "Iteration 2091, loss = 0.37498523\n",
      "Iteration 2092, loss = 0.37487290\n",
      "Iteration 2093, loss = 0.37476059\n",
      "Iteration 2094, loss = 0.37464823\n",
      "Iteration 2095, loss = 0.37453639\n",
      "Iteration 2096, loss = 0.37442462\n",
      "Iteration 2097, loss = 0.37431275\n",
      "Iteration 2098, loss = 0.37420111\n",
      "Iteration 2099, loss = 0.37408960\n",
      "Iteration 2100, loss = 0.37397816\n",
      "Iteration 2101, loss = 0.37386679\n",
      "Iteration 2102, loss = 0.37375546\n",
      "Iteration 2103, loss = 0.37364424\n",
      "Iteration 2104, loss = 0.37353331\n",
      "Iteration 2105, loss = 0.37342246\n",
      "Iteration 2106, loss = 0.37331184\n",
      "Iteration 2107, loss = 0.37320134\n",
      "Iteration 2108, loss = 0.37309218\n",
      "Iteration 2109, loss = 0.37298323\n",
      "Iteration 2110, loss = 0.37287470\n",
      "Iteration 2111, loss = 0.37276612\n",
      "Iteration 2112, loss = 0.37265755\n",
      "Iteration 2113, loss = 0.37254906\n",
      "Iteration 2114, loss = 0.37244032\n",
      "Iteration 2115, loss = 0.37233163\n",
      "Iteration 2116, loss = 0.37222327\n",
      "Iteration 2117, loss = 0.37211481\n",
      "Iteration 2118, loss = 0.37200672\n",
      "Iteration 2119, loss = 0.37189925\n",
      "Iteration 2120, loss = 0.37179173\n",
      "Iteration 2121, loss = 0.37168445\n",
      "Iteration 2122, loss = 0.37157728\n",
      "Iteration 2123, loss = 0.37147000\n",
      "Iteration 2124, loss = 0.37136277\n",
      "Iteration 2125, loss = 0.37125565\n",
      "Iteration 2126, loss = 0.37114867\n",
      "Iteration 2127, loss = 0.37104174\n",
      "Iteration 2128, loss = 0.37093533\n",
      "Iteration 2129, loss = 0.37082955\n",
      "Iteration 2130, loss = 0.37072416\n",
      "Iteration 2131, loss = 0.37061849\n",
      "Iteration 2132, loss = 0.37051279\n",
      "Iteration 2133, loss = 0.37040714\n",
      "Iteration 2134, loss = 0.37030144\n",
      "Iteration 2135, loss = 0.37019656\n",
      "Iteration 2136, loss = 0.37009170\n",
      "Iteration 2137, loss = 0.36998659\n",
      "Iteration 2138, loss = 0.36988169\n",
      "Iteration 2139, loss = 0.36977693\n",
      "Iteration 2140, loss = 0.36967210\n",
      "Iteration 2141, loss = 0.36956726\n",
      "Iteration 2142, loss = 0.36946250\n",
      "Iteration 2143, loss = 0.36935802\n",
      "Iteration 2144, loss = 0.36925401\n",
      "Iteration 2145, loss = 0.36914971\n",
      "Iteration 2146, loss = 0.36904536\n",
      "Iteration 2147, loss = 0.36894090\n",
      "Iteration 2148, loss = 0.36883666\n",
      "Iteration 2149, loss = 0.36873274\n",
      "Iteration 2150, loss = 0.36862894\n",
      "Iteration 2151, loss = 0.36852505\n",
      "Iteration 2152, loss = 0.36842128\n",
      "Iteration 2153, loss = 0.36831760\n",
      "Iteration 2154, loss = 0.36821398\n",
      "Iteration 2155, loss = 0.36811099\n",
      "Iteration 2156, loss = 0.36800789\n",
      "Iteration 2157, loss = 0.36790503\n",
      "Iteration 2158, loss = 0.36780221\n",
      "Iteration 2159, loss = 0.36769936\n",
      "Iteration 2160, loss = 0.36759672\n",
      "Iteration 2161, loss = 0.36749411\n",
      "Iteration 2162, loss = 0.36739130\n",
      "Iteration 2163, loss = 0.36728884\n",
      "Iteration 2164, loss = 0.36718655\n",
      "Iteration 2165, loss = 0.36708453\n",
      "Iteration 2166, loss = 0.36698249\n",
      "Iteration 2167, loss = 0.36688029\n",
      "Iteration 2168, loss = 0.36677848\n",
      "Iteration 2169, loss = 0.36667676\n",
      "Iteration 2170, loss = 0.36657489\n",
      "Iteration 2171, loss = 0.36647336\n",
      "Iteration 2172, loss = 0.36637204\n",
      "Iteration 2173, loss = 0.36627052\n",
      "Iteration 2174, loss = 0.36616916\n",
      "Iteration 2175, loss = 0.36606835\n",
      "Iteration 2176, loss = 0.36596738\n",
      "Iteration 2177, loss = 0.36586642\n",
      "Iteration 2178, loss = 0.36576562\n",
      "Iteration 2179, loss = 0.36566485\n",
      "Iteration 2180, loss = 0.36556444\n",
      "Iteration 2181, loss = 0.36546362\n",
      "Iteration 2182, loss = 0.36536292\n",
      "Iteration 2183, loss = 0.36526211\n",
      "Iteration 2184, loss = 0.36516163\n",
      "Iteration 2185, loss = 0.36506104\n",
      "Iteration 2186, loss = 0.36496067\n",
      "Iteration 2187, loss = 0.36486050\n",
      "Iteration 2188, loss = 0.36476065\n",
      "Iteration 2189, loss = 0.36466087\n",
      "Iteration 2190, loss = 0.36456146\n",
      "Iteration 2191, loss = 0.36446211\n",
      "Iteration 2192, loss = 0.36436257\n",
      "Iteration 2193, loss = 0.36426326\n",
      "Iteration 2194, loss = 0.36416424\n",
      "Iteration 2195, loss = 0.36406539\n",
      "Iteration 2196, loss = 0.36396616\n",
      "Iteration 2197, loss = 0.36386730\n",
      "Iteration 2198, loss = 0.36376885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33701451\n",
      "Iteration 2, loss = 1.33489125\n",
      "Iteration 3, loss = 1.33278115\n",
      "Iteration 4, loss = 1.33068249\n",
      "Iteration 5, loss = 1.32859783\n",
      "Iteration 6, loss = 1.32652082\n",
      "Iteration 7, loss = 1.32445295\n",
      "Iteration 8, loss = 1.32239285\n",
      "Iteration 9, loss = 1.32033720\n",
      "Iteration 10, loss = 1.31828965\n",
      "Iteration 11, loss = 1.31624425\n",
      "Iteration 12, loss = 1.31419786\n",
      "Iteration 13, loss = 1.31215983\n",
      "Iteration 14, loss = 1.31014484\n",
      "Iteration 15, loss = 1.30813803\n",
      "Iteration 16, loss = 1.30614246\n",
      "Iteration 17, loss = 1.30415875\n",
      "Iteration 18, loss = 1.30218432\n",
      "Iteration 19, loss = 1.30021470\n",
      "Iteration 20, loss = 1.29825409\n",
      "Iteration 21, loss = 1.29629993\n",
      "Iteration 22, loss = 1.29435437\n",
      "Iteration 23, loss = 1.29241866\n",
      "Iteration 24, loss = 1.29048842\n",
      "Iteration 25, loss = 1.28856408\n",
      "Iteration 26, loss = 1.28664634\n",
      "Iteration 27, loss = 1.28473792\n",
      "Iteration 28, loss = 1.28283322\n",
      "Iteration 29, loss = 1.28093504\n",
      "Iteration 30, loss = 1.27904621\n",
      "Iteration 31, loss = 1.27717606\n",
      "Iteration 32, loss = 1.27531865\n",
      "Iteration 33, loss = 1.27346927\n",
      "Iteration 34, loss = 1.27163592\n",
      "Iteration 35, loss = 1.26980152\n",
      "Iteration 36, loss = 1.26797103\n",
      "Iteration 37, loss = 1.26614642\n",
      "Iteration 38, loss = 1.26432952\n",
      "Iteration 39, loss = 1.26251389\n",
      "Iteration 40, loss = 1.26070318\n",
      "Iteration 41, loss = 1.25889750\n",
      "Iteration 42, loss = 1.25710561\n",
      "Iteration 43, loss = 1.25533922\n",
      "Iteration 44, loss = 1.25357713\n",
      "Iteration 45, loss = 1.25181791\n",
      "Iteration 46, loss = 1.25006190\n",
      "Iteration 47, loss = 1.24831448\n",
      "Iteration 48, loss = 1.24658223\n",
      "Iteration 49, loss = 1.24486137\n",
      "Iteration 50, loss = 1.24314839\n",
      "Iteration 51, loss = 1.24144450\n",
      "Iteration 52, loss = 1.23974896\n",
      "Iteration 53, loss = 1.23805161\n",
      "Iteration 54, loss = 1.23635646\n",
      "Iteration 55, loss = 1.23466761\n",
      "Iteration 56, loss = 1.23299170\n",
      "Iteration 57, loss = 1.23131580\n",
      "Iteration 58, loss = 1.22964463\n",
      "Iteration 59, loss = 1.22797945\n",
      "Iteration 60, loss = 1.22631744\n",
      "Iteration 61, loss = 1.22467082\n",
      "Iteration 62, loss = 1.22305268\n",
      "Iteration 63, loss = 1.22143912\n",
      "Iteration 64, loss = 1.21983239\n",
      "Iteration 65, loss = 1.21823137\n",
      "Iteration 66, loss = 1.21663565\n",
      "Iteration 67, loss = 1.21505271\n",
      "Iteration 68, loss = 1.21348117\n",
      "Iteration 69, loss = 1.21191639\n",
      "Iteration 70, loss = 1.21036281\n",
      "Iteration 71, loss = 1.20882351\n",
      "Iteration 72, loss = 1.20728660\n",
      "Iteration 73, loss = 1.20575450\n",
      "Iteration 74, loss = 1.20422950\n",
      "Iteration 75, loss = 1.20271507\n",
      "Iteration 76, loss = 1.20120727\n",
      "Iteration 77, loss = 1.19970615\n",
      "Iteration 78, loss = 1.19821039\n",
      "Iteration 79, loss = 1.19671920\n",
      "Iteration 80, loss = 1.19523392\n",
      "Iteration 81, loss = 1.19375627\n",
      "Iteration 82, loss = 1.19228545\n",
      "Iteration 83, loss = 1.19082584\n",
      "Iteration 84, loss = 1.18937329\n",
      "Iteration 85, loss = 1.18792624\n",
      "Iteration 86, loss = 1.18649110\n",
      "Iteration 87, loss = 1.18506846\n",
      "Iteration 88, loss = 1.18365226\n",
      "Iteration 89, loss = 1.18224088\n",
      "Iteration 90, loss = 1.18083451\n",
      "Iteration 91, loss = 1.17943434\n",
      "Iteration 92, loss = 1.17804120\n",
      "Iteration 93, loss = 1.17665326\n",
      "Iteration 94, loss = 1.17527201\n",
      "Iteration 95, loss = 1.17389544\n",
      "Iteration 96, loss = 1.17252384\n",
      "Iteration 97, loss = 1.17115539\n",
      "Iteration 98, loss = 1.16979257\n",
      "Iteration 99, loss = 1.16843194\n",
      "Iteration 100, loss = 1.16707607\n",
      "Iteration 101, loss = 1.16572482\n",
      "Iteration 102, loss = 1.16437784\n",
      "Iteration 103, loss = 1.16304096\n",
      "Iteration 104, loss = 1.16170739\n",
      "Iteration 105, loss = 1.16038108\n",
      "Iteration 106, loss = 1.15905659\n",
      "Iteration 107, loss = 1.15774934\n",
      "Iteration 108, loss = 1.15644734\n",
      "Iteration 109, loss = 1.15516277\n",
      "Iteration 110, loss = 1.15388479\n",
      "Iteration 111, loss = 1.15261679\n",
      "Iteration 112, loss = 1.15135148\n",
      "Iteration 113, loss = 1.15009220\n",
      "Iteration 114, loss = 1.14884108\n",
      "Iteration 115, loss = 1.14759422\n",
      "Iteration 116, loss = 1.14635245\n",
      "Iteration 117, loss = 1.14511312\n",
      "Iteration 118, loss = 1.14387604\n",
      "Iteration 119, loss = 1.14264226\n",
      "Iteration 120, loss = 1.14141257\n",
      "Iteration 121, loss = 1.14018236\n",
      "Iteration 122, loss = 1.13895456\n",
      "Iteration 123, loss = 1.13773055\n",
      "Iteration 124, loss = 1.13651613\n",
      "Iteration 125, loss = 1.13530251\n",
      "Iteration 126, loss = 1.13409192\n",
      "Iteration 127, loss = 1.13289276\n",
      "Iteration 128, loss = 1.13169757\n",
      "Iteration 129, loss = 1.13050529\n",
      "Iteration 130, loss = 1.12931555\n",
      "Iteration 131, loss = 1.12812823\n",
      "Iteration 132, loss = 1.12694183\n",
      "Iteration 133, loss = 1.12575838\n",
      "Iteration 134, loss = 1.12457896\n",
      "Iteration 135, loss = 1.12340531\n",
      "Iteration 136, loss = 1.12223483\n",
      "Iteration 137, loss = 1.12106529\n",
      "Iteration 138, loss = 1.11990064\n",
      "Iteration 139, loss = 1.11873786\n",
      "Iteration 140, loss = 1.11757763\n",
      "Iteration 141, loss = 1.11642194\n",
      "Iteration 142, loss = 1.11526862\n",
      "Iteration 143, loss = 1.11411876\n",
      "Iteration 144, loss = 1.11297296\n",
      "Iteration 145, loss = 1.11183099\n",
      "Iteration 146, loss = 1.11069526\n",
      "Iteration 147, loss = 1.10956835\n",
      "Iteration 148, loss = 1.10844336\n",
      "Iteration 149, loss = 1.10732223\n",
      "Iteration 150, loss = 1.10621068\n",
      "Iteration 151, loss = 1.10510076\n",
      "Iteration 152, loss = 1.10398807\n",
      "Iteration 153, loss = 1.10287725\n",
      "Iteration 154, loss = 1.10176789\n",
      "Iteration 155, loss = 1.10066103\n",
      "Iteration 156, loss = 1.09955387\n",
      "Iteration 157, loss = 1.09845411\n",
      "Iteration 158, loss = 1.09735633\n",
      "Iteration 159, loss = 1.09626059\n",
      "Iteration 160, loss = 1.09516538\n",
      "Iteration 161, loss = 1.09406817\n",
      "Iteration 162, loss = 1.09297179\n",
      "Iteration 163, loss = 1.09187577\n",
      "Iteration 164, loss = 1.09078202\n",
      "Iteration 165, loss = 1.08968973\n",
      "Iteration 166, loss = 1.08859761\n",
      "Iteration 167, loss = 1.08750656\n",
      "Iteration 168, loss = 1.08641934\n",
      "Iteration 169, loss = 1.08533323\n",
      "Iteration 170, loss = 1.08425138\n",
      "Iteration 171, loss = 1.08316982\n",
      "Iteration 172, loss = 1.08208758\n",
      "Iteration 173, loss = 1.08100941\n",
      "Iteration 174, loss = 1.07993329\n",
      "Iteration 175, loss = 1.07885810\n",
      "Iteration 176, loss = 1.07778446\n",
      "Iteration 177, loss = 1.07671206\n",
      "Iteration 178, loss = 1.07564079\n",
      "Iteration 179, loss = 1.07456905\n",
      "Iteration 180, loss = 1.07349134\n",
      "Iteration 181, loss = 1.07241080\n",
      "Iteration 182, loss = 1.07132430\n",
      "Iteration 183, loss = 1.07023314\n",
      "Iteration 184, loss = 1.06914152\n",
      "Iteration 185, loss = 1.06804947\n",
      "Iteration 186, loss = 1.06695880\n",
      "Iteration 187, loss = 1.06586886\n",
      "Iteration 188, loss = 1.06477913\n",
      "Iteration 189, loss = 1.06369564\n",
      "Iteration 190, loss = 1.06261415\n",
      "Iteration 191, loss = 1.06153205\n",
      "Iteration 192, loss = 1.06045435\n",
      "Iteration 193, loss = 1.05937866\n",
      "Iteration 194, loss = 1.05830536\n",
      "Iteration 195, loss = 1.05723482\n",
      "Iteration 196, loss = 1.05616726\n",
      "Iteration 197, loss = 1.05510211\n",
      "Iteration 198, loss = 1.05403984\n",
      "Iteration 199, loss = 1.05298060\n",
      "Iteration 200, loss = 1.05192473\n",
      "Iteration 201, loss = 1.05087299\n",
      "Iteration 202, loss = 1.04982460\n",
      "Iteration 203, loss = 1.04877764\n",
      "Iteration 204, loss = 1.04773444\n",
      "Iteration 205, loss = 1.04669540\n",
      "Iteration 206, loss = 1.04566037\n",
      "Iteration 207, loss = 1.04463537\n",
      "Iteration 208, loss = 1.04361383\n",
      "Iteration 209, loss = 1.04259556\n",
      "Iteration 210, loss = 1.04157473\n",
      "Iteration 211, loss = 1.04055202\n",
      "Iteration 212, loss = 1.03953232\n",
      "Iteration 213, loss = 1.03851679\n",
      "Iteration 214, loss = 1.03750175\n",
      "Iteration 215, loss = 1.03648819\n",
      "Iteration 216, loss = 1.03548239\n",
      "Iteration 217, loss = 1.03447486\n",
      "Iteration 218, loss = 1.03346892\n",
      "Iteration 219, loss = 1.03246503\n",
      "Iteration 220, loss = 1.03146475\n",
      "Iteration 221, loss = 1.03046636\n",
      "Iteration 222, loss = 1.02946809\n",
      "Iteration 223, loss = 1.02847191\n",
      "Iteration 224, loss = 1.02747523\n",
      "Iteration 225, loss = 1.02648418\n",
      "Iteration 226, loss = 1.02549196\n",
      "Iteration 227, loss = 1.02450343\n",
      "Iteration 228, loss = 1.02351539\n",
      "Iteration 229, loss = 1.02253172\n",
      "Iteration 230, loss = 1.02154998\n",
      "Iteration 231, loss = 1.02057432\n",
      "Iteration 232, loss = 1.01960162\n",
      "Iteration 233, loss = 1.01863075\n",
      "Iteration 234, loss = 1.01765947\n",
      "Iteration 235, loss = 1.01669149\n",
      "Iteration 236, loss = 1.01572590\n",
      "Iteration 237, loss = 1.01475725\n",
      "Iteration 238, loss = 1.01378907\n",
      "Iteration 239, loss = 1.01282446\n",
      "Iteration 240, loss = 1.01186497\n",
      "Iteration 241, loss = 1.01090661\n",
      "Iteration 242, loss = 1.00994800\n",
      "Iteration 243, loss = 1.00899129\n",
      "Iteration 244, loss = 1.00803855\n",
      "Iteration 245, loss = 1.00708804\n",
      "Iteration 246, loss = 1.00613935\n",
      "Iteration 247, loss = 1.00518907\n",
      "Iteration 248, loss = 1.00423713\n",
      "Iteration 249, loss = 1.00328487\n",
      "Iteration 250, loss = 1.00233613\n",
      "Iteration 251, loss = 1.00139147\n",
      "Iteration 252, loss = 1.00044954\n",
      "Iteration 253, loss = 0.99950712\n",
      "Iteration 254, loss = 0.99855796\n",
      "Iteration 255, loss = 0.99761165\n",
      "Iteration 256, loss = 0.99667061\n",
      "Iteration 257, loss = 0.99573146\n",
      "Iteration 258, loss = 0.99478913\n",
      "Iteration 259, loss = 0.99384683\n",
      "Iteration 260, loss = 0.99290719\n",
      "Iteration 261, loss = 0.99196817\n",
      "Iteration 262, loss = 0.99102885\n",
      "Iteration 263, loss = 0.99009164\n",
      "Iteration 264, loss = 0.98915172\n",
      "Iteration 265, loss = 0.98821268\n",
      "Iteration 266, loss = 0.98727386\n",
      "Iteration 267, loss = 0.98633759\n",
      "Iteration 268, loss = 0.98540303\n",
      "Iteration 269, loss = 0.98447387\n",
      "Iteration 270, loss = 0.98354516\n",
      "Iteration 271, loss = 0.98262031\n",
      "Iteration 272, loss = 0.98169650\n",
      "Iteration 273, loss = 0.98077507\n",
      "Iteration 274, loss = 0.97985662\n",
      "Iteration 275, loss = 0.97894031\n",
      "Iteration 276, loss = 0.97802989\n",
      "Iteration 277, loss = 0.97711991\n",
      "Iteration 278, loss = 0.97621382\n",
      "Iteration 279, loss = 0.97530823\n",
      "Iteration 280, loss = 0.97440340\n",
      "Iteration 281, loss = 0.97350166\n",
      "Iteration 282, loss = 0.97260234\n",
      "Iteration 283, loss = 0.97170344\n",
      "Iteration 284, loss = 0.97080728\n",
      "Iteration 285, loss = 0.96991459\n",
      "Iteration 286, loss = 0.96902185\n",
      "Iteration 287, loss = 0.96813306\n",
      "Iteration 288, loss = 0.96724868\n",
      "Iteration 289, loss = 0.96636843\n",
      "Iteration 290, loss = 0.96548782\n",
      "Iteration 291, loss = 0.96460887\n",
      "Iteration 292, loss = 0.96373330\n",
      "Iteration 293, loss = 0.96285994\n",
      "Iteration 294, loss = 0.96198989\n",
      "Iteration 295, loss = 0.96112197\n",
      "Iteration 296, loss = 0.96025572\n",
      "Iteration 297, loss = 0.95939251\n",
      "Iteration 298, loss = 0.95853089\n",
      "Iteration 299, loss = 0.95767207\n",
      "Iteration 300, loss = 0.95681368\n",
      "Iteration 301, loss = 0.95595623\n",
      "Iteration 302, loss = 0.95510079\n",
      "Iteration 303, loss = 0.95424781\n",
      "Iteration 304, loss = 0.95339588\n",
      "Iteration 305, loss = 0.95254438\n",
      "Iteration 306, loss = 0.95169317\n",
      "Iteration 307, loss = 0.95084266\n",
      "Iteration 308, loss = 0.94999554\n",
      "Iteration 309, loss = 0.94914896\n",
      "Iteration 310, loss = 0.94830208\n",
      "Iteration 311, loss = 0.94745285\n",
      "Iteration 312, loss = 0.94660637\n",
      "Iteration 313, loss = 0.94576172\n",
      "Iteration 314, loss = 0.94491914\n",
      "Iteration 315, loss = 0.94407801\n",
      "Iteration 316, loss = 0.94323971\n",
      "Iteration 317, loss = 0.94240354\n",
      "Iteration 318, loss = 0.94156756\n",
      "Iteration 319, loss = 0.94073347\n",
      "Iteration 320, loss = 0.93990156\n",
      "Iteration 321, loss = 0.93906911\n",
      "Iteration 322, loss = 0.93824262\n",
      "Iteration 323, loss = 0.93742289\n",
      "Iteration 324, loss = 0.93660457\n",
      "Iteration 325, loss = 0.93578860\n",
      "Iteration 326, loss = 0.93497338\n",
      "Iteration 327, loss = 0.93415881\n",
      "Iteration 328, loss = 0.93334708\n",
      "Iteration 329, loss = 0.93253609\n",
      "Iteration 330, loss = 0.93172773\n",
      "Iteration 331, loss = 0.93091908\n",
      "Iteration 332, loss = 0.93011224\n",
      "Iteration 333, loss = 0.92930638\n",
      "Iteration 334, loss = 0.92849915\n",
      "Iteration 335, loss = 0.92769170\n",
      "Iteration 336, loss = 0.92688874\n",
      "Iteration 337, loss = 0.92608615\n",
      "Iteration 338, loss = 0.92528372\n",
      "Iteration 339, loss = 0.92448219\n",
      "Iteration 340, loss = 0.92368837\n",
      "Iteration 341, loss = 0.92289456\n",
      "Iteration 342, loss = 0.92209913\n",
      "Iteration 343, loss = 0.92130246\n",
      "Iteration 344, loss = 0.92050719\n",
      "Iteration 345, loss = 0.91971210\n",
      "Iteration 346, loss = 0.91891889\n",
      "Iteration 347, loss = 0.91812641\n",
      "Iteration 348, loss = 0.91733342\n",
      "Iteration 349, loss = 0.91654694\n",
      "Iteration 350, loss = 0.91576422\n",
      "Iteration 351, loss = 0.91498205\n",
      "Iteration 352, loss = 0.91420081\n",
      "Iteration 353, loss = 0.91342042\n",
      "Iteration 354, loss = 0.91264225\n",
      "Iteration 355, loss = 0.91186506\n",
      "Iteration 356, loss = 0.91108951\n",
      "Iteration 357, loss = 0.91031400\n",
      "Iteration 358, loss = 0.90953886\n",
      "Iteration 359, loss = 0.90876875\n",
      "Iteration 360, loss = 0.90800147\n",
      "Iteration 361, loss = 0.90723587\n",
      "Iteration 362, loss = 0.90647004\n",
      "Iteration 363, loss = 0.90570529\n",
      "Iteration 364, loss = 0.90494367\n",
      "Iteration 365, loss = 0.90418722\n",
      "Iteration 366, loss = 0.90343562\n",
      "Iteration 367, loss = 0.90268580\n",
      "Iteration 368, loss = 0.90193811\n",
      "Iteration 369, loss = 0.90119271\n",
      "Iteration 370, loss = 0.90044722\n",
      "Iteration 371, loss = 0.89969984\n",
      "Iteration 372, loss = 0.89895257\n",
      "Iteration 373, loss = 0.89820220\n",
      "Iteration 374, loss = 0.89745089\n",
      "Iteration 375, loss = 0.89670047\n",
      "Iteration 376, loss = 0.89595256\n",
      "Iteration 377, loss = 0.89520289\n",
      "Iteration 378, loss = 0.89445579\n",
      "Iteration 379, loss = 0.89371230\n",
      "Iteration 380, loss = 0.89297102\n",
      "Iteration 381, loss = 0.89223271\n",
      "Iteration 382, loss = 0.89149680\n",
      "Iteration 383, loss = 0.89076361\n",
      "Iteration 384, loss = 0.89003066\n",
      "Iteration 385, loss = 0.88929867\n",
      "Iteration 386, loss = 0.88856868\n",
      "Iteration 387, loss = 0.88784109\n",
      "Iteration 388, loss = 0.88711364\n",
      "Iteration 389, loss = 0.88638631\n",
      "Iteration 390, loss = 0.88566601\n",
      "Iteration 391, loss = 0.88494536\n",
      "Iteration 392, loss = 0.88422616\n",
      "Iteration 393, loss = 0.88350863\n",
      "Iteration 394, loss = 0.88279228\n",
      "Iteration 395, loss = 0.88207568\n",
      "Iteration 396, loss = 0.88136131\n",
      "Iteration 397, loss = 0.88065105\n",
      "Iteration 398, loss = 0.87994273\n",
      "Iteration 399, loss = 0.87923274\n",
      "Iteration 400, loss = 0.87852172\n",
      "Iteration 401, loss = 0.87781252\n",
      "Iteration 402, loss = 0.87710335\n",
      "Iteration 403, loss = 0.87639643\n",
      "Iteration 404, loss = 0.87568985\n",
      "Iteration 405, loss = 0.87498512\n",
      "Iteration 406, loss = 0.87428342\n",
      "Iteration 407, loss = 0.87358254\n",
      "Iteration 408, loss = 0.87288622\n",
      "Iteration 409, loss = 0.87219102\n",
      "Iteration 410, loss = 0.87149575\n",
      "Iteration 411, loss = 0.87080396\n",
      "Iteration 412, loss = 0.87011361\n",
      "Iteration 413, loss = 0.86942632\n",
      "Iteration 414, loss = 0.86874001\n",
      "Iteration 415, loss = 0.86805366\n",
      "Iteration 416, loss = 0.86737015\n",
      "Iteration 417, loss = 0.86668721\n",
      "Iteration 418, loss = 0.86600510\n",
      "Iteration 419, loss = 0.86532410\n",
      "Iteration 420, loss = 0.86464384\n",
      "Iteration 421, loss = 0.86396641\n",
      "Iteration 422, loss = 0.86328906\n",
      "Iteration 423, loss = 0.86261233\n",
      "Iteration 424, loss = 0.86193845\n",
      "Iteration 425, loss = 0.86126680\n",
      "Iteration 426, loss = 0.86059538\n",
      "Iteration 427, loss = 0.85992453\n",
      "Iteration 428, loss = 0.85925633\n",
      "Iteration 429, loss = 0.85859000\n",
      "Iteration 430, loss = 0.85792468\n",
      "Iteration 431, loss = 0.85726007\n",
      "Iteration 432, loss = 0.85659537\n",
      "Iteration 433, loss = 0.85593523\n",
      "Iteration 434, loss = 0.85527870\n",
      "Iteration 435, loss = 0.85462329\n",
      "Iteration 436, loss = 0.85396664\n",
      "Iteration 437, loss = 0.85331290\n",
      "Iteration 438, loss = 0.85266121\n",
      "Iteration 439, loss = 0.85201024\n",
      "Iteration 440, loss = 0.85136181\n",
      "Iteration 441, loss = 0.85071397\n",
      "Iteration 442, loss = 0.85006919\n",
      "Iteration 443, loss = 0.84942659\n",
      "Iteration 444, loss = 0.84878705\n",
      "Iteration 445, loss = 0.84814872\n",
      "Iteration 446, loss = 0.84751572\n",
      "Iteration 447, loss = 0.84688492\n",
      "Iteration 448, loss = 0.84625364\n",
      "Iteration 449, loss = 0.84562259\n",
      "Iteration 450, loss = 0.84499268\n",
      "Iteration 451, loss = 0.84436441\n",
      "Iteration 452, loss = 0.84373651\n",
      "Iteration 453, loss = 0.84310923\n",
      "Iteration 454, loss = 0.84248457\n",
      "Iteration 455, loss = 0.84186109\n",
      "Iteration 456, loss = 0.84123776\n",
      "Iteration 457, loss = 0.84061668\n",
      "Iteration 458, loss = 0.83999708\n",
      "Iteration 459, loss = 0.83937826\n",
      "Iteration 460, loss = 0.83876032\n",
      "Iteration 461, loss = 0.83814442\n",
      "Iteration 462, loss = 0.83752962\n",
      "Iteration 463, loss = 0.83691477\n",
      "Iteration 464, loss = 0.83630227\n",
      "Iteration 465, loss = 0.83569308\n",
      "Iteration 466, loss = 0.83508621\n",
      "Iteration 467, loss = 0.83447891\n",
      "Iteration 468, loss = 0.83387220\n",
      "Iteration 469, loss = 0.83326814\n",
      "Iteration 470, loss = 0.83266617\n",
      "Iteration 471, loss = 0.83206439\n",
      "Iteration 472, loss = 0.83146443\n",
      "Iteration 473, loss = 0.83086565\n",
      "Iteration 474, loss = 0.83026911\n",
      "Iteration 475, loss = 0.82967278\n",
      "Iteration 476, loss = 0.82907860\n",
      "Iteration 477, loss = 0.82848314\n",
      "Iteration 478, loss = 0.82788664\n",
      "Iteration 479, loss = 0.82729126\n",
      "Iteration 480, loss = 0.82669630\n",
      "Iteration 481, loss = 0.82610209\n",
      "Iteration 482, loss = 0.82550701\n",
      "Iteration 483, loss = 0.82491226\n",
      "Iteration 484, loss = 0.82431921\n",
      "Iteration 485, loss = 0.82372769\n",
      "Iteration 486, loss = 0.82313736\n",
      "Iteration 487, loss = 0.82254742\n",
      "Iteration 488, loss = 0.82195723\n",
      "Iteration 489, loss = 0.82136933\n",
      "Iteration 490, loss = 0.82078366\n",
      "Iteration 491, loss = 0.82019831\n",
      "Iteration 492, loss = 0.81961173\n",
      "Iteration 493, loss = 0.81902777\n",
      "Iteration 494, loss = 0.81844538\n",
      "Iteration 495, loss = 0.81786324\n",
      "Iteration 496, loss = 0.81728135\n",
      "Iteration 497, loss = 0.81670152\n",
      "Iteration 498, loss = 0.81612400\n",
      "Iteration 499, loss = 0.81554666\n",
      "Iteration 500, loss = 0.81497020\n",
      "Iteration 501, loss = 0.81439504\n",
      "Iteration 502, loss = 0.81382039\n",
      "Iteration 503, loss = 0.81324696\n",
      "Iteration 504, loss = 0.81267567\n",
      "Iteration 505, loss = 0.81210639\n",
      "Iteration 506, loss = 0.81153796\n",
      "Iteration 507, loss = 0.81097151\n",
      "Iteration 508, loss = 0.81040381\n",
      "Iteration 509, loss = 0.80983782\n",
      "Iteration 510, loss = 0.80927255\n",
      "Iteration 511, loss = 0.80870992\n",
      "Iteration 512, loss = 0.80814819\n",
      "Iteration 513, loss = 0.80758792\n",
      "Iteration 514, loss = 0.80702818\n",
      "Iteration 515, loss = 0.80647000\n",
      "Iteration 516, loss = 0.80591309\n",
      "Iteration 517, loss = 0.80535825\n",
      "Iteration 518, loss = 0.80480439\n",
      "Iteration 519, loss = 0.80425110\n",
      "Iteration 520, loss = 0.80370005\n",
      "Iteration 521, loss = 0.80314863\n",
      "Iteration 522, loss = 0.80259525\n",
      "Iteration 523, loss = 0.80204034\n",
      "Iteration 524, loss = 0.80148542\n",
      "Iteration 525, loss = 0.80092894\n",
      "Iteration 526, loss = 0.80036829\n",
      "Iteration 527, loss = 0.79980654\n",
      "Iteration 528, loss = 0.79924465\n",
      "Iteration 529, loss = 0.79868371\n",
      "Iteration 530, loss = 0.79812248\n",
      "Iteration 531, loss = 0.79756107\n",
      "Iteration 532, loss = 0.79699961\n",
      "Iteration 533, loss = 0.79643813\n",
      "Iteration 534, loss = 0.79587898\n",
      "Iteration 535, loss = 0.79532048\n",
      "Iteration 536, loss = 0.79476240\n",
      "Iteration 537, loss = 0.79420567\n",
      "Iteration 538, loss = 0.79365014\n",
      "Iteration 539, loss = 0.79309584\n",
      "Iteration 540, loss = 0.79254297\n",
      "Iteration 541, loss = 0.79199067\n",
      "Iteration 542, loss = 0.79143990\n",
      "Iteration 543, loss = 0.79089044\n",
      "Iteration 544, loss = 0.79034111\n",
      "Iteration 545, loss = 0.78979311\n",
      "Iteration 546, loss = 0.78924792\n",
      "Iteration 547, loss = 0.78870579\n",
      "Iteration 548, loss = 0.78816435\n",
      "Iteration 549, loss = 0.78762484\n",
      "Iteration 550, loss = 0.78708756\n",
      "Iteration 551, loss = 0.78655155\n",
      "Iteration 552, loss = 0.78601553\n",
      "Iteration 553, loss = 0.78548199\n",
      "Iteration 554, loss = 0.78494889\n",
      "Iteration 555, loss = 0.78441721\n",
      "Iteration 556, loss = 0.78388608\n",
      "Iteration 557, loss = 0.78335583\n",
      "Iteration 558, loss = 0.78282708\n",
      "Iteration 559, loss = 0.78230034\n",
      "Iteration 560, loss = 0.78177485\n",
      "Iteration 561, loss = 0.78124988\n",
      "Iteration 562, loss = 0.78072567\n",
      "Iteration 563, loss = 0.78020142\n",
      "Iteration 564, loss = 0.77967802\n",
      "Iteration 565, loss = 0.77915352\n",
      "Iteration 566, loss = 0.77862892\n",
      "Iteration 567, loss = 0.77810447\n",
      "Iteration 568, loss = 0.77757898\n",
      "Iteration 569, loss = 0.77705479\n",
      "Iteration 570, loss = 0.77653230\n",
      "Iteration 571, loss = 0.77600939\n",
      "Iteration 572, loss = 0.77548743\n",
      "Iteration 573, loss = 0.77496607\n",
      "Iteration 574, loss = 0.77444569\n",
      "Iteration 575, loss = 0.77392579\n",
      "Iteration 576, loss = 0.77340730\n",
      "Iteration 577, loss = 0.77288949\n",
      "Iteration 578, loss = 0.77237205\n",
      "Iteration 579, loss = 0.77185669\n",
      "Iteration 580, loss = 0.77134198\n",
      "Iteration 581, loss = 0.77082744\n",
      "Iteration 582, loss = 0.77031398\n",
      "Iteration 583, loss = 0.76980270\n",
      "Iteration 584, loss = 0.76929264\n",
      "Iteration 585, loss = 0.76878278\n",
      "Iteration 586, loss = 0.76827361\n",
      "Iteration 587, loss = 0.76776473\n",
      "Iteration 588, loss = 0.76725453\n",
      "Iteration 589, loss = 0.76674545\n",
      "Iteration 590, loss = 0.76623675\n",
      "Iteration 591, loss = 0.76572940\n",
      "Iteration 592, loss = 0.76522014\n",
      "Iteration 593, loss = 0.76471264\n",
      "Iteration 594, loss = 0.76420672\n",
      "Iteration 595, loss = 0.76370178\n",
      "Iteration 596, loss = 0.76319927\n",
      "Iteration 597, loss = 0.76269688\n",
      "Iteration 598, loss = 0.76219535\n",
      "Iteration 599, loss = 0.76169516\n",
      "Iteration 600, loss = 0.76119618\n",
      "Iteration 601, loss = 0.76069688\n",
      "Iteration 602, loss = 0.76019681\n",
      "Iteration 603, loss = 0.75969786\n",
      "Iteration 604, loss = 0.75919960\n",
      "Iteration 605, loss = 0.75870117\n",
      "Iteration 606, loss = 0.75820337\n",
      "Iteration 607, loss = 0.75770681\n",
      "Iteration 608, loss = 0.75721105\n",
      "Iteration 609, loss = 0.75671586\n",
      "Iteration 610, loss = 0.75622323\n",
      "Iteration 611, loss = 0.75573171\n",
      "Iteration 612, loss = 0.75524042\n",
      "Iteration 613, loss = 0.75475032\n",
      "Iteration 614, loss = 0.75426103\n",
      "Iteration 615, loss = 0.75377320\n",
      "Iteration 616, loss = 0.75328645\n",
      "Iteration 617, loss = 0.75280062\n",
      "Iteration 618, loss = 0.75231587\n",
      "Iteration 619, loss = 0.75183224\n",
      "Iteration 620, loss = 0.75134947\n",
      "Iteration 621, loss = 0.75086694\n",
      "Iteration 622, loss = 0.75038458\n",
      "Iteration 623, loss = 0.74990347\n",
      "Iteration 624, loss = 0.74942260\n",
      "Iteration 625, loss = 0.74894240\n",
      "Iteration 626, loss = 0.74846338\n",
      "Iteration 627, loss = 0.74798451\n",
      "Iteration 628, loss = 0.74750670\n",
      "Iteration 629, loss = 0.74703005\n",
      "Iteration 630, loss = 0.74655382\n",
      "Iteration 631, loss = 0.74607998\n",
      "Iteration 632, loss = 0.74560691\n",
      "Iteration 633, loss = 0.74513438\n",
      "Iteration 634, loss = 0.74466327\n",
      "Iteration 635, loss = 0.74419465\n",
      "Iteration 636, loss = 0.74372678\n",
      "Iteration 637, loss = 0.74326005\n",
      "Iteration 638, loss = 0.74279496\n",
      "Iteration 639, loss = 0.74233029\n",
      "Iteration 640, loss = 0.74186588\n",
      "Iteration 641, loss = 0.74140256\n",
      "Iteration 642, loss = 0.74094075\n",
      "Iteration 643, loss = 0.74047947\n",
      "Iteration 644, loss = 0.74001948\n",
      "Iteration 645, loss = 0.73956065\n",
      "Iteration 646, loss = 0.73910307\n",
      "Iteration 647, loss = 0.73864631\n",
      "Iteration 648, loss = 0.73819095\n",
      "Iteration 649, loss = 0.73773658\n",
      "Iteration 650, loss = 0.73728364\n",
      "Iteration 651, loss = 0.73683100\n",
      "Iteration 652, loss = 0.73637940\n",
      "Iteration 653, loss = 0.73592865\n",
      "Iteration 654, loss = 0.73547965\n",
      "Iteration 655, loss = 0.73503140\n",
      "Iteration 656, loss = 0.73458450\n",
      "Iteration 657, loss = 0.73413827\n",
      "Iteration 658, loss = 0.73369306\n",
      "Iteration 659, loss = 0.73324816\n",
      "Iteration 660, loss = 0.73280680\n",
      "Iteration 661, loss = 0.73236622\n",
      "Iteration 662, loss = 0.73192637\n",
      "Iteration 663, loss = 0.73148740\n",
      "Iteration 664, loss = 0.73104981\n",
      "Iteration 665, loss = 0.73061347\n",
      "Iteration 666, loss = 0.73017757\n",
      "Iteration 667, loss = 0.72974204\n",
      "Iteration 668, loss = 0.72930777\n",
      "Iteration 669, loss = 0.72887430\n",
      "Iteration 670, loss = 0.72844155\n",
      "Iteration 671, loss = 0.72800955\n",
      "Iteration 672, loss = 0.72757917\n",
      "Iteration 673, loss = 0.72714944\n",
      "Iteration 674, loss = 0.72672045\n",
      "Iteration 675, loss = 0.72629292\n",
      "Iteration 676, loss = 0.72586531\n",
      "Iteration 677, loss = 0.72543892\n",
      "Iteration 678, loss = 0.72501376\n",
      "Iteration 679, loss = 0.72458977\n",
      "Iteration 680, loss = 0.72416657\n",
      "Iteration 681, loss = 0.72374368\n",
      "Iteration 682, loss = 0.72332165\n",
      "Iteration 683, loss = 0.72289946\n",
      "Iteration 684, loss = 0.72247755\n",
      "Iteration 685, loss = 0.72205603\n",
      "Iteration 686, loss = 0.72163504\n",
      "Iteration 687, loss = 0.72121453\n",
      "Iteration 688, loss = 0.72079477\n",
      "Iteration 689, loss = 0.72037547\n",
      "Iteration 690, loss = 0.71995722\n",
      "Iteration 691, loss = 0.71954020\n",
      "Iteration 692, loss = 0.71912350\n",
      "Iteration 693, loss = 0.71870729\n",
      "Iteration 694, loss = 0.71829196\n",
      "Iteration 695, loss = 0.71787815\n",
      "Iteration 696, loss = 0.71746504\n",
      "Iteration 697, loss = 0.71705215\n",
      "Iteration 698, loss = 0.71663994\n",
      "Iteration 699, loss = 0.71622832\n",
      "Iteration 700, loss = 0.71581814\n",
      "Iteration 701, loss = 0.71540922\n",
      "Iteration 702, loss = 0.71500046\n",
      "Iteration 703, loss = 0.71459274\n",
      "Iteration 704, loss = 0.71418585\n",
      "Iteration 705, loss = 0.71377948\n",
      "Iteration 706, loss = 0.71337402\n",
      "Iteration 707, loss = 0.71296929\n",
      "Iteration 708, loss = 0.71256517\n",
      "Iteration 709, loss = 0.71216170\n",
      "Iteration 710, loss = 0.71175869\n",
      "Iteration 711, loss = 0.71135674\n",
      "Iteration 712, loss = 0.71095474\n",
      "Iteration 713, loss = 0.71055410\n",
      "Iteration 714, loss = 0.71015398\n",
      "Iteration 715, loss = 0.70975296\n",
      "Iteration 716, loss = 0.70935344\n",
      "Iteration 717, loss = 0.70895380\n",
      "Iteration 718, loss = 0.70855407\n",
      "Iteration 719, loss = 0.70815499\n",
      "Iteration 720, loss = 0.70775672\n",
      "Iteration 721, loss = 0.70735901\n",
      "Iteration 722, loss = 0.70696195\n",
      "Iteration 723, loss = 0.70656560\n",
      "Iteration 724, loss = 0.70616933\n",
      "Iteration 725, loss = 0.70577368\n",
      "Iteration 726, loss = 0.70537922\n",
      "Iteration 727, loss = 0.70498576\n",
      "Iteration 728, loss = 0.70459249\n",
      "Iteration 729, loss = 0.70419981\n",
      "Iteration 730, loss = 0.70380771\n",
      "Iteration 731, loss = 0.70341565\n",
      "Iteration 732, loss = 0.70302469\n",
      "Iteration 733, loss = 0.70263442\n",
      "Iteration 734, loss = 0.70224502\n",
      "Iteration 735, loss = 0.70185602\n",
      "Iteration 736, loss = 0.70146755\n",
      "Iteration 737, loss = 0.70108045\n",
      "Iteration 738, loss = 0.70069353\n",
      "Iteration 739, loss = 0.70030752\n",
      "Iteration 740, loss = 0.69992237\n",
      "Iteration 741, loss = 0.69953770\n",
      "Iteration 742, loss = 0.69915364\n",
      "Iteration 743, loss = 0.69877025\n",
      "Iteration 744, loss = 0.69838767\n",
      "Iteration 745, loss = 0.69800669\n",
      "Iteration 746, loss = 0.69762632\n",
      "Iteration 747, loss = 0.69724624\n",
      "Iteration 748, loss = 0.69686677\n",
      "Iteration 749, loss = 0.69648884\n",
      "Iteration 750, loss = 0.69611151\n",
      "Iteration 751, loss = 0.69573499\n",
      "Iteration 752, loss = 0.69535867\n",
      "Iteration 753, loss = 0.69498311\n",
      "Iteration 754, loss = 0.69460827\n",
      "Iteration 755, loss = 0.69423429\n",
      "Iteration 756, loss = 0.69385987\n",
      "Iteration 757, loss = 0.69348629\n",
      "Iteration 758, loss = 0.69311332\n",
      "Iteration 759, loss = 0.69274079\n",
      "Iteration 760, loss = 0.69236950\n",
      "Iteration 761, loss = 0.69199782\n",
      "Iteration 762, loss = 0.69162638\n",
      "Iteration 763, loss = 0.69125559\n",
      "Iteration 764, loss = 0.69088516\n",
      "Iteration 765, loss = 0.69051556\n",
      "Iteration 766, loss = 0.69014600\n",
      "Iteration 767, loss = 0.68977706\n",
      "Iteration 768, loss = 0.68940858\n",
      "Iteration 769, loss = 0.68904088\n",
      "Iteration 770, loss = 0.68867396\n",
      "Iteration 771, loss = 0.68830794\n",
      "Iteration 772, loss = 0.68794220\n",
      "Iteration 773, loss = 0.68757677\n",
      "Iteration 774, loss = 0.68721218\n",
      "Iteration 775, loss = 0.68684723\n",
      "Iteration 776, loss = 0.68648256\n",
      "Iteration 777, loss = 0.68611817\n",
      "Iteration 778, loss = 0.68575413\n",
      "Iteration 779, loss = 0.68539022\n",
      "Iteration 780, loss = 0.68502615\n",
      "Iteration 781, loss = 0.68466208\n",
      "Iteration 782, loss = 0.68429809\n",
      "Iteration 783, loss = 0.68393474\n",
      "Iteration 784, loss = 0.68357146\n",
      "Iteration 785, loss = 0.68320874\n",
      "Iteration 786, loss = 0.68284707\n",
      "Iteration 787, loss = 0.68248538\n",
      "Iteration 788, loss = 0.68212430\n",
      "Iteration 789, loss = 0.68176360\n",
      "Iteration 790, loss = 0.68140356\n",
      "Iteration 791, loss = 0.68104365\n",
      "Iteration 792, loss = 0.68068416\n",
      "Iteration 793, loss = 0.68032447\n",
      "Iteration 794, loss = 0.67996479\n",
      "Iteration 795, loss = 0.67960574\n",
      "Iteration 796, loss = 0.67924710\n",
      "Iteration 797, loss = 0.67888931\n",
      "Iteration 798, loss = 0.67853181\n",
      "Iteration 799, loss = 0.67817353\n",
      "Iteration 800, loss = 0.67781543\n",
      "Iteration 801, loss = 0.67745835\n",
      "Iteration 802, loss = 0.67710154\n",
      "Iteration 803, loss = 0.67674616\n",
      "Iteration 804, loss = 0.67639206\n",
      "Iteration 805, loss = 0.67603857\n",
      "Iteration 806, loss = 0.67568531\n",
      "Iteration 807, loss = 0.67533266\n",
      "Iteration 808, loss = 0.67498055\n",
      "Iteration 809, loss = 0.67462906\n",
      "Iteration 810, loss = 0.67427869\n",
      "Iteration 811, loss = 0.67392873\n",
      "Iteration 812, loss = 0.67357938\n",
      "Iteration 813, loss = 0.67323074\n",
      "Iteration 814, loss = 0.67288186\n",
      "Iteration 815, loss = 0.67253312\n",
      "Iteration 816, loss = 0.67218478\n",
      "Iteration 817, loss = 0.67183660\n",
      "Iteration 818, loss = 0.67148913\n",
      "Iteration 819, loss = 0.67114191\n",
      "Iteration 820, loss = 0.67079511\n",
      "Iteration 821, loss = 0.67044770\n",
      "Iteration 822, loss = 0.67010056\n",
      "Iteration 823, loss = 0.66975465\n",
      "Iteration 824, loss = 0.66940950\n",
      "Iteration 825, loss = 0.66906484\n",
      "Iteration 826, loss = 0.66872089\n",
      "Iteration 827, loss = 0.66837715\n",
      "Iteration 828, loss = 0.66803392\n",
      "Iteration 829, loss = 0.66769086\n",
      "Iteration 830, loss = 0.66734786\n",
      "Iteration 831, loss = 0.66700555\n",
      "Iteration 832, loss = 0.66666398\n",
      "Iteration 833, loss = 0.66632275\n",
      "Iteration 834, loss = 0.66598179\n",
      "Iteration 835, loss = 0.66564160\n",
      "Iteration 836, loss = 0.66530167\n",
      "Iteration 837, loss = 0.66496213\n",
      "Iteration 838, loss = 0.66462316\n",
      "Iteration 839, loss = 0.66428492\n",
      "Iteration 840, loss = 0.66394716\n",
      "Iteration 841, loss = 0.66360997\n",
      "Iteration 842, loss = 0.66327357\n",
      "Iteration 843, loss = 0.66293735\n",
      "Iteration 844, loss = 0.66260062\n",
      "Iteration 845, loss = 0.66226440\n",
      "Iteration 846, loss = 0.66192910\n",
      "Iteration 847, loss = 0.66159393\n",
      "Iteration 848, loss = 0.66125875\n",
      "Iteration 849, loss = 0.66092369\n",
      "Iteration 850, loss = 0.66058891\n",
      "Iteration 851, loss = 0.66025466\n",
      "Iteration 852, loss = 0.65992089\n",
      "Iteration 853, loss = 0.65958707\n",
      "Iteration 854, loss = 0.65925363\n",
      "Iteration 855, loss = 0.65892046\n",
      "Iteration 856, loss = 0.65858818\n",
      "Iteration 857, loss = 0.65825626\n",
      "Iteration 858, loss = 0.65792473\n",
      "Iteration 859, loss = 0.65759318\n",
      "Iteration 860, loss = 0.65726142\n",
      "Iteration 861, loss = 0.65693012\n",
      "Iteration 862, loss = 0.65659914\n",
      "Iteration 863, loss = 0.65626865\n",
      "Iteration 864, loss = 0.65593835\n",
      "Iteration 865, loss = 0.65560846\n",
      "Iteration 866, loss = 0.65527917\n",
      "Iteration 867, loss = 0.65495020\n",
      "Iteration 868, loss = 0.65462133\n",
      "Iteration 869, loss = 0.65429282\n",
      "Iteration 870, loss = 0.65396438\n",
      "Iteration 871, loss = 0.65363587\n",
      "Iteration 872, loss = 0.65330810\n",
      "Iteration 873, loss = 0.65298041\n",
      "Iteration 874, loss = 0.65265307\n",
      "Iteration 875, loss = 0.65232606\n",
      "Iteration 876, loss = 0.65199813\n",
      "Iteration 877, loss = 0.65167064\n",
      "Iteration 878, loss = 0.65134342\n",
      "Iteration 879, loss = 0.65101733\n",
      "Iteration 880, loss = 0.65069110\n",
      "Iteration 881, loss = 0.65036471\n",
      "Iteration 882, loss = 0.65003665\n",
      "Iteration 883, loss = 0.64970925\n",
      "Iteration 884, loss = 0.64938190\n",
      "Iteration 885, loss = 0.64905425\n",
      "Iteration 886, loss = 0.64872528\n",
      "Iteration 887, loss = 0.64839620\n",
      "Iteration 888, loss = 0.64806747\n",
      "Iteration 889, loss = 0.64773873\n",
      "Iteration 890, loss = 0.64740974\n",
      "Iteration 891, loss = 0.64708206\n",
      "Iteration 892, loss = 0.64675578\n",
      "Iteration 893, loss = 0.64642988\n",
      "Iteration 894, loss = 0.64610490\n",
      "Iteration 895, loss = 0.64578109\n",
      "Iteration 896, loss = 0.64545822\n",
      "Iteration 897, loss = 0.64513493\n",
      "Iteration 898, loss = 0.64481194\n",
      "Iteration 899, loss = 0.64449018\n",
      "Iteration 900, loss = 0.64416934\n",
      "Iteration 901, loss = 0.64384873\n",
      "Iteration 902, loss = 0.64352933\n",
      "Iteration 903, loss = 0.64321118\n",
      "Iteration 904, loss = 0.64289308\n",
      "Iteration 905, loss = 0.64257537\n",
      "Iteration 906, loss = 0.64225813\n",
      "Iteration 907, loss = 0.64194084\n",
      "Iteration 908, loss = 0.64162347\n",
      "Iteration 909, loss = 0.64130647\n",
      "Iteration 910, loss = 0.64099042\n",
      "Iteration 911, loss = 0.64067485\n",
      "Iteration 912, loss = 0.64035973\n",
      "Iteration 913, loss = 0.64004525\n",
      "Iteration 914, loss = 0.63973083\n",
      "Iteration 915, loss = 0.63941675\n",
      "Iteration 916, loss = 0.63910311\n",
      "Iteration 917, loss = 0.63878933\n",
      "Iteration 918, loss = 0.63847479\n",
      "Iteration 919, loss = 0.63816099\n",
      "Iteration 920, loss = 0.63784808\n",
      "Iteration 921, loss = 0.63753656\n",
      "Iteration 922, loss = 0.63722576\n",
      "Iteration 923, loss = 0.63691532\n",
      "Iteration 924, loss = 0.63660472\n",
      "Iteration 925, loss = 0.63629445\n",
      "Iteration 926, loss = 0.63598457\n",
      "Iteration 927, loss = 0.63567524\n",
      "Iteration 928, loss = 0.63536587\n",
      "Iteration 929, loss = 0.63505696\n",
      "Iteration 930, loss = 0.63474921\n",
      "Iteration 931, loss = 0.63444179\n",
      "Iteration 932, loss = 0.63413549\n",
      "Iteration 933, loss = 0.63382961\n",
      "Iteration 934, loss = 0.63352422\n",
      "Iteration 935, loss = 0.63321948\n",
      "Iteration 936, loss = 0.63291410\n",
      "Iteration 937, loss = 0.63260903\n",
      "Iteration 938, loss = 0.63230418\n",
      "Iteration 939, loss = 0.63199975\n",
      "Iteration 940, loss = 0.63169597\n",
      "Iteration 941, loss = 0.63139253\n",
      "Iteration 942, loss = 0.63108929\n",
      "Iteration 943, loss = 0.63078653\n",
      "Iteration 944, loss = 0.63048409\n",
      "Iteration 945, loss = 0.63018244\n",
      "Iteration 946, loss = 0.62988164\n",
      "Iteration 947, loss = 0.62958124\n",
      "Iteration 948, loss = 0.62928148\n",
      "Iteration 949, loss = 0.62898285\n",
      "Iteration 950, loss = 0.62868467\n",
      "Iteration 951, loss = 0.62838597\n",
      "Iteration 952, loss = 0.62808602\n",
      "Iteration 953, loss = 0.62778605\n",
      "Iteration 954, loss = 0.62748633\n",
      "Iteration 955, loss = 0.62718859\n",
      "Iteration 956, loss = 0.62689089\n",
      "Iteration 957, loss = 0.62659344\n",
      "Iteration 958, loss = 0.62629499\n",
      "Iteration 959, loss = 0.62599577\n",
      "Iteration 960, loss = 0.62569526\n",
      "Iteration 961, loss = 0.62539406\n",
      "Iteration 962, loss = 0.62509002\n",
      "Iteration 963, loss = 0.62478303\n",
      "Iteration 964, loss = 0.62447512\n",
      "Iteration 965, loss = 0.62416614\n",
      "Iteration 966, loss = 0.62386211\n",
      "Iteration 967, loss = 0.62355993\n",
      "Iteration 968, loss = 0.62325813\n",
      "Iteration 969, loss = 0.62295896\n",
      "Iteration 970, loss = 0.62266142\n",
      "Iteration 971, loss = 0.62236467\n",
      "Iteration 972, loss = 0.62206842\n",
      "Iteration 973, loss = 0.62177212\n",
      "Iteration 974, loss = 0.62147634\n",
      "Iteration 975, loss = 0.62118086\n",
      "Iteration 976, loss = 0.62088596\n",
      "Iteration 977, loss = 0.62059179\n",
      "Iteration 978, loss = 0.62029815\n",
      "Iteration 979, loss = 0.62000489\n",
      "Iteration 980, loss = 0.61971244\n",
      "Iteration 981, loss = 0.61942034\n",
      "Iteration 982, loss = 0.61912853\n",
      "Iteration 983, loss = 0.61883709\n",
      "Iteration 984, loss = 0.61854632\n",
      "Iteration 985, loss = 0.61825589\n",
      "Iteration 986, loss = 0.61796580\n",
      "Iteration 987, loss = 0.61767657\n",
      "Iteration 988, loss = 0.61738749\n",
      "Iteration 989, loss = 0.61709818\n",
      "Iteration 990, loss = 0.61680889\n",
      "Iteration 991, loss = 0.61651979\n",
      "Iteration 992, loss = 0.61623115\n",
      "Iteration 993, loss = 0.61594265\n",
      "Iteration 994, loss = 0.61565432\n",
      "Iteration 995, loss = 0.61536693\n",
      "Iteration 996, loss = 0.61507950\n",
      "Iteration 997, loss = 0.61479231\n",
      "Iteration 998, loss = 0.61450569\n",
      "Iteration 999, loss = 0.61421929\n",
      "Iteration 1000, loss = 0.61393270\n",
      "Iteration 1001, loss = 0.61364679\n",
      "Iteration 1002, loss = 0.61336077\n",
      "Iteration 1003, loss = 0.61307516\n",
      "Iteration 1004, loss = 0.61278990\n",
      "Iteration 1005, loss = 0.61250539\n",
      "Iteration 1006, loss = 0.61222089\n",
      "Iteration 1007, loss = 0.61193663\n",
      "Iteration 1008, loss = 0.61165270\n",
      "Iteration 1009, loss = 0.61136902\n",
      "Iteration 1010, loss = 0.61108552\n",
      "Iteration 1011, loss = 0.61080223\n",
      "Iteration 1012, loss = 0.61051926\n",
      "Iteration 1013, loss = 0.61023644\n",
      "Iteration 1014, loss = 0.60995369\n",
      "Iteration 1015, loss = 0.60967146\n",
      "Iteration 1016, loss = 0.60938946\n",
      "Iteration 1017, loss = 0.60910739\n",
      "Iteration 1018, loss = 0.60882616\n",
      "Iteration 1019, loss = 0.60854532\n",
      "Iteration 1020, loss = 0.60826536\n",
      "Iteration 1021, loss = 0.60798569\n",
      "Iteration 1022, loss = 0.60770650\n",
      "Iteration 1023, loss = 0.60742756\n",
      "Iteration 1024, loss = 0.60714870\n",
      "Iteration 1025, loss = 0.60686996\n",
      "Iteration 1026, loss = 0.60659180\n",
      "Iteration 1027, loss = 0.60631408\n",
      "Iteration 1028, loss = 0.60603636\n",
      "Iteration 1029, loss = 0.60575888\n",
      "Iteration 1030, loss = 0.60548223\n",
      "Iteration 1031, loss = 0.60520610\n",
      "Iteration 1032, loss = 0.60493011\n",
      "Iteration 1033, loss = 0.60465443\n",
      "Iteration 1034, loss = 0.60437926\n",
      "Iteration 1035, loss = 0.60410458\n",
      "Iteration 1036, loss = 0.60383019\n",
      "Iteration 1037, loss = 0.60355616\n",
      "Iteration 1038, loss = 0.60328247\n",
      "Iteration 1039, loss = 0.60300933\n",
      "Iteration 1040, loss = 0.60273664\n",
      "Iteration 1041, loss = 0.60246417\n",
      "Iteration 1042, loss = 0.60219198\n",
      "Iteration 1043, loss = 0.60192025\n",
      "Iteration 1044, loss = 0.60164896\n",
      "Iteration 1045, loss = 0.60137784\n",
      "Iteration 1046, loss = 0.60110738\n",
      "Iteration 1047, loss = 0.60083715\n",
      "Iteration 1048, loss = 0.60056717\n",
      "Iteration 1049, loss = 0.60029764\n",
      "Iteration 1050, loss = 0.60002857\n",
      "Iteration 1051, loss = 0.59975961\n",
      "Iteration 1052, loss = 0.59949125\n",
      "Iteration 1053, loss = 0.59922314\n",
      "Iteration 1054, loss = 0.59895530\n",
      "Iteration 1055, loss = 0.59868786\n",
      "Iteration 1056, loss = 0.59842076\n",
      "Iteration 1057, loss = 0.59815333\n",
      "Iteration 1058, loss = 0.59788631\n",
      "Iteration 1059, loss = 0.59761943\n",
      "Iteration 1060, loss = 0.59735273\n",
      "Iteration 1061, loss = 0.59708639\n",
      "Iteration 1062, loss = 0.59682033\n",
      "Iteration 1063, loss = 0.59655403\n",
      "Iteration 1064, loss = 0.59628789\n",
      "Iteration 1065, loss = 0.59602172\n",
      "Iteration 1066, loss = 0.59575635\n",
      "Iteration 1067, loss = 0.59549118\n",
      "Iteration 1068, loss = 0.59522602\n",
      "Iteration 1069, loss = 0.59496135\n",
      "Iteration 1070, loss = 0.59469692\n",
      "Iteration 1071, loss = 0.59443267\n",
      "Iteration 1072, loss = 0.59416880\n",
      "Iteration 1073, loss = 0.59390548\n",
      "Iteration 1074, loss = 0.59364245\n",
      "Iteration 1075, loss = 0.59337953\n",
      "Iteration 1076, loss = 0.59311714\n",
      "Iteration 1077, loss = 0.59285499\n",
      "Iteration 1078, loss = 0.59259324\n",
      "Iteration 1079, loss = 0.59233164\n",
      "Iteration 1080, loss = 0.59207066\n",
      "Iteration 1081, loss = 0.59180981\n",
      "Iteration 1082, loss = 0.59154907\n",
      "Iteration 1083, loss = 0.59128866\n",
      "Iteration 1084, loss = 0.59102852\n",
      "Iteration 1085, loss = 0.59076868\n",
      "Iteration 1086, loss = 0.59050927\n",
      "Iteration 1087, loss = 0.59024965\n",
      "Iteration 1088, loss = 0.58998930\n",
      "Iteration 1089, loss = 0.58972848\n",
      "Iteration 1090, loss = 0.58946748\n",
      "Iteration 1091, loss = 0.58920628\n",
      "Iteration 1092, loss = 0.58894503\n",
      "Iteration 1093, loss = 0.58868398\n",
      "Iteration 1094, loss = 0.58842233\n",
      "Iteration 1095, loss = 0.58816073\n",
      "Iteration 1096, loss = 0.58789896\n",
      "Iteration 1097, loss = 0.58763725\n",
      "Iteration 1098, loss = 0.58737586\n",
      "Iteration 1099, loss = 0.58711449\n",
      "Iteration 1100, loss = 0.58685320\n",
      "Iteration 1101, loss = 0.58659210\n",
      "Iteration 1102, loss = 0.58633114\n",
      "Iteration 1103, loss = 0.58607025\n",
      "Iteration 1104, loss = 0.58580909\n",
      "Iteration 1105, loss = 0.58554823\n",
      "Iteration 1106, loss = 0.58528763\n",
      "Iteration 1107, loss = 0.58502764\n",
      "Iteration 1108, loss = 0.58476780\n",
      "Iteration 1109, loss = 0.58450818\n",
      "Iteration 1110, loss = 0.58424891\n",
      "Iteration 1111, loss = 0.58398981\n",
      "Iteration 1112, loss = 0.58373091\n",
      "Iteration 1113, loss = 0.58347244\n",
      "Iteration 1114, loss = 0.58321414\n",
      "Iteration 1115, loss = 0.58295602\n",
      "Iteration 1116, loss = 0.58269904\n",
      "Iteration 1117, loss = 0.58244258\n",
      "Iteration 1118, loss = 0.58218651\n",
      "Iteration 1119, loss = 0.58193135\n",
      "Iteration 1120, loss = 0.58167638\n",
      "Iteration 1121, loss = 0.58142183\n",
      "Iteration 1122, loss = 0.58116746\n",
      "Iteration 1123, loss = 0.58091367\n",
      "Iteration 1124, loss = 0.58066019\n",
      "Iteration 1125, loss = 0.58040670\n",
      "Iteration 1126, loss = 0.58015329\n",
      "Iteration 1127, loss = 0.57990026\n",
      "Iteration 1128, loss = 0.57964759\n",
      "Iteration 1129, loss = 0.57939518\n",
      "Iteration 1130, loss = 0.57914307\n",
      "Iteration 1131, loss = 0.57889146\n",
      "Iteration 1132, loss = 0.57864047\n",
      "Iteration 1133, loss = 0.57838998\n",
      "Iteration 1134, loss = 0.57813983\n",
      "Iteration 1135, loss = 0.57789035\n",
      "Iteration 1136, loss = 0.57764099\n",
      "Iteration 1137, loss = 0.57739190\n",
      "Iteration 1138, loss = 0.57714302\n",
      "Iteration 1139, loss = 0.57689443\n",
      "Iteration 1140, loss = 0.57664626\n",
      "Iteration 1141, loss = 0.57639833\n",
      "Iteration 1142, loss = 0.57615055\n",
      "Iteration 1143, loss = 0.57590286\n",
      "Iteration 1144, loss = 0.57565522\n",
      "Iteration 1145, loss = 0.57540761\n",
      "Iteration 1146, loss = 0.57516011\n",
      "Iteration 1147, loss = 0.57491261\n",
      "Iteration 1148, loss = 0.57466531\n",
      "Iteration 1149, loss = 0.57441799\n",
      "Iteration 1150, loss = 0.57417087\n",
      "Iteration 1151, loss = 0.57392411\n",
      "Iteration 1152, loss = 0.57367849\n",
      "Iteration 1153, loss = 0.57343368\n",
      "Iteration 1154, loss = 0.57318930\n",
      "Iteration 1155, loss = 0.57294519\n",
      "Iteration 1156, loss = 0.57270150\n",
      "Iteration 1157, loss = 0.57245808\n",
      "Iteration 1158, loss = 0.57221483\n",
      "Iteration 1159, loss = 0.57197129\n",
      "Iteration 1160, loss = 0.57172788\n",
      "Iteration 1161, loss = 0.57148512\n",
      "Iteration 1162, loss = 0.57124281\n",
      "Iteration 1163, loss = 0.57100020\n",
      "Iteration 1164, loss = 0.57075714\n",
      "Iteration 1165, loss = 0.57051421\n",
      "Iteration 1166, loss = 0.57027141\n",
      "Iteration 1167, loss = 0.57002862\n",
      "Iteration 1168, loss = 0.56978610\n",
      "Iteration 1169, loss = 0.56954377\n",
      "Iteration 1170, loss = 0.56930151\n",
      "Iteration 1171, loss = 0.56905953\n",
      "Iteration 1172, loss = 0.56881784\n",
      "Iteration 1173, loss = 0.56857606\n",
      "Iteration 1174, loss = 0.56833465\n",
      "Iteration 1175, loss = 0.56809438\n",
      "Iteration 1176, loss = 0.56785553\n",
      "Iteration 1177, loss = 0.56761630\n",
      "Iteration 1178, loss = 0.56737680\n",
      "Iteration 1179, loss = 0.56713774\n",
      "Iteration 1180, loss = 0.56690022\n",
      "Iteration 1181, loss = 0.56666360\n",
      "Iteration 1182, loss = 0.56642742\n",
      "Iteration 1183, loss = 0.56619160\n",
      "Iteration 1184, loss = 0.56595584\n",
      "Iteration 1185, loss = 0.56572056\n",
      "Iteration 1186, loss = 0.56548487\n",
      "Iteration 1187, loss = 0.56524917\n",
      "Iteration 1188, loss = 0.56501362\n",
      "Iteration 1189, loss = 0.56477840\n",
      "Iteration 1190, loss = 0.56454301\n",
      "Iteration 1191, loss = 0.56430726\n",
      "Iteration 1192, loss = 0.56407216\n",
      "Iteration 1193, loss = 0.56383731\n",
      "Iteration 1194, loss = 0.56360261\n",
      "Iteration 1195, loss = 0.56336813\n",
      "Iteration 1196, loss = 0.56313377\n",
      "Iteration 1197, loss = 0.56289969\n",
      "Iteration 1198, loss = 0.56266584\n",
      "Iteration 1199, loss = 0.56243226\n",
      "Iteration 1200, loss = 0.56219899\n",
      "Iteration 1201, loss = 0.56196582\n",
      "Iteration 1202, loss = 0.56173286\n",
      "Iteration 1203, loss = 0.56150047\n",
      "Iteration 1204, loss = 0.56126805\n",
      "Iteration 1205, loss = 0.56103484\n",
      "Iteration 1206, loss = 0.56080169\n",
      "Iteration 1207, loss = 0.56056871\n",
      "Iteration 1208, loss = 0.56033583\n",
      "Iteration 1209, loss = 0.56010472\n",
      "Iteration 1210, loss = 0.55987410\n",
      "Iteration 1211, loss = 0.55964416\n",
      "Iteration 1212, loss = 0.55941471\n",
      "Iteration 1213, loss = 0.55918548\n",
      "Iteration 1214, loss = 0.55895652\n",
      "Iteration 1215, loss = 0.55872797\n",
      "Iteration 1216, loss = 0.55849971\n",
      "Iteration 1217, loss = 0.55827179\n",
      "Iteration 1218, loss = 0.55804409\n",
      "Iteration 1219, loss = 0.55781645\n",
      "Iteration 1220, loss = 0.55758902\n",
      "Iteration 1221, loss = 0.55736191\n",
      "Iteration 1222, loss = 0.55713493\n",
      "Iteration 1223, loss = 0.55690827\n",
      "Iteration 1224, loss = 0.55668203\n",
      "Iteration 1225, loss = 0.55645606\n",
      "Iteration 1226, loss = 0.55623034\n",
      "Iteration 1227, loss = 0.55600471\n",
      "Iteration 1228, loss = 0.55577964\n",
      "Iteration 1229, loss = 0.55555473\n",
      "Iteration 1230, loss = 0.55532993\n",
      "Iteration 1231, loss = 0.55510552\n",
      "Iteration 1232, loss = 0.55488130\n",
      "Iteration 1233, loss = 0.55465718\n",
      "Iteration 1234, loss = 0.55443351\n",
      "Iteration 1235, loss = 0.55421036\n",
      "Iteration 1236, loss = 0.55398716\n",
      "Iteration 1237, loss = 0.55376420\n",
      "Iteration 1238, loss = 0.55354152\n",
      "Iteration 1239, loss = 0.55331910\n",
      "Iteration 1240, loss = 0.55309703\n",
      "Iteration 1241, loss = 0.55287520\n",
      "Iteration 1242, loss = 0.55265340\n",
      "Iteration 1243, loss = 0.55243201\n",
      "Iteration 1244, loss = 0.55221118\n",
      "Iteration 1245, loss = 0.55199053\n",
      "Iteration 1246, loss = 0.55177012\n",
      "Iteration 1247, loss = 0.55154987\n",
      "Iteration 1248, loss = 0.55132978\n",
      "Iteration 1249, loss = 0.55110976\n",
      "Iteration 1250, loss = 0.55089002\n",
      "Iteration 1251, loss = 0.55067050\n",
      "Iteration 1252, loss = 0.55045123\n",
      "Iteration 1253, loss = 0.55023219\n",
      "Iteration 1254, loss = 0.55001318\n",
      "Iteration 1255, loss = 0.54979453\n",
      "Iteration 1256, loss = 0.54957618\n",
      "Iteration 1257, loss = 0.54935799\n",
      "Iteration 1258, loss = 0.54913985\n",
      "Iteration 1259, loss = 0.54892220\n",
      "Iteration 1260, loss = 0.54870482\n",
      "Iteration 1261, loss = 0.54848733\n",
      "Iteration 1262, loss = 0.54827026\n",
      "Iteration 1263, loss = 0.54805341\n",
      "Iteration 1264, loss = 0.54783655\n",
      "Iteration 1265, loss = 0.54762006\n",
      "Iteration 1266, loss = 0.54740384\n",
      "Iteration 1267, loss = 0.54718774\n",
      "Iteration 1268, loss = 0.54697186\n",
      "Iteration 1269, loss = 0.54675631\n",
      "Iteration 1270, loss = 0.54654098\n",
      "Iteration 1271, loss = 0.54632589\n",
      "Iteration 1272, loss = 0.54611080\n",
      "Iteration 1273, loss = 0.54589611\n",
      "Iteration 1274, loss = 0.54568166\n",
      "Iteration 1275, loss = 0.54546721\n",
      "Iteration 1276, loss = 0.54525298\n",
      "Iteration 1277, loss = 0.54503917\n",
      "Iteration 1278, loss = 0.54482555\n",
      "Iteration 1279, loss = 0.54461211\n",
      "Iteration 1280, loss = 0.54439933\n",
      "Iteration 1281, loss = 0.54418752\n",
      "Iteration 1282, loss = 0.54397581\n",
      "Iteration 1283, loss = 0.54376447\n",
      "Iteration 1284, loss = 0.54355340\n",
      "Iteration 1285, loss = 0.54334248\n",
      "Iteration 1286, loss = 0.54313188\n",
      "Iteration 1287, loss = 0.54292136\n",
      "Iteration 1288, loss = 0.54271131\n",
      "Iteration 1289, loss = 0.54250143\n",
      "Iteration 1290, loss = 0.54229164\n",
      "Iteration 1291, loss = 0.54208194\n",
      "Iteration 1292, loss = 0.54187258\n",
      "Iteration 1293, loss = 0.54166345\n",
      "Iteration 1294, loss = 0.54145439\n",
      "Iteration 1295, loss = 0.54124547\n",
      "Iteration 1296, loss = 0.54103651\n",
      "Iteration 1297, loss = 0.54082793\n",
      "Iteration 1298, loss = 0.54061960\n",
      "Iteration 1299, loss = 0.54041143\n",
      "Iteration 1300, loss = 0.54020346\n",
      "Iteration 1301, loss = 0.53999567\n",
      "Iteration 1302, loss = 0.53978809\n",
      "Iteration 1303, loss = 0.53957973\n",
      "Iteration 1304, loss = 0.53936960\n",
      "Iteration 1305, loss = 0.53915965\n",
      "Iteration 1306, loss = 0.53895009\n",
      "Iteration 1307, loss = 0.53874047\n",
      "Iteration 1308, loss = 0.53853076\n",
      "Iteration 1309, loss = 0.53832085\n",
      "Iteration 1310, loss = 0.53811093\n",
      "Iteration 1311, loss = 0.53790136\n",
      "Iteration 1312, loss = 0.53769187\n",
      "Iteration 1313, loss = 0.53748244\n",
      "Iteration 1314, loss = 0.53727315\n",
      "Iteration 1315, loss = 0.53706389\n",
      "Iteration 1316, loss = 0.53685472\n",
      "Iteration 1317, loss = 0.53664584\n",
      "Iteration 1318, loss = 0.53643698\n",
      "Iteration 1319, loss = 0.53622886\n",
      "Iteration 1320, loss = 0.53602146\n",
      "Iteration 1321, loss = 0.53581367\n",
      "Iteration 1322, loss = 0.53560487\n",
      "Iteration 1323, loss = 0.53539609\n",
      "Iteration 1324, loss = 0.53518715\n",
      "Iteration 1325, loss = 0.53497842\n",
      "Iteration 1326, loss = 0.53477017\n",
      "Iteration 1327, loss = 0.53456307\n",
      "Iteration 1328, loss = 0.53435611\n",
      "Iteration 1329, loss = 0.53414963\n",
      "Iteration 1330, loss = 0.53394343\n",
      "Iteration 1331, loss = 0.53373720\n",
      "Iteration 1332, loss = 0.53353132\n",
      "Iteration 1333, loss = 0.53332552\n",
      "Iteration 1334, loss = 0.53312001\n",
      "Iteration 1335, loss = 0.53291275\n",
      "Iteration 1336, loss = 0.53270280\n",
      "Iteration 1337, loss = 0.53249439\n",
      "Iteration 1338, loss = 0.53228680\n",
      "Iteration 1339, loss = 0.53207899\n",
      "Iteration 1340, loss = 0.53187335\n",
      "Iteration 1341, loss = 0.53166881\n",
      "Iteration 1342, loss = 0.53146434\n",
      "Iteration 1343, loss = 0.53126017\n",
      "Iteration 1344, loss = 0.53105786\n",
      "Iteration 1345, loss = 0.53085625\n",
      "Iteration 1346, loss = 0.53065308\n",
      "Iteration 1347, loss = 0.53045029\n",
      "Iteration 1348, loss = 0.53024918\n",
      "Iteration 1349, loss = 0.53004749\n",
      "Iteration 1350, loss = 0.52984389\n",
      "Iteration 1351, loss = 0.52963987\n",
      "Iteration 1352, loss = 0.52943576\n",
      "Iteration 1353, loss = 0.52923151\n",
      "Iteration 1354, loss = 0.52902814\n",
      "Iteration 1355, loss = 0.52882629\n",
      "Iteration 1356, loss = 0.52862472\n",
      "Iteration 1357, loss = 0.52842472\n",
      "Iteration 1358, loss = 0.52822583\n",
      "Iteration 1359, loss = 0.52802738\n",
      "Iteration 1360, loss = 0.52782942\n",
      "Iteration 1361, loss = 0.52763191\n",
      "Iteration 1362, loss = 0.52743468\n",
      "Iteration 1363, loss = 0.52723779\n",
      "Iteration 1364, loss = 0.52704076\n",
      "Iteration 1365, loss = 0.52684411\n",
      "Iteration 1366, loss = 0.52664756\n",
      "Iteration 1367, loss = 0.52645100\n",
      "Iteration 1368, loss = 0.52625484\n",
      "Iteration 1369, loss = 0.52605877\n",
      "Iteration 1370, loss = 0.52586271\n",
      "Iteration 1371, loss = 0.52566702\n",
      "Iteration 1372, loss = 0.52547170\n",
      "Iteration 1373, loss = 0.52527649\n",
      "Iteration 1374, loss = 0.52508137\n",
      "Iteration 1375, loss = 0.52488650\n",
      "Iteration 1376, loss = 0.52469204\n",
      "Iteration 1377, loss = 0.52449782\n",
      "Iteration 1378, loss = 0.52430368\n",
      "Iteration 1379, loss = 0.52410989\n",
      "Iteration 1380, loss = 0.52391633\n",
      "Iteration 1381, loss = 0.52372324\n",
      "Iteration 1382, loss = 0.52353020\n",
      "Iteration 1383, loss = 0.52333700\n",
      "Iteration 1384, loss = 0.52314404\n",
      "Iteration 1385, loss = 0.52295130\n",
      "Iteration 1386, loss = 0.52275853\n",
      "Iteration 1387, loss = 0.52256575\n",
      "Iteration 1388, loss = 0.52237317\n",
      "Iteration 1389, loss = 0.52218074\n",
      "Iteration 1390, loss = 0.52198828\n",
      "Iteration 1391, loss = 0.52179590\n",
      "Iteration 1392, loss = 0.52160360\n",
      "Iteration 1393, loss = 0.52141177\n",
      "Iteration 1394, loss = 0.52122014\n",
      "Iteration 1395, loss = 0.52102871\n",
      "Iteration 1396, loss = 0.52083769\n",
      "Iteration 1397, loss = 0.52064686\n",
      "Iteration 1398, loss = 0.52045610\n",
      "Iteration 1399, loss = 0.52026548\n",
      "Iteration 1400, loss = 0.52007499\n",
      "Iteration 1401, loss = 0.51988465\n",
      "Iteration 1402, loss = 0.51969453\n",
      "Iteration 1403, loss = 0.51950468\n",
      "Iteration 1404, loss = 0.51931492\n",
      "Iteration 1405, loss = 0.51912545\n",
      "Iteration 1406, loss = 0.51893605\n",
      "Iteration 1407, loss = 0.51874671\n",
      "Iteration 1408, loss = 0.51855756\n",
      "Iteration 1409, loss = 0.51836876\n",
      "Iteration 1410, loss = 0.51818005\n",
      "Iteration 1411, loss = 0.51799148\n",
      "Iteration 1412, loss = 0.51780301\n",
      "Iteration 1413, loss = 0.51761466\n",
      "Iteration 1414, loss = 0.51742657\n",
      "Iteration 1415, loss = 0.51723877\n",
      "Iteration 1416, loss = 0.51705109\n",
      "Iteration 1417, loss = 0.51686367\n",
      "Iteration 1418, loss = 0.51667644\n",
      "Iteration 1419, loss = 0.51648925\n",
      "Iteration 1420, loss = 0.51630236\n",
      "Iteration 1421, loss = 0.51611563\n",
      "Iteration 1422, loss = 0.51592906\n",
      "Iteration 1423, loss = 0.51574275\n",
      "Iteration 1424, loss = 0.51555649\n",
      "Iteration 1425, loss = 0.51537045\n",
      "Iteration 1426, loss = 0.51518457\n",
      "Iteration 1427, loss = 0.51499879\n",
      "Iteration 1428, loss = 0.51481341\n",
      "Iteration 1429, loss = 0.51462814\n",
      "Iteration 1430, loss = 0.51444300\n",
      "Iteration 1431, loss = 0.51425808\n",
      "Iteration 1432, loss = 0.51407336\n",
      "Iteration 1433, loss = 0.51388877\n",
      "Iteration 1434, loss = 0.51370441\n",
      "Iteration 1435, loss = 0.51352023\n",
      "Iteration 1436, loss = 0.51333615\n",
      "Iteration 1437, loss = 0.51315218\n",
      "Iteration 1438, loss = 0.51296831\n",
      "Iteration 1439, loss = 0.51278461\n",
      "Iteration 1440, loss = 0.51260107\n",
      "Iteration 1441, loss = 0.51241794\n",
      "Iteration 1442, loss = 0.51223498\n",
      "Iteration 1443, loss = 0.51205215\n",
      "Iteration 1444, loss = 0.51186939\n",
      "Iteration 1445, loss = 0.51168682\n",
      "Iteration 1446, loss = 0.51150431\n",
      "Iteration 1447, loss = 0.51132205\n",
      "Iteration 1448, loss = 0.51113985\n",
      "Iteration 1449, loss = 0.51095770\n",
      "Iteration 1450, loss = 0.51077578\n",
      "Iteration 1451, loss = 0.51059412\n",
      "Iteration 1452, loss = 0.51041256\n",
      "Iteration 1453, loss = 0.51023116\n",
      "Iteration 1454, loss = 0.51004992\n",
      "Iteration 1455, loss = 0.50986893\n",
      "Iteration 1456, loss = 0.50968808\n",
      "Iteration 1457, loss = 0.50950746\n",
      "Iteration 1458, loss = 0.50932687\n",
      "Iteration 1459, loss = 0.50914643\n",
      "Iteration 1460, loss = 0.50896629\n",
      "Iteration 1461, loss = 0.50878616\n",
      "Iteration 1462, loss = 0.50860615\n",
      "Iteration 1463, loss = 0.50842622\n",
      "Iteration 1464, loss = 0.50824629\n",
      "Iteration 1465, loss = 0.50806642\n",
      "Iteration 1466, loss = 0.50788672\n",
      "Iteration 1467, loss = 0.50770724\n",
      "Iteration 1468, loss = 0.50752780\n",
      "Iteration 1469, loss = 0.50734842\n",
      "Iteration 1470, loss = 0.50716919\n",
      "Iteration 1471, loss = 0.50698998\n",
      "Iteration 1472, loss = 0.50681091\n",
      "Iteration 1473, loss = 0.50663205\n",
      "Iteration 1474, loss = 0.50645330\n",
      "Iteration 1475, loss = 0.50627466\n",
      "Iteration 1476, loss = 0.50609630\n",
      "Iteration 1477, loss = 0.50591786\n",
      "Iteration 1478, loss = 0.50573955\n",
      "Iteration 1479, loss = 0.50556145\n",
      "Iteration 1480, loss = 0.50538332\n",
      "Iteration 1481, loss = 0.50520524\n",
      "Iteration 1482, loss = 0.50502732\n",
      "Iteration 1483, loss = 0.50484944\n",
      "Iteration 1484, loss = 0.50467153\n",
      "Iteration 1485, loss = 0.50449378\n",
      "Iteration 1486, loss = 0.50431628\n",
      "Iteration 1487, loss = 0.50413876\n",
      "Iteration 1488, loss = 0.50396143\n",
      "Iteration 1489, loss = 0.50378435\n",
      "Iteration 1490, loss = 0.50360751\n",
      "Iteration 1491, loss = 0.50343089\n",
      "Iteration 1492, loss = 0.50325442\n",
      "Iteration 1493, loss = 0.50307809\n",
      "Iteration 1494, loss = 0.50290187\n",
      "Iteration 1495, loss = 0.50272575\n",
      "Iteration 1496, loss = 0.50254985\n",
      "Iteration 1497, loss = 0.50237418\n",
      "Iteration 1498, loss = 0.50219870\n",
      "Iteration 1499, loss = 0.50202328\n",
      "Iteration 1500, loss = 0.50184800\n",
      "Iteration 1501, loss = 0.50167295\n",
      "Iteration 1502, loss = 0.50149814\n",
      "Iteration 1503, loss = 0.50132352\n",
      "Iteration 1504, loss = 0.50114898\n",
      "Iteration 1505, loss = 0.50097456\n",
      "Iteration 1506, loss = 0.50080032\n",
      "Iteration 1507, loss = 0.50062630\n",
      "Iteration 1508, loss = 0.50045227\n",
      "Iteration 1509, loss = 0.50027837\n",
      "Iteration 1510, loss = 0.50010467\n",
      "Iteration 1511, loss = 0.49993091\n",
      "Iteration 1512, loss = 0.49975755\n",
      "Iteration 1513, loss = 0.49958435\n",
      "Iteration 1514, loss = 0.49941119\n",
      "Iteration 1515, loss = 0.49923812\n",
      "Iteration 1516, loss = 0.49906509\n",
      "Iteration 1517, loss = 0.49889235\n",
      "Iteration 1518, loss = 0.49871965\n",
      "Iteration 1519, loss = 0.49854720\n",
      "Iteration 1520, loss = 0.49837481\n",
      "Iteration 1521, loss = 0.49820275\n",
      "Iteration 1522, loss = 0.49803059\n",
      "Iteration 1523, loss = 0.49785864\n",
      "Iteration 1524, loss = 0.49768681\n",
      "Iteration 1525, loss = 0.49751513\n",
      "Iteration 1526, loss = 0.49734355\n",
      "Iteration 1527, loss = 0.49717214\n",
      "Iteration 1528, loss = 0.49700098\n",
      "Iteration 1529, loss = 0.49682990\n",
      "Iteration 1530, loss = 0.49665898\n",
      "Iteration 1531, loss = 0.49648792\n",
      "Iteration 1532, loss = 0.49631681\n",
      "Iteration 1533, loss = 0.49614583\n",
      "Iteration 1534, loss = 0.49597506\n",
      "Iteration 1535, loss = 0.49580434\n",
      "Iteration 1536, loss = 0.49563354\n",
      "Iteration 1537, loss = 0.49546310\n",
      "Iteration 1538, loss = 0.49529271\n",
      "Iteration 1539, loss = 0.49512254\n",
      "Iteration 1540, loss = 0.49495257\n",
      "Iteration 1541, loss = 0.49478256\n",
      "Iteration 1542, loss = 0.49461280\n",
      "Iteration 1543, loss = 0.49444317\n",
      "Iteration 1544, loss = 0.49427366\n",
      "Iteration 1545, loss = 0.49410425\n",
      "Iteration 1546, loss = 0.49393453\n",
      "Iteration 1547, loss = 0.49376487\n",
      "Iteration 1548, loss = 0.49359519\n",
      "Iteration 1549, loss = 0.49342566\n",
      "Iteration 1550, loss = 0.49325631\n",
      "Iteration 1551, loss = 0.49308703\n",
      "Iteration 1552, loss = 0.49291796\n",
      "Iteration 1553, loss = 0.49274889\n",
      "Iteration 1554, loss = 0.49257986\n",
      "Iteration 1555, loss = 0.49241111\n",
      "Iteration 1556, loss = 0.49224255\n",
      "Iteration 1557, loss = 0.49207402\n",
      "Iteration 1558, loss = 0.49190560\n",
      "Iteration 1559, loss = 0.49173729\n",
      "Iteration 1560, loss = 0.49156913\n",
      "Iteration 1561, loss = 0.49140123\n",
      "Iteration 1562, loss = 0.49123352\n",
      "Iteration 1563, loss = 0.49106598\n",
      "Iteration 1564, loss = 0.49089859\n",
      "Iteration 1565, loss = 0.49073142\n",
      "Iteration 1566, loss = 0.49056436\n",
      "Iteration 1567, loss = 0.49039747\n",
      "Iteration 1568, loss = 0.49023061\n",
      "Iteration 1569, loss = 0.49006387\n",
      "Iteration 1570, loss = 0.48989710\n",
      "Iteration 1571, loss = 0.48973042\n",
      "Iteration 1572, loss = 0.48956401\n",
      "Iteration 1573, loss = 0.48939764\n",
      "Iteration 1574, loss = 0.48923176\n",
      "Iteration 1575, loss = 0.48906622\n",
      "Iteration 1576, loss = 0.48890059\n",
      "Iteration 1577, loss = 0.48873502\n",
      "Iteration 1578, loss = 0.48856970\n",
      "Iteration 1579, loss = 0.48840459\n",
      "Iteration 1580, loss = 0.48823952\n",
      "Iteration 1581, loss = 0.48807460\n",
      "Iteration 1582, loss = 0.48790985\n",
      "Iteration 1583, loss = 0.48774529\n",
      "Iteration 1584, loss = 0.48758094\n",
      "Iteration 1585, loss = 0.48741661\n",
      "Iteration 1586, loss = 0.48725255\n",
      "Iteration 1587, loss = 0.48708850\n",
      "Iteration 1588, loss = 0.48692479\n",
      "Iteration 1589, loss = 0.48676119\n",
      "Iteration 1590, loss = 0.48659778\n",
      "Iteration 1591, loss = 0.48643453\n",
      "Iteration 1592, loss = 0.48627143\n",
      "Iteration 1593, loss = 0.48610834\n",
      "Iteration 1594, loss = 0.48594556\n",
      "Iteration 1595, loss = 0.48578298\n",
      "Iteration 1596, loss = 0.48562061\n",
      "Iteration 1597, loss = 0.48545829\n",
      "Iteration 1598, loss = 0.48529594\n",
      "Iteration 1599, loss = 0.48513384\n",
      "Iteration 1600, loss = 0.48497193\n",
      "Iteration 1601, loss = 0.48481006\n",
      "Iteration 1602, loss = 0.48464831\n",
      "Iteration 1603, loss = 0.48448673\n",
      "Iteration 1604, loss = 0.48432540\n",
      "Iteration 1605, loss = 0.48416425\n",
      "Iteration 1606, loss = 0.48400313\n",
      "Iteration 1607, loss = 0.48384224\n",
      "Iteration 1608, loss = 0.48368135\n",
      "Iteration 1609, loss = 0.48352076\n",
      "Iteration 1610, loss = 0.48336040\n",
      "Iteration 1611, loss = 0.48320009\n",
      "Iteration 1612, loss = 0.48303988\n",
      "Iteration 1613, loss = 0.48287993\n",
      "Iteration 1614, loss = 0.48272020\n",
      "Iteration 1615, loss = 0.48256072\n",
      "Iteration 1616, loss = 0.48240145\n",
      "Iteration 1617, loss = 0.48224233\n",
      "Iteration 1618, loss = 0.48208338\n",
      "Iteration 1619, loss = 0.48192443\n",
      "Iteration 1620, loss = 0.48176570\n",
      "Iteration 1621, loss = 0.48160716\n",
      "Iteration 1622, loss = 0.48144876\n",
      "Iteration 1623, loss = 0.48129074\n",
      "Iteration 1624, loss = 0.48113268\n",
      "Iteration 1625, loss = 0.48097477\n",
      "Iteration 1626, loss = 0.48081708\n",
      "Iteration 1627, loss = 0.48065948\n",
      "Iteration 1628, loss = 0.48050210\n",
      "Iteration 1629, loss = 0.48034476\n",
      "Iteration 1630, loss = 0.48018770\n",
      "Iteration 1631, loss = 0.48003080\n",
      "Iteration 1632, loss = 0.47987393\n",
      "Iteration 1633, loss = 0.47971731\n",
      "Iteration 1634, loss = 0.47956089\n",
      "Iteration 1635, loss = 0.47940446\n",
      "Iteration 1636, loss = 0.47924831\n",
      "Iteration 1637, loss = 0.47909228\n",
      "Iteration 1638, loss = 0.47893634\n",
      "Iteration 1639, loss = 0.47878066\n",
      "Iteration 1640, loss = 0.47862506\n",
      "Iteration 1641, loss = 0.47846969\n",
      "Iteration 1642, loss = 0.47831452\n",
      "Iteration 1643, loss = 0.47815951\n",
      "Iteration 1644, loss = 0.47800473\n",
      "Iteration 1645, loss = 0.47785005\n",
      "Iteration 1646, loss = 0.47769529\n",
      "Iteration 1647, loss = 0.47754073\n",
      "Iteration 1648, loss = 0.47738650\n",
      "Iteration 1649, loss = 0.47723242\n",
      "Iteration 1650, loss = 0.47707835\n",
      "Iteration 1651, loss = 0.47692443\n",
      "Iteration 1652, loss = 0.47677069\n",
      "Iteration 1653, loss = 0.47661703\n",
      "Iteration 1654, loss = 0.47646342\n",
      "Iteration 1655, loss = 0.47631002\n",
      "Iteration 1656, loss = 0.47615673\n",
      "Iteration 1657, loss = 0.47600358\n",
      "Iteration 1658, loss = 0.47585059\n",
      "Iteration 1659, loss = 0.47569788\n",
      "Iteration 1660, loss = 0.47554519\n",
      "Iteration 1661, loss = 0.47539257\n",
      "Iteration 1662, loss = 0.47524021\n",
      "Iteration 1663, loss = 0.47508796\n",
      "Iteration 1664, loss = 0.47493658\n",
      "Iteration 1665, loss = 0.47478544\n",
      "Iteration 1666, loss = 0.47463451\n",
      "Iteration 1667, loss = 0.47448377\n",
      "Iteration 1668, loss = 0.47433306\n",
      "Iteration 1669, loss = 0.47418250\n",
      "Iteration 1670, loss = 0.47403207\n",
      "Iteration 1671, loss = 0.47388190\n",
      "Iteration 1672, loss = 0.47373177\n",
      "Iteration 1673, loss = 0.47358179\n",
      "Iteration 1674, loss = 0.47343202\n",
      "Iteration 1675, loss = 0.47328232\n",
      "Iteration 1676, loss = 0.47313290\n",
      "Iteration 1677, loss = 0.47298351\n",
      "Iteration 1678, loss = 0.47283433\n",
      "Iteration 1679, loss = 0.47268522\n",
      "Iteration 1680, loss = 0.47253621\n",
      "Iteration 1681, loss = 0.47238747\n",
      "Iteration 1682, loss = 0.47223878\n",
      "Iteration 1683, loss = 0.47209010\n",
      "Iteration 1684, loss = 0.47194155\n",
      "Iteration 1685, loss = 0.47179329\n",
      "Iteration 1686, loss = 0.47164518\n",
      "Iteration 1687, loss = 0.47149724\n",
      "Iteration 1688, loss = 0.47134929\n",
      "Iteration 1689, loss = 0.47120142\n",
      "Iteration 1690, loss = 0.47105361\n",
      "Iteration 1691, loss = 0.47090588\n",
      "Iteration 1692, loss = 0.47075824\n",
      "Iteration 1693, loss = 0.47061060\n",
      "Iteration 1694, loss = 0.47046309\n",
      "Iteration 1695, loss = 0.47031579\n",
      "Iteration 1696, loss = 0.47016842\n",
      "Iteration 1697, loss = 0.47002119\n",
      "Iteration 1698, loss = 0.46987426\n",
      "Iteration 1699, loss = 0.46972739\n",
      "Iteration 1700, loss = 0.46958071\n",
      "Iteration 1701, loss = 0.46943413\n",
      "Iteration 1702, loss = 0.46928763\n",
      "Iteration 1703, loss = 0.46914118\n",
      "Iteration 1704, loss = 0.46899508\n",
      "Iteration 1705, loss = 0.46884903\n",
      "Iteration 1706, loss = 0.46870299\n",
      "Iteration 1707, loss = 0.46855719\n",
      "Iteration 1708, loss = 0.46841153\n",
      "Iteration 1709, loss = 0.46826606\n",
      "Iteration 1710, loss = 0.46812069\n",
      "Iteration 1711, loss = 0.46797569\n",
      "Iteration 1712, loss = 0.46783063\n",
      "Iteration 1713, loss = 0.46768554\n",
      "Iteration 1714, loss = 0.46754082\n",
      "Iteration 1715, loss = 0.46739619\n",
      "Iteration 1716, loss = 0.46725170\n",
      "Iteration 1717, loss = 0.46710726\n",
      "Iteration 1718, loss = 0.46696307\n",
      "Iteration 1719, loss = 0.46681888\n",
      "Iteration 1720, loss = 0.46667487\n",
      "Iteration 1721, loss = 0.46653100\n",
      "Iteration 1722, loss = 0.46638730\n",
      "Iteration 1723, loss = 0.46624367\n",
      "Iteration 1724, loss = 0.46610007\n",
      "Iteration 1725, loss = 0.46595690\n",
      "Iteration 1726, loss = 0.46581374\n",
      "Iteration 1727, loss = 0.46567059\n",
      "Iteration 1728, loss = 0.46552761\n",
      "Iteration 1729, loss = 0.46538481\n",
      "Iteration 1730, loss = 0.46524217\n",
      "Iteration 1731, loss = 0.46509958\n",
      "Iteration 1732, loss = 0.46495692\n",
      "Iteration 1733, loss = 0.46481418\n",
      "Iteration 1734, loss = 0.46467155\n",
      "Iteration 1735, loss = 0.46452888\n",
      "Iteration 1736, loss = 0.46438632\n",
      "Iteration 1737, loss = 0.46424367\n",
      "Iteration 1738, loss = 0.46410111\n",
      "Iteration 1739, loss = 0.46395852\n",
      "Iteration 1740, loss = 0.46381612\n",
      "Iteration 1741, loss = 0.46367367\n",
      "Iteration 1742, loss = 0.46353123\n",
      "Iteration 1743, loss = 0.46338887\n",
      "Iteration 1744, loss = 0.46324679\n",
      "Iteration 1745, loss = 0.46310468\n",
      "Iteration 1746, loss = 0.46296239\n",
      "Iteration 1747, loss = 0.46282040\n",
      "Iteration 1748, loss = 0.46267862\n",
      "Iteration 1749, loss = 0.46253685\n",
      "Iteration 1750, loss = 0.46239603\n",
      "Iteration 1751, loss = 0.46225446\n",
      "Iteration 1752, loss = 0.46211254\n",
      "Iteration 1753, loss = 0.46197071\n",
      "Iteration 1754, loss = 0.46182887\n",
      "Iteration 1755, loss = 0.46168727\n",
      "Iteration 1756, loss = 0.46154683\n",
      "Iteration 1757, loss = 0.46140645\n",
      "Iteration 1758, loss = 0.46126612\n",
      "Iteration 1759, loss = 0.46112670\n",
      "Iteration 1760, loss = 0.46098723\n",
      "Iteration 1761, loss = 0.46084796\n",
      "Iteration 1762, loss = 0.46070879\n",
      "Iteration 1763, loss = 0.46056975\n",
      "Iteration 1764, loss = 0.46043137\n",
      "Iteration 1765, loss = 0.46029357\n",
      "Iteration 1766, loss = 0.46015591\n",
      "Iteration 1767, loss = 0.46001837\n",
      "Iteration 1768, loss = 0.45988095\n",
      "Iteration 1769, loss = 0.45974372\n",
      "Iteration 1770, loss = 0.45960678\n",
      "Iteration 1771, loss = 0.45947035\n",
      "Iteration 1772, loss = 0.45933355\n",
      "Iteration 1773, loss = 0.45919700\n",
      "Iteration 1774, loss = 0.45906068\n",
      "Iteration 1775, loss = 0.45892463\n",
      "Iteration 1776, loss = 0.45878855\n",
      "Iteration 1777, loss = 0.45865260\n",
      "Iteration 1778, loss = 0.45851677\n",
      "Iteration 1779, loss = 0.45838102\n",
      "Iteration 1780, loss = 0.45824531\n",
      "Iteration 1781, loss = 0.45810980\n",
      "Iteration 1782, loss = 0.45797441\n",
      "Iteration 1783, loss = 0.45783902\n",
      "Iteration 1784, loss = 0.45770379\n",
      "Iteration 1785, loss = 0.45756888\n",
      "Iteration 1786, loss = 0.45743394\n",
      "Iteration 1787, loss = 0.45729901\n",
      "Iteration 1788, loss = 0.45716425\n",
      "Iteration 1789, loss = 0.45702973\n",
      "Iteration 1790, loss = 0.45689535\n",
      "Iteration 1791, loss = 0.45676106\n",
      "Iteration 1792, loss = 0.45662673\n",
      "Iteration 1793, loss = 0.45649271\n",
      "Iteration 1794, loss = 0.45635881\n",
      "Iteration 1795, loss = 0.45622505\n",
      "Iteration 1796, loss = 0.45609128\n",
      "Iteration 1797, loss = 0.45595754\n",
      "Iteration 1798, loss = 0.45582412\n",
      "Iteration 1799, loss = 0.45569072\n",
      "Iteration 1800, loss = 0.45555731\n",
      "Iteration 1801, loss = 0.45542396\n",
      "Iteration 1802, loss = 0.45529092\n",
      "Iteration 1803, loss = 0.45515795\n",
      "Iteration 1804, loss = 0.45502503\n",
      "Iteration 1805, loss = 0.45489235\n",
      "Iteration 1806, loss = 0.45475975\n",
      "Iteration 1807, loss = 0.45462726\n",
      "Iteration 1808, loss = 0.45449499\n",
      "Iteration 1809, loss = 0.45436269\n",
      "Iteration 1810, loss = 0.45423053\n",
      "Iteration 1811, loss = 0.45409842\n",
      "Iteration 1812, loss = 0.45396651\n",
      "Iteration 1813, loss = 0.45383453\n",
      "Iteration 1814, loss = 0.45370275\n",
      "Iteration 1815, loss = 0.45357123\n",
      "Iteration 1816, loss = 0.45343969\n",
      "Iteration 1817, loss = 0.45330824\n",
      "Iteration 1818, loss = 0.45317695\n",
      "Iteration 1819, loss = 0.45304572\n",
      "Iteration 1820, loss = 0.45291462\n",
      "Iteration 1821, loss = 0.45278368\n",
      "Iteration 1822, loss = 0.45265282\n",
      "Iteration 1823, loss = 0.45252221\n",
      "Iteration 1824, loss = 0.45239168\n",
      "Iteration 1825, loss = 0.45226108\n",
      "Iteration 1826, loss = 0.45213066\n",
      "Iteration 1827, loss = 0.45200032\n",
      "Iteration 1828, loss = 0.45187026\n",
      "Iteration 1829, loss = 0.45174029\n",
      "Iteration 1830, loss = 0.45161038\n",
      "Iteration 1831, loss = 0.45148067\n",
      "Iteration 1832, loss = 0.45135110\n",
      "Iteration 1833, loss = 0.45122157\n",
      "Iteration 1834, loss = 0.45109218\n",
      "Iteration 1835, loss = 0.45096307\n",
      "Iteration 1836, loss = 0.45083396\n",
      "Iteration 1837, loss = 0.45070505\n",
      "Iteration 1838, loss = 0.45057615\n",
      "Iteration 1839, loss = 0.45044741\n",
      "Iteration 1840, loss = 0.45031882\n",
      "Iteration 1841, loss = 0.45019018\n",
      "Iteration 1842, loss = 0.45006161\n",
      "Iteration 1843, loss = 0.44993297\n",
      "Iteration 1844, loss = 0.44980469\n",
      "Iteration 1845, loss = 0.44967654\n",
      "Iteration 1846, loss = 0.44954852\n",
      "Iteration 1847, loss = 0.44942062\n",
      "Iteration 1848, loss = 0.44929285\n",
      "Iteration 1849, loss = 0.44916522\n",
      "Iteration 1850, loss = 0.44903776\n",
      "Iteration 1851, loss = 0.44891022\n",
      "Iteration 1852, loss = 0.44878288\n",
      "Iteration 1853, loss = 0.44865566\n",
      "Iteration 1854, loss = 0.44852852\n",
      "Iteration 1855, loss = 0.44840147\n",
      "Iteration 1856, loss = 0.44827455\n",
      "Iteration 1857, loss = 0.44814769\n",
      "Iteration 1858, loss = 0.44802096\n",
      "Iteration 1859, loss = 0.44789438\n",
      "Iteration 1860, loss = 0.44776785\n",
      "Iteration 1861, loss = 0.44764152\n",
      "Iteration 1862, loss = 0.44751517\n",
      "Iteration 1863, loss = 0.44738907\n",
      "Iteration 1864, loss = 0.44726319\n",
      "Iteration 1865, loss = 0.44713726\n",
      "Iteration 1866, loss = 0.44701155\n",
      "Iteration 1867, loss = 0.44688597\n",
      "Iteration 1868, loss = 0.44676044\n",
      "Iteration 1869, loss = 0.44663500\n",
      "Iteration 1870, loss = 0.44650971\n",
      "Iteration 1871, loss = 0.44638503\n",
      "Iteration 1872, loss = 0.44626043\n",
      "Iteration 1873, loss = 0.44613593\n",
      "Iteration 1874, loss = 0.44601161\n",
      "Iteration 1875, loss = 0.44588735\n",
      "Iteration 1876, loss = 0.44576310\n",
      "Iteration 1877, loss = 0.44563907\n",
      "Iteration 1878, loss = 0.44551513\n",
      "Iteration 1879, loss = 0.44539125\n",
      "Iteration 1880, loss = 0.44526746\n",
      "Iteration 1881, loss = 0.44514382\n",
      "Iteration 1882, loss = 0.44502029\n",
      "Iteration 1883, loss = 0.44489690\n",
      "Iteration 1884, loss = 0.44477338\n",
      "Iteration 1885, loss = 0.44464998\n",
      "Iteration 1886, loss = 0.44452672\n",
      "Iteration 1887, loss = 0.44440344\n",
      "Iteration 1888, loss = 0.44428016\n",
      "Iteration 1889, loss = 0.44415720\n",
      "Iteration 1890, loss = 0.44403424\n",
      "Iteration 1891, loss = 0.44391150\n",
      "Iteration 1892, loss = 0.44378898\n",
      "Iteration 1893, loss = 0.44366651\n",
      "Iteration 1894, loss = 0.44354393\n",
      "Iteration 1895, loss = 0.44342159\n",
      "Iteration 1896, loss = 0.44329942\n",
      "Iteration 1897, loss = 0.44317724\n",
      "Iteration 1898, loss = 0.44305516\n",
      "Iteration 1899, loss = 0.44293315\n",
      "Iteration 1900, loss = 0.44281141\n",
      "Iteration 1901, loss = 0.44268959\n",
      "Iteration 1902, loss = 0.44256812\n",
      "Iteration 1903, loss = 0.44244662\n",
      "Iteration 1904, loss = 0.44232544\n",
      "Iteration 1905, loss = 0.44220436\n",
      "Iteration 1906, loss = 0.44208336\n",
      "Iteration 1907, loss = 0.44196235\n",
      "Iteration 1908, loss = 0.44184135\n",
      "Iteration 1909, loss = 0.44172065\n",
      "Iteration 1910, loss = 0.44159996\n",
      "Iteration 1911, loss = 0.44147939\n",
      "Iteration 1912, loss = 0.44135909\n",
      "Iteration 1913, loss = 0.44123897\n",
      "Iteration 1914, loss = 0.44111887\n",
      "Iteration 1915, loss = 0.44099882\n",
      "Iteration 1916, loss = 0.44087896\n",
      "Iteration 1917, loss = 0.44075910\n",
      "Iteration 1918, loss = 0.44063932\n",
      "Iteration 1919, loss = 0.44051983\n",
      "Iteration 1920, loss = 0.44040048\n",
      "Iteration 1921, loss = 0.44028106\n",
      "Iteration 1922, loss = 0.44016166\n",
      "Iteration 1923, loss = 0.44004263\n",
      "Iteration 1924, loss = 0.43992364\n",
      "Iteration 1925, loss = 0.43980488\n",
      "Iteration 1926, loss = 0.43968607\n",
      "Iteration 1927, loss = 0.43956717\n",
      "Iteration 1928, loss = 0.43944859\n",
      "Iteration 1929, loss = 0.43933019\n",
      "Iteration 1930, loss = 0.43921185\n",
      "Iteration 1931, loss = 0.43909345\n",
      "Iteration 1932, loss = 0.43897540\n",
      "Iteration 1933, loss = 0.43885749\n",
      "Iteration 1934, loss = 0.43873966\n",
      "Iteration 1935, loss = 0.43862180\n",
      "Iteration 1936, loss = 0.43850411\n",
      "Iteration 1937, loss = 0.43838667\n",
      "Iteration 1938, loss = 0.43826940\n",
      "Iteration 1939, loss = 0.43815225\n",
      "Iteration 1940, loss = 0.43803499\n",
      "Iteration 1941, loss = 0.43791793\n",
      "Iteration 1942, loss = 0.43780109\n",
      "Iteration 1943, loss = 0.43768422\n",
      "Iteration 1944, loss = 0.43756743\n",
      "Iteration 1945, loss = 0.43745091\n",
      "Iteration 1946, loss = 0.43733447\n",
      "Iteration 1947, loss = 0.43721805\n",
      "Iteration 1948, loss = 0.43710162\n",
      "Iteration 1949, loss = 0.43698544\n",
      "Iteration 1950, loss = 0.43686946\n",
      "Iteration 1951, loss = 0.43675350\n",
      "Iteration 1952, loss = 0.43663773\n",
      "Iteration 1953, loss = 0.43652175\n",
      "Iteration 1954, loss = 0.43640610\n",
      "Iteration 1955, loss = 0.43629073\n",
      "Iteration 1956, loss = 0.43617518\n",
      "Iteration 1957, loss = 0.43605991\n",
      "Iteration 1958, loss = 0.43594479\n",
      "Iteration 1959, loss = 0.43582964\n",
      "Iteration 1960, loss = 0.43571465\n",
      "Iteration 1961, loss = 0.43559971\n",
      "Iteration 1962, loss = 0.43548491\n",
      "Iteration 1963, loss = 0.43537017\n",
      "Iteration 1964, loss = 0.43525556\n",
      "Iteration 1965, loss = 0.43514111\n",
      "Iteration 1966, loss = 0.43502676\n",
      "Iteration 1967, loss = 0.43491256\n",
      "Iteration 1968, loss = 0.43479836\n",
      "Iteration 1969, loss = 0.43468431\n",
      "Iteration 1970, loss = 0.43457042\n",
      "Iteration 1971, loss = 0.43445661\n",
      "Iteration 1972, loss = 0.43434277\n",
      "Iteration 1973, loss = 0.43422917\n",
      "Iteration 1974, loss = 0.43411559\n",
      "Iteration 1975, loss = 0.43400212\n",
      "Iteration 1976, loss = 0.43388883\n",
      "Iteration 1977, loss = 0.43377560\n",
      "Iteration 1978, loss = 0.43366247\n",
      "Iteration 1979, loss = 0.43354949\n",
      "Iteration 1980, loss = 0.43343656\n",
      "Iteration 1981, loss = 0.43332363\n",
      "Iteration 1982, loss = 0.43321065\n",
      "Iteration 1983, loss = 0.43309795\n",
      "Iteration 1984, loss = 0.43298517\n",
      "Iteration 1985, loss = 0.43287263\n",
      "Iteration 1986, loss = 0.43276019\n",
      "Iteration 1987, loss = 0.43264778\n",
      "Iteration 1988, loss = 0.43253549\n",
      "Iteration 1989, loss = 0.43242331\n",
      "Iteration 1990, loss = 0.43231126\n",
      "Iteration 1991, loss = 0.43219929\n",
      "Iteration 1992, loss = 0.43208745\n",
      "Iteration 1993, loss = 0.43197562\n",
      "Iteration 1994, loss = 0.43186388\n",
      "Iteration 1995, loss = 0.43175227\n",
      "Iteration 1996, loss = 0.43164068\n",
      "Iteration 1997, loss = 0.43152929\n",
      "Iteration 1998, loss = 0.43141790\n",
      "Iteration 1999, loss = 0.43130662\n",
      "Iteration 2000, loss = 0.43119566\n",
      "Iteration 2001, loss = 0.43108461\n",
      "Iteration 2002, loss = 0.43097371\n",
      "Iteration 2003, loss = 0.43086299\n",
      "Iteration 2004, loss = 0.43075228\n",
      "Iteration 2005, loss = 0.43064171\n",
      "Iteration 2006, loss = 0.43053120\n",
      "Iteration 2007, loss = 0.43042078\n",
      "Iteration 2008, loss = 0.43031056\n",
      "Iteration 2009, loss = 0.43020040\n",
      "Iteration 2010, loss = 0.43009038\n",
      "Iteration 2011, loss = 0.42998044\n",
      "Iteration 2012, loss = 0.42987055\n",
      "Iteration 2013, loss = 0.42976080\n",
      "Iteration 2014, loss = 0.42965102\n",
      "Iteration 2015, loss = 0.42954146\n",
      "Iteration 2016, loss = 0.42943192\n",
      "Iteration 2017, loss = 0.42932255\n",
      "Iteration 2018, loss = 0.42921326\n",
      "Iteration 2019, loss = 0.42910402\n",
      "Iteration 2020, loss = 0.42899509\n",
      "Iteration 2021, loss = 0.42888606\n",
      "Iteration 2022, loss = 0.42877721\n",
      "Iteration 2023, loss = 0.42866850\n",
      "Iteration 2024, loss = 0.42855990\n",
      "Iteration 2025, loss = 0.42845133\n",
      "Iteration 2026, loss = 0.42834290\n",
      "Iteration 2027, loss = 0.42823458\n",
      "Iteration 2028, loss = 0.42812638\n",
      "Iteration 2029, loss = 0.42801834\n",
      "Iteration 2030, loss = 0.42791038\n",
      "Iteration 2031, loss = 0.42780258\n",
      "Iteration 2032, loss = 0.42769479\n",
      "Iteration 2033, loss = 0.42758722\n",
      "Iteration 2034, loss = 0.42747976\n",
      "Iteration 2035, loss = 0.42737243\n",
      "Iteration 2036, loss = 0.42726518\n",
      "Iteration 2037, loss = 0.42715803\n",
      "Iteration 2038, loss = 0.42705103\n",
      "Iteration 2039, loss = 0.42694416\n",
      "Iteration 2040, loss = 0.42683745\n",
      "Iteration 2041, loss = 0.42673084\n",
      "Iteration 2042, loss = 0.42662436\n",
      "Iteration 2043, loss = 0.42651803\n",
      "Iteration 2044, loss = 0.42641174\n",
      "Iteration 2045, loss = 0.42630556\n",
      "Iteration 2046, loss = 0.42619953\n",
      "Iteration 2047, loss = 0.42609357\n",
      "Iteration 2048, loss = 0.42598778\n",
      "Iteration 2049, loss = 0.42588211\n",
      "Iteration 2050, loss = 0.42577636\n",
      "Iteration 2051, loss = 0.42567096\n",
      "Iteration 2052, loss = 0.42556560\n",
      "Iteration 2053, loss = 0.42546028\n",
      "Iteration 2054, loss = 0.42535503\n",
      "Iteration 2055, loss = 0.42525002\n",
      "Iteration 2056, loss = 0.42514505\n",
      "Iteration 2057, loss = 0.42504021\n",
      "Iteration 2058, loss = 0.42493548\n",
      "Iteration 2059, loss = 0.42483086\n",
      "Iteration 2060, loss = 0.42472640\n",
      "Iteration 2061, loss = 0.42462199\n",
      "Iteration 2062, loss = 0.42451759\n",
      "Iteration 2063, loss = 0.42441303\n",
      "Iteration 2064, loss = 0.42430846\n",
      "Iteration 2065, loss = 0.42420376\n",
      "Iteration 2066, loss = 0.42409906\n",
      "Iteration 2067, loss = 0.42399447\n",
      "Iteration 2068, loss = 0.42388992\n",
      "Iteration 2069, loss = 0.42378526\n",
      "Iteration 2070, loss = 0.42368071\n",
      "Iteration 2071, loss = 0.42357631\n",
      "Iteration 2072, loss = 0.42347182\n",
      "Iteration 2073, loss = 0.42336730\n",
      "Iteration 2074, loss = 0.42326295\n",
      "Iteration 2075, loss = 0.42315865\n",
      "Iteration 2076, loss = 0.42305449\n",
      "Iteration 2077, loss = 0.42295033\n",
      "Iteration 2078, loss = 0.42284631\n",
      "Iteration 2079, loss = 0.42274233\n",
      "Iteration 2080, loss = 0.42263843\n",
      "Iteration 2081, loss = 0.42253475\n",
      "Iteration 2082, loss = 0.42243111\n",
      "Iteration 2083, loss = 0.42232759\n",
      "Iteration 2084, loss = 0.42222432\n",
      "Iteration 2085, loss = 0.42212108\n",
      "Iteration 2086, loss = 0.42201782\n",
      "Iteration 2087, loss = 0.42191369\n",
      "Iteration 2088, loss = 0.42180967\n",
      "Iteration 2089, loss = 0.42170395\n",
      "Iteration 2090, loss = 0.42159776\n",
      "Iteration 2091, loss = 0.42149144\n",
      "Iteration 2092, loss = 0.42138606\n",
      "Iteration 2093, loss = 0.42128073\n",
      "Iteration 2094, loss = 0.42117535\n",
      "Iteration 2095, loss = 0.42106910\n",
      "Iteration 2096, loss = 0.42096252\n",
      "Iteration 2097, loss = 0.42085683\n",
      "Iteration 2098, loss = 0.42075301\n",
      "Iteration 2099, loss = 0.42064985\n",
      "Iteration 2100, loss = 0.42054881\n",
      "Iteration 2101, loss = 0.42044858\n",
      "Iteration 2102, loss = 0.42034837\n",
      "Iteration 2103, loss = 0.42024837\n",
      "Iteration 2104, loss = 0.42014862\n",
      "Iteration 2105, loss = 0.42004895\n",
      "Iteration 2106, loss = 0.41994929\n",
      "Iteration 2107, loss = 0.41984946\n",
      "Iteration 2108, loss = 0.41974950\n",
      "Iteration 2109, loss = 0.41964982\n",
      "Iteration 2110, loss = 0.41955027\n",
      "Iteration 2111, loss = 0.41945090\n",
      "Iteration 2112, loss = 0.41935189\n",
      "Iteration 2113, loss = 0.41925307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36352952\n",
      "Iteration 2, loss = 1.36134051\n",
      "Iteration 3, loss = 1.35916474\n",
      "Iteration 4, loss = 1.35699710\n",
      "Iteration 5, loss = 1.35483783\n",
      "Iteration 6, loss = 1.35268191\n",
      "Iteration 7, loss = 1.35053579\n",
      "Iteration 8, loss = 1.34840059\n",
      "Iteration 9, loss = 1.34627552\n",
      "Iteration 10, loss = 1.34416429\n",
      "Iteration 11, loss = 1.34209003\n",
      "Iteration 12, loss = 1.34002560\n",
      "Iteration 13, loss = 1.33795699\n",
      "Iteration 14, loss = 1.33589668\n",
      "Iteration 15, loss = 1.33384555\n",
      "Iteration 16, loss = 1.33179981\n",
      "Iteration 17, loss = 1.32976365\n",
      "Iteration 18, loss = 1.32773031\n",
      "Iteration 19, loss = 1.32570612\n",
      "Iteration 20, loss = 1.32369537\n",
      "Iteration 21, loss = 1.32169508\n",
      "Iteration 22, loss = 1.31970539\n",
      "Iteration 23, loss = 1.31772870\n",
      "Iteration 24, loss = 1.31576080\n",
      "Iteration 25, loss = 1.31379766\n",
      "Iteration 26, loss = 1.31184621\n",
      "Iteration 27, loss = 1.30990912\n",
      "Iteration 28, loss = 1.30797917\n",
      "Iteration 29, loss = 1.30606039\n",
      "Iteration 30, loss = 1.30415347\n",
      "Iteration 31, loss = 1.30225353\n",
      "Iteration 32, loss = 1.30034823\n",
      "Iteration 33, loss = 1.29845216\n",
      "Iteration 34, loss = 1.29656877\n",
      "Iteration 35, loss = 1.29470235\n",
      "Iteration 36, loss = 1.29285333\n",
      "Iteration 37, loss = 1.29101834\n",
      "Iteration 38, loss = 1.28920284\n",
      "Iteration 39, loss = 1.28739751\n",
      "Iteration 40, loss = 1.28560167\n",
      "Iteration 41, loss = 1.28381529\n",
      "Iteration 42, loss = 1.28203848\n",
      "Iteration 43, loss = 1.28027413\n",
      "Iteration 44, loss = 1.27852849\n",
      "Iteration 45, loss = 1.27679383\n",
      "Iteration 46, loss = 1.27506829\n",
      "Iteration 47, loss = 1.27335244\n",
      "Iteration 48, loss = 1.27165257\n",
      "Iteration 49, loss = 1.26995679\n",
      "Iteration 50, loss = 1.26826888\n",
      "Iteration 51, loss = 1.26659091\n",
      "Iteration 52, loss = 1.26491982\n",
      "Iteration 53, loss = 1.26325172\n",
      "Iteration 54, loss = 1.26159019\n",
      "Iteration 55, loss = 1.25994966\n",
      "Iteration 56, loss = 1.25831701\n",
      "Iteration 57, loss = 1.25668169\n",
      "Iteration 58, loss = 1.25504380\n",
      "Iteration 59, loss = 1.25341950\n",
      "Iteration 60, loss = 1.25180072\n",
      "Iteration 61, loss = 1.25018870\n",
      "Iteration 62, loss = 1.24858310\n",
      "Iteration 63, loss = 1.24698354\n",
      "Iteration 64, loss = 1.24539274\n",
      "Iteration 65, loss = 1.24382457\n",
      "Iteration 66, loss = 1.24226851\n",
      "Iteration 67, loss = 1.24071924\n",
      "Iteration 68, loss = 1.23917664\n",
      "Iteration 69, loss = 1.23764089\n",
      "Iteration 70, loss = 1.23611705\n",
      "Iteration 71, loss = 1.23460292\n",
      "Iteration 72, loss = 1.23309566\n",
      "Iteration 73, loss = 1.23158380\n",
      "Iteration 74, loss = 1.23007666\n",
      "Iteration 75, loss = 1.22858012\n",
      "Iteration 76, loss = 1.22710207\n",
      "Iteration 77, loss = 1.22562874\n",
      "Iteration 78, loss = 1.22416272\n",
      "Iteration 79, loss = 1.22270465\n",
      "Iteration 80, loss = 1.22125352\n",
      "Iteration 81, loss = 1.21981364\n",
      "Iteration 82, loss = 1.21838165\n",
      "Iteration 83, loss = 1.21695604\n",
      "Iteration 84, loss = 1.21553477\n",
      "Iteration 85, loss = 1.21411889\n",
      "Iteration 86, loss = 1.21271289\n",
      "Iteration 87, loss = 1.21132266\n",
      "Iteration 88, loss = 1.20993936\n",
      "Iteration 89, loss = 1.20856225\n",
      "Iteration 90, loss = 1.20719140\n",
      "Iteration 91, loss = 1.20582807\n",
      "Iteration 92, loss = 1.20446656\n",
      "Iteration 93, loss = 1.20311103\n",
      "Iteration 94, loss = 1.20176323\n",
      "Iteration 95, loss = 1.20042169\n",
      "Iteration 96, loss = 1.19909261\n",
      "Iteration 97, loss = 1.19776924\n",
      "Iteration 98, loss = 1.19645055\n",
      "Iteration 99, loss = 1.19514196\n",
      "Iteration 100, loss = 1.19384027\n",
      "Iteration 101, loss = 1.19254160\n",
      "Iteration 102, loss = 1.19125099\n",
      "Iteration 103, loss = 1.18996336\n",
      "Iteration 104, loss = 1.18869146\n",
      "Iteration 105, loss = 1.18742916\n",
      "Iteration 106, loss = 1.18617092\n",
      "Iteration 107, loss = 1.18491734\n",
      "Iteration 108, loss = 1.18366819\n",
      "Iteration 109, loss = 1.18242402\n",
      "Iteration 110, loss = 1.18118932\n",
      "Iteration 111, loss = 1.17996343\n",
      "Iteration 112, loss = 1.17874280\n",
      "Iteration 113, loss = 1.17752629\n",
      "Iteration 114, loss = 1.17631671\n",
      "Iteration 115, loss = 1.17511006\n",
      "Iteration 116, loss = 1.17390501\n",
      "Iteration 117, loss = 1.17270865\n",
      "Iteration 118, loss = 1.17153105\n",
      "Iteration 119, loss = 1.17035832\n",
      "Iteration 120, loss = 1.16918783\n",
      "Iteration 121, loss = 1.16802351\n",
      "Iteration 122, loss = 1.16686240\n",
      "Iteration 123, loss = 1.16570396\n",
      "Iteration 124, loss = 1.16454998\n",
      "Iteration 125, loss = 1.16340303\n",
      "Iteration 126, loss = 1.16226342\n",
      "Iteration 127, loss = 1.16112618\n",
      "Iteration 128, loss = 1.15998949\n",
      "Iteration 129, loss = 1.15885244\n",
      "Iteration 130, loss = 1.15771903\n",
      "Iteration 131, loss = 1.15657855\n",
      "Iteration 132, loss = 1.15543830\n",
      "Iteration 133, loss = 1.15429820\n",
      "Iteration 134, loss = 1.15315929\n",
      "Iteration 135, loss = 1.15202321\n",
      "Iteration 136, loss = 1.15088769\n",
      "Iteration 137, loss = 1.14975338\n",
      "Iteration 138, loss = 1.14862107\n",
      "Iteration 139, loss = 1.14748464\n",
      "Iteration 140, loss = 1.14634536\n",
      "Iteration 141, loss = 1.14520667\n",
      "Iteration 142, loss = 1.14407053\n",
      "Iteration 143, loss = 1.14293894\n",
      "Iteration 144, loss = 1.14181217\n",
      "Iteration 145, loss = 1.14068827\n",
      "Iteration 146, loss = 1.13956242\n",
      "Iteration 147, loss = 1.13843700\n",
      "Iteration 148, loss = 1.13731229\n",
      "Iteration 149, loss = 1.13619295\n",
      "Iteration 150, loss = 1.13507591\n",
      "Iteration 151, loss = 1.13395906\n",
      "Iteration 152, loss = 1.13285128\n",
      "Iteration 153, loss = 1.13174161\n",
      "Iteration 154, loss = 1.13063149\n",
      "Iteration 155, loss = 1.12952551\n",
      "Iteration 156, loss = 1.12842102\n",
      "Iteration 157, loss = 1.12731415\n",
      "Iteration 158, loss = 1.12620780\n",
      "Iteration 159, loss = 1.12510218\n",
      "Iteration 160, loss = 1.12400549\n",
      "Iteration 161, loss = 1.12290983\n",
      "Iteration 162, loss = 1.12181588\n",
      "Iteration 163, loss = 1.12072395\n",
      "Iteration 164, loss = 1.11963404\n",
      "Iteration 165, loss = 1.11854392\n",
      "Iteration 166, loss = 1.11745678\n",
      "Iteration 167, loss = 1.11637528\n",
      "Iteration 168, loss = 1.11529386\n",
      "Iteration 169, loss = 1.11421443\n",
      "Iteration 170, loss = 1.11314072\n",
      "Iteration 171, loss = 1.11207721\n",
      "Iteration 172, loss = 1.11101505\n",
      "Iteration 173, loss = 1.10995553\n",
      "Iteration 174, loss = 1.10889299\n",
      "Iteration 175, loss = 1.10782638\n",
      "Iteration 176, loss = 1.10675945\n",
      "Iteration 177, loss = 1.10569346\n",
      "Iteration 178, loss = 1.10463227\n",
      "Iteration 179, loss = 1.10357340\n",
      "Iteration 180, loss = 1.10251728\n",
      "Iteration 181, loss = 1.10146117\n",
      "Iteration 182, loss = 1.10040689\n",
      "Iteration 183, loss = 1.09935893\n",
      "Iteration 184, loss = 1.09831425\n",
      "Iteration 185, loss = 1.09727429\n",
      "Iteration 186, loss = 1.09623403\n",
      "Iteration 187, loss = 1.09520209\n",
      "Iteration 188, loss = 1.09416704\n",
      "Iteration 189, loss = 1.09313086\n",
      "Iteration 190, loss = 1.09209836\n",
      "Iteration 191, loss = 1.09106818\n",
      "Iteration 192, loss = 1.09004181\n",
      "Iteration 193, loss = 1.08901455\n",
      "Iteration 194, loss = 1.08798820\n",
      "Iteration 195, loss = 1.08696564\n",
      "Iteration 196, loss = 1.08594396\n",
      "Iteration 197, loss = 1.08492276\n",
      "Iteration 198, loss = 1.08390134\n",
      "Iteration 199, loss = 1.08288341\n",
      "Iteration 200, loss = 1.08186567\n",
      "Iteration 201, loss = 1.08084790\n",
      "Iteration 202, loss = 1.07983289\n",
      "Iteration 203, loss = 1.07881792\n",
      "Iteration 204, loss = 1.07780364\n",
      "Iteration 205, loss = 1.07679995\n",
      "Iteration 206, loss = 1.07579904\n",
      "Iteration 207, loss = 1.07479981\n",
      "Iteration 208, loss = 1.07380124\n",
      "Iteration 209, loss = 1.07280837\n",
      "Iteration 210, loss = 1.07181477\n",
      "Iteration 211, loss = 1.07082450\n",
      "Iteration 212, loss = 1.06983693\n",
      "Iteration 213, loss = 1.06885044\n",
      "Iteration 214, loss = 1.06786687\n",
      "Iteration 215, loss = 1.06688657\n",
      "Iteration 216, loss = 1.06590987\n",
      "Iteration 217, loss = 1.06493330\n",
      "Iteration 218, loss = 1.06395472\n",
      "Iteration 219, loss = 1.06297673\n",
      "Iteration 220, loss = 1.06200136\n",
      "Iteration 221, loss = 1.06103095\n",
      "Iteration 222, loss = 1.06006555\n",
      "Iteration 223, loss = 1.05910061\n",
      "Iteration 224, loss = 1.05813409\n",
      "Iteration 225, loss = 1.05716843\n",
      "Iteration 226, loss = 1.05620426\n",
      "Iteration 227, loss = 1.05524107\n",
      "Iteration 228, loss = 1.05427969\n",
      "Iteration 229, loss = 1.05332054\n",
      "Iteration 230, loss = 1.05236349\n",
      "Iteration 231, loss = 1.05140794\n",
      "Iteration 232, loss = 1.05046014\n",
      "Iteration 233, loss = 1.04951527\n",
      "Iteration 234, loss = 1.04857262\n",
      "Iteration 235, loss = 1.04762515\n",
      "Iteration 236, loss = 1.04667843\n",
      "Iteration 237, loss = 1.04573284\n",
      "Iteration 238, loss = 1.04478909\n",
      "Iteration 239, loss = 1.04384391\n",
      "Iteration 240, loss = 1.04290128\n",
      "Iteration 241, loss = 1.04196013\n",
      "Iteration 242, loss = 1.04101746\n",
      "Iteration 243, loss = 1.04007572\n",
      "Iteration 244, loss = 1.03913504\n",
      "Iteration 245, loss = 1.03819955\n",
      "Iteration 246, loss = 1.03725943\n",
      "Iteration 247, loss = 1.03631834\n",
      "Iteration 248, loss = 1.03538081\n",
      "Iteration 249, loss = 1.03444550\n",
      "Iteration 250, loss = 1.03351413\n",
      "Iteration 251, loss = 1.03258609\n",
      "Iteration 252, loss = 1.03166229\n",
      "Iteration 253, loss = 1.03073822\n",
      "Iteration 254, loss = 1.02981519\n",
      "Iteration 255, loss = 1.02889327\n",
      "Iteration 256, loss = 1.02797215\n",
      "Iteration 257, loss = 1.02705011\n",
      "Iteration 258, loss = 1.02613015\n",
      "Iteration 259, loss = 1.02521232\n",
      "Iteration 260, loss = 1.02429247\n",
      "Iteration 261, loss = 1.02337201\n",
      "Iteration 262, loss = 1.02245283\n",
      "Iteration 263, loss = 1.02153724\n",
      "Iteration 264, loss = 1.02062202\n",
      "Iteration 265, loss = 1.01970914\n",
      "Iteration 266, loss = 1.01879944\n",
      "Iteration 267, loss = 1.01788949\n",
      "Iteration 268, loss = 1.01698154\n",
      "Iteration 269, loss = 1.01607463\n",
      "Iteration 270, loss = 1.01517161\n",
      "Iteration 271, loss = 1.01426857\n",
      "Iteration 272, loss = 1.01336662\n",
      "Iteration 273, loss = 1.01246703\n",
      "Iteration 274, loss = 1.01157283\n",
      "Iteration 275, loss = 1.01067968\n",
      "Iteration 276, loss = 1.00978939\n",
      "Iteration 277, loss = 1.00890109\n",
      "Iteration 278, loss = 1.00801438\n",
      "Iteration 279, loss = 1.00712949\n",
      "Iteration 280, loss = 1.00624400\n",
      "Iteration 281, loss = 1.00535825\n",
      "Iteration 282, loss = 1.00447284\n",
      "Iteration 283, loss = 1.00359189\n",
      "Iteration 284, loss = 1.00271195\n",
      "Iteration 285, loss = 1.00183304\n",
      "Iteration 286, loss = 1.00095612\n",
      "Iteration 287, loss = 1.00008025\n",
      "Iteration 288, loss = 0.99920751\n",
      "Iteration 289, loss = 0.99833650\n",
      "Iteration 290, loss = 0.99746744\n",
      "Iteration 291, loss = 0.99659573\n",
      "Iteration 292, loss = 0.99571986\n",
      "Iteration 293, loss = 0.99484108\n",
      "Iteration 294, loss = 0.99396383\n",
      "Iteration 295, loss = 0.99308749\n",
      "Iteration 296, loss = 0.99221187\n",
      "Iteration 297, loss = 0.99133439\n",
      "Iteration 298, loss = 0.99045491\n",
      "Iteration 299, loss = 0.98957632\n",
      "Iteration 300, loss = 0.98869459\n",
      "Iteration 301, loss = 0.98781376\n",
      "Iteration 302, loss = 0.98693246\n",
      "Iteration 303, loss = 0.98605110\n",
      "Iteration 304, loss = 0.98517295\n",
      "Iteration 305, loss = 0.98429382\n",
      "Iteration 306, loss = 0.98341748\n",
      "Iteration 307, loss = 0.98254252\n",
      "Iteration 308, loss = 0.98166817\n",
      "Iteration 309, loss = 0.98079721\n",
      "Iteration 310, loss = 0.97992544\n",
      "Iteration 311, loss = 0.97905770\n",
      "Iteration 312, loss = 0.97818898\n",
      "Iteration 313, loss = 0.97732177\n",
      "Iteration 314, loss = 0.97645678\n",
      "Iteration 315, loss = 0.97559411\n",
      "Iteration 316, loss = 0.97473256\n",
      "Iteration 317, loss = 0.97387205\n",
      "Iteration 318, loss = 0.97301253\n",
      "Iteration 319, loss = 0.97215205\n",
      "Iteration 320, loss = 0.97128990\n",
      "Iteration 321, loss = 0.97042734\n",
      "Iteration 322, loss = 0.96956221\n",
      "Iteration 323, loss = 0.96869937\n",
      "Iteration 324, loss = 0.96784097\n",
      "Iteration 325, loss = 0.96698190\n",
      "Iteration 326, loss = 0.96612280\n",
      "Iteration 327, loss = 0.96526636\n",
      "Iteration 328, loss = 0.96441128\n",
      "Iteration 329, loss = 0.96356906\n",
      "Iteration 330, loss = 0.96272589\n",
      "Iteration 331, loss = 0.96188499\n",
      "Iteration 332, loss = 0.96104551\n",
      "Iteration 333, loss = 0.96020531\n",
      "Iteration 334, loss = 0.95936751\n",
      "Iteration 335, loss = 0.95852907\n",
      "Iteration 336, loss = 0.95769265\n",
      "Iteration 337, loss = 0.95685844\n",
      "Iteration 338, loss = 0.95602474\n",
      "Iteration 339, loss = 0.95519130\n",
      "Iteration 340, loss = 0.95436343\n",
      "Iteration 341, loss = 0.95353912\n",
      "Iteration 342, loss = 0.95271667\n",
      "Iteration 343, loss = 0.95189415\n",
      "Iteration 344, loss = 0.95107447\n",
      "Iteration 345, loss = 0.95025615\n",
      "Iteration 346, loss = 0.94943938\n",
      "Iteration 347, loss = 0.94862301\n",
      "Iteration 348, loss = 0.94781285\n",
      "Iteration 349, loss = 0.94700597\n",
      "Iteration 350, loss = 0.94620200\n",
      "Iteration 351, loss = 0.94539710\n",
      "Iteration 352, loss = 0.94459332\n",
      "Iteration 353, loss = 0.94379106\n",
      "Iteration 354, loss = 0.94299156\n",
      "Iteration 355, loss = 0.94219069\n",
      "Iteration 356, loss = 0.94139146\n",
      "Iteration 357, loss = 0.94059746\n",
      "Iteration 358, loss = 0.93980163\n",
      "Iteration 359, loss = 0.93900713\n",
      "Iteration 360, loss = 0.93821482\n",
      "Iteration 361, loss = 0.93742295\n",
      "Iteration 362, loss = 0.93662497\n",
      "Iteration 363, loss = 0.93582577\n",
      "Iteration 364, loss = 0.93502662\n",
      "Iteration 365, loss = 0.93422890\n",
      "Iteration 366, loss = 0.93343462\n",
      "Iteration 367, loss = 0.93264205\n",
      "Iteration 368, loss = 0.93185103\n",
      "Iteration 369, loss = 0.93105943\n",
      "Iteration 370, loss = 0.93026940\n",
      "Iteration 371, loss = 0.92948107\n",
      "Iteration 372, loss = 0.92869374\n",
      "Iteration 373, loss = 0.92790559\n",
      "Iteration 374, loss = 0.92711822\n",
      "Iteration 375, loss = 0.92633476\n",
      "Iteration 376, loss = 0.92555935\n",
      "Iteration 377, loss = 0.92478449\n",
      "Iteration 378, loss = 0.92401056\n",
      "Iteration 379, loss = 0.92323870\n",
      "Iteration 380, loss = 0.92246803\n",
      "Iteration 381, loss = 0.92169966\n",
      "Iteration 382, loss = 0.92093073\n",
      "Iteration 383, loss = 0.92016276\n",
      "Iteration 384, loss = 0.91939908\n",
      "Iteration 385, loss = 0.91863611\n",
      "Iteration 386, loss = 0.91787563\n",
      "Iteration 387, loss = 0.91712094\n",
      "Iteration 388, loss = 0.91636685\n",
      "Iteration 389, loss = 0.91561364\n",
      "Iteration 390, loss = 0.91486062\n",
      "Iteration 391, loss = 0.91410767\n",
      "Iteration 392, loss = 0.91335748\n",
      "Iteration 393, loss = 0.91260918\n",
      "Iteration 394, loss = 0.91186450\n",
      "Iteration 395, loss = 0.91112129\n",
      "Iteration 396, loss = 0.91038109\n",
      "Iteration 397, loss = 0.90964447\n",
      "Iteration 398, loss = 0.90890960\n",
      "Iteration 399, loss = 0.90817665\n",
      "Iteration 400, loss = 0.90744731\n",
      "Iteration 401, loss = 0.90671869\n",
      "Iteration 402, loss = 0.90598915\n",
      "Iteration 403, loss = 0.90526309\n",
      "Iteration 404, loss = 0.90453894\n",
      "Iteration 405, loss = 0.90381661\n",
      "Iteration 406, loss = 0.90309425\n",
      "Iteration 407, loss = 0.90237341\n",
      "Iteration 408, loss = 0.90165427\n",
      "Iteration 409, loss = 0.90093549\n",
      "Iteration 410, loss = 0.90021870\n",
      "Iteration 411, loss = 0.89950259\n",
      "Iteration 412, loss = 0.89879054\n",
      "Iteration 413, loss = 0.89807841\n",
      "Iteration 414, loss = 0.89736968\n",
      "Iteration 415, loss = 0.89666190\n",
      "Iteration 416, loss = 0.89595379\n",
      "Iteration 417, loss = 0.89525044\n",
      "Iteration 418, loss = 0.89454813\n",
      "Iteration 419, loss = 0.89384579\n",
      "Iteration 420, loss = 0.89314586\n",
      "Iteration 421, loss = 0.89244759\n",
      "Iteration 422, loss = 0.89174958\n",
      "Iteration 423, loss = 0.89105508\n",
      "Iteration 424, loss = 0.89036123\n",
      "Iteration 425, loss = 0.88966883\n",
      "Iteration 426, loss = 0.88897789\n",
      "Iteration 427, loss = 0.88828788\n",
      "Iteration 428, loss = 0.88759848\n",
      "Iteration 429, loss = 0.88691147\n",
      "Iteration 430, loss = 0.88622664\n",
      "Iteration 431, loss = 0.88554263\n",
      "Iteration 432, loss = 0.88485899\n",
      "Iteration 433, loss = 0.88417759\n",
      "Iteration 434, loss = 0.88349825\n",
      "Iteration 435, loss = 0.88282001\n",
      "Iteration 436, loss = 0.88214164\n",
      "Iteration 437, loss = 0.88146656\n",
      "Iteration 438, loss = 0.88079203\n",
      "Iteration 439, loss = 0.88011859\n",
      "Iteration 440, loss = 0.87944776\n",
      "Iteration 441, loss = 0.87877880\n",
      "Iteration 442, loss = 0.87811126\n",
      "Iteration 443, loss = 0.87744371\n",
      "Iteration 444, loss = 0.87677810\n",
      "Iteration 445, loss = 0.87611412\n",
      "Iteration 446, loss = 0.87545164\n",
      "Iteration 447, loss = 0.87478959\n",
      "Iteration 448, loss = 0.87412958\n",
      "Iteration 449, loss = 0.87347099\n",
      "Iteration 450, loss = 0.87281442\n",
      "Iteration 451, loss = 0.87215856\n",
      "Iteration 452, loss = 0.87150363\n",
      "Iteration 453, loss = 0.87084934\n",
      "Iteration 454, loss = 0.87019481\n",
      "Iteration 455, loss = 0.86954177\n",
      "Iteration 456, loss = 0.86889088\n",
      "Iteration 457, loss = 0.86824117\n",
      "Iteration 458, loss = 0.86759420\n",
      "Iteration 459, loss = 0.86694767\n",
      "Iteration 460, loss = 0.86630174\n",
      "Iteration 461, loss = 0.86565713\n",
      "Iteration 462, loss = 0.86501401\n",
      "Iteration 463, loss = 0.86436888\n",
      "Iteration 464, loss = 0.86372522\n",
      "Iteration 465, loss = 0.86308377\n",
      "Iteration 466, loss = 0.86244222\n",
      "Iteration 467, loss = 0.86180119\n",
      "Iteration 468, loss = 0.86116012\n",
      "Iteration 469, loss = 0.86051579\n",
      "Iteration 470, loss = 0.85987456\n",
      "Iteration 471, loss = 0.85923233\n",
      "Iteration 472, loss = 0.85858953\n",
      "Iteration 473, loss = 0.85794681\n",
      "Iteration 474, loss = 0.85730584\n",
      "Iteration 475, loss = 0.85666655\n",
      "Iteration 476, loss = 0.85602785\n",
      "Iteration 477, loss = 0.85539068\n",
      "Iteration 478, loss = 0.85475387\n",
      "Iteration 479, loss = 0.85411843\n",
      "Iteration 480, loss = 0.85348312\n",
      "Iteration 481, loss = 0.85284844\n",
      "Iteration 482, loss = 0.85221660\n",
      "Iteration 483, loss = 0.85158497\n",
      "Iteration 484, loss = 0.85095543\n",
      "Iteration 485, loss = 0.85032799\n",
      "Iteration 486, loss = 0.84970223\n",
      "Iteration 487, loss = 0.84907660\n",
      "Iteration 488, loss = 0.84845217\n",
      "Iteration 489, loss = 0.84782934\n",
      "Iteration 490, loss = 0.84720752\n",
      "Iteration 491, loss = 0.84658522\n",
      "Iteration 492, loss = 0.84596469\n",
      "Iteration 493, loss = 0.84534577\n",
      "Iteration 494, loss = 0.84472578\n",
      "Iteration 495, loss = 0.84410544\n",
      "Iteration 496, loss = 0.84348871\n",
      "Iteration 497, loss = 0.84287347\n",
      "Iteration 498, loss = 0.84225699\n",
      "Iteration 499, loss = 0.84164176\n",
      "Iteration 500, loss = 0.84102966\n",
      "Iteration 501, loss = 0.84042153\n",
      "Iteration 502, loss = 0.83981394\n",
      "Iteration 503, loss = 0.83920675\n",
      "Iteration 504, loss = 0.83860126\n",
      "Iteration 505, loss = 0.83799734\n",
      "Iteration 506, loss = 0.83739348\n",
      "Iteration 507, loss = 0.83679141\n",
      "Iteration 508, loss = 0.83619041\n",
      "Iteration 509, loss = 0.83559133\n",
      "Iteration 510, loss = 0.83499258\n",
      "Iteration 511, loss = 0.83439615\n",
      "Iteration 512, loss = 0.83380127\n",
      "Iteration 513, loss = 0.83320900\n",
      "Iteration 514, loss = 0.83261637\n",
      "Iteration 515, loss = 0.83202552\n",
      "Iteration 516, loss = 0.83143664\n",
      "Iteration 517, loss = 0.83084975\n",
      "Iteration 518, loss = 0.83026464\n",
      "Iteration 519, loss = 0.82968269\n",
      "Iteration 520, loss = 0.82910273\n",
      "Iteration 521, loss = 0.82852414\n",
      "Iteration 522, loss = 0.82794577\n",
      "Iteration 523, loss = 0.82736974\n",
      "Iteration 524, loss = 0.82679573\n",
      "Iteration 525, loss = 0.82622171\n",
      "Iteration 526, loss = 0.82564874\n",
      "Iteration 527, loss = 0.82507770\n",
      "Iteration 528, loss = 0.82450607\n",
      "Iteration 529, loss = 0.82393452\n",
      "Iteration 530, loss = 0.82335858\n",
      "Iteration 531, loss = 0.82277983\n",
      "Iteration 532, loss = 0.82219952\n",
      "Iteration 533, loss = 0.82161734\n",
      "Iteration 534, loss = 0.82103518\n",
      "Iteration 535, loss = 0.82045303\n",
      "Iteration 536, loss = 0.81987065\n",
      "Iteration 537, loss = 0.81928735\n",
      "Iteration 538, loss = 0.81870346\n",
      "Iteration 539, loss = 0.81812041\n",
      "Iteration 540, loss = 0.81753728\n",
      "Iteration 541, loss = 0.81695491\n",
      "Iteration 542, loss = 0.81637275\n",
      "Iteration 543, loss = 0.81579009\n",
      "Iteration 544, loss = 0.81520860\n",
      "Iteration 545, loss = 0.81462882\n",
      "Iteration 546, loss = 0.81404908\n",
      "Iteration 547, loss = 0.81347021\n",
      "Iteration 548, loss = 0.81289211\n",
      "Iteration 549, loss = 0.81231514\n",
      "Iteration 550, loss = 0.81173762\n",
      "Iteration 551, loss = 0.81116079\n",
      "Iteration 552, loss = 0.81058430\n",
      "Iteration 553, loss = 0.81000801\n",
      "Iteration 554, loss = 0.80943334\n",
      "Iteration 555, loss = 0.80885877\n",
      "Iteration 556, loss = 0.80828553\n",
      "Iteration 557, loss = 0.80771273\n",
      "Iteration 558, loss = 0.80714100\n",
      "Iteration 559, loss = 0.80657015\n",
      "Iteration 560, loss = 0.80599944\n",
      "Iteration 561, loss = 0.80543012\n",
      "Iteration 562, loss = 0.80486267\n",
      "Iteration 563, loss = 0.80429607\n",
      "Iteration 564, loss = 0.80373056\n",
      "Iteration 565, loss = 0.80316582\n",
      "Iteration 566, loss = 0.80260134\n",
      "Iteration 567, loss = 0.80203713\n",
      "Iteration 568, loss = 0.80147343\n",
      "Iteration 569, loss = 0.80091025\n",
      "Iteration 570, loss = 0.80034848\n",
      "Iteration 571, loss = 0.79978839\n",
      "Iteration 572, loss = 0.79922842\n",
      "Iteration 573, loss = 0.79867024\n",
      "Iteration 574, loss = 0.79811400\n",
      "Iteration 575, loss = 0.79755834\n",
      "Iteration 576, loss = 0.79700563\n",
      "Iteration 577, loss = 0.79645366\n",
      "Iteration 578, loss = 0.79590250\n",
      "Iteration 579, loss = 0.79535337\n",
      "Iteration 580, loss = 0.79480653\n",
      "Iteration 581, loss = 0.79426151\n",
      "Iteration 582, loss = 0.79371606\n",
      "Iteration 583, loss = 0.79317158\n",
      "Iteration 584, loss = 0.79262790\n",
      "Iteration 585, loss = 0.79208575\n",
      "Iteration 586, loss = 0.79154275\n",
      "Iteration 587, loss = 0.79099579\n",
      "Iteration 588, loss = 0.79044922\n",
      "Iteration 589, loss = 0.78989652\n",
      "Iteration 590, loss = 0.78934372\n",
      "Iteration 591, loss = 0.78879126\n",
      "Iteration 592, loss = 0.78823865\n",
      "Iteration 593, loss = 0.78768706\n",
      "Iteration 594, loss = 0.78713675\n",
      "Iteration 595, loss = 0.78658866\n",
      "Iteration 596, loss = 0.78604303\n",
      "Iteration 597, loss = 0.78549793\n",
      "Iteration 598, loss = 0.78495439\n",
      "Iteration 599, loss = 0.78441283\n",
      "Iteration 600, loss = 0.78387351\n",
      "Iteration 601, loss = 0.78333492\n",
      "Iteration 602, loss = 0.78279741\n",
      "Iteration 603, loss = 0.78226422\n",
      "Iteration 604, loss = 0.78173392\n",
      "Iteration 605, loss = 0.78120484\n",
      "Iteration 606, loss = 0.78067620\n",
      "Iteration 607, loss = 0.78014847\n",
      "Iteration 608, loss = 0.77962176\n",
      "Iteration 609, loss = 0.77909556\n",
      "Iteration 610, loss = 0.77857086\n",
      "Iteration 611, loss = 0.77804763\n",
      "Iteration 612, loss = 0.77752511\n",
      "Iteration 613, loss = 0.77700432\n",
      "Iteration 614, loss = 0.77648557\n",
      "Iteration 615, loss = 0.77596935\n",
      "Iteration 616, loss = 0.77545241\n",
      "Iteration 617, loss = 0.77493607\n",
      "Iteration 618, loss = 0.77442140\n",
      "Iteration 619, loss = 0.77390632\n",
      "Iteration 620, loss = 0.77339221\n",
      "Iteration 621, loss = 0.77287950\n",
      "Iteration 622, loss = 0.77236707\n",
      "Iteration 623, loss = 0.77185595\n",
      "Iteration 624, loss = 0.77134655\n",
      "Iteration 625, loss = 0.77083828\n",
      "Iteration 626, loss = 0.77033055\n",
      "Iteration 627, loss = 0.76982467\n",
      "Iteration 628, loss = 0.76931912\n",
      "Iteration 629, loss = 0.76881449\n",
      "Iteration 630, loss = 0.76830964\n",
      "Iteration 631, loss = 0.76780660\n",
      "Iteration 632, loss = 0.76730355\n",
      "Iteration 633, loss = 0.76680279\n",
      "Iteration 634, loss = 0.76630231\n",
      "Iteration 635, loss = 0.76580389\n",
      "Iteration 636, loss = 0.76530586\n",
      "Iteration 637, loss = 0.76480842\n",
      "Iteration 638, loss = 0.76431117\n",
      "Iteration 639, loss = 0.76381484\n",
      "Iteration 640, loss = 0.76332023\n",
      "Iteration 641, loss = 0.76282658\n",
      "Iteration 642, loss = 0.76233304\n",
      "Iteration 643, loss = 0.76183958\n",
      "Iteration 644, loss = 0.76134715\n",
      "Iteration 645, loss = 0.76085544\n",
      "Iteration 646, loss = 0.76036402\n",
      "Iteration 647, loss = 0.75987430\n",
      "Iteration 648, loss = 0.75938489\n",
      "Iteration 649, loss = 0.75889531\n",
      "Iteration 650, loss = 0.75840634\n",
      "Iteration 651, loss = 0.75791800\n",
      "Iteration 652, loss = 0.75743141\n",
      "Iteration 653, loss = 0.75694560\n",
      "Iteration 654, loss = 0.75645993\n",
      "Iteration 655, loss = 0.75597493\n",
      "Iteration 656, loss = 0.75549026\n",
      "Iteration 657, loss = 0.75500689\n",
      "Iteration 658, loss = 0.75452396\n",
      "Iteration 659, loss = 0.75404071\n",
      "Iteration 660, loss = 0.75355837\n",
      "Iteration 661, loss = 0.75307731\n",
      "Iteration 662, loss = 0.75259674\n",
      "Iteration 663, loss = 0.75211683\n",
      "Iteration 664, loss = 0.75163901\n",
      "Iteration 665, loss = 0.75116313\n",
      "Iteration 666, loss = 0.75068798\n",
      "Iteration 667, loss = 0.75021346\n",
      "Iteration 668, loss = 0.74973981\n",
      "Iteration 669, loss = 0.74926837\n",
      "Iteration 670, loss = 0.74879781\n",
      "Iteration 671, loss = 0.74832821\n",
      "Iteration 672, loss = 0.74785935\n",
      "Iteration 673, loss = 0.74739178\n",
      "Iteration 674, loss = 0.74692661\n",
      "Iteration 675, loss = 0.74646057\n",
      "Iteration 676, loss = 0.74599650\n",
      "Iteration 677, loss = 0.74553304\n",
      "Iteration 678, loss = 0.74507161\n",
      "Iteration 679, loss = 0.74461209\n",
      "Iteration 680, loss = 0.74415310\n",
      "Iteration 681, loss = 0.74369392\n",
      "Iteration 682, loss = 0.74323280\n",
      "Iteration 683, loss = 0.74277169\n",
      "Iteration 684, loss = 0.74231425\n",
      "Iteration 685, loss = 0.74185846\n",
      "Iteration 686, loss = 0.74140291\n",
      "Iteration 687, loss = 0.74094819\n",
      "Iteration 688, loss = 0.74049360\n",
      "Iteration 689, loss = 0.74003890\n",
      "Iteration 690, loss = 0.73958555\n",
      "Iteration 691, loss = 0.73913228\n",
      "Iteration 692, loss = 0.73867858\n",
      "Iteration 693, loss = 0.73822518\n",
      "Iteration 694, loss = 0.73777215\n",
      "Iteration 695, loss = 0.73732091\n",
      "Iteration 696, loss = 0.73687017\n",
      "Iteration 697, loss = 0.73642022\n",
      "Iteration 698, loss = 0.73597058\n",
      "Iteration 699, loss = 0.73552130\n",
      "Iteration 700, loss = 0.73507271\n",
      "Iteration 701, loss = 0.73462507\n",
      "Iteration 702, loss = 0.73417985\n",
      "Iteration 703, loss = 0.73373642\n",
      "Iteration 704, loss = 0.73329354\n",
      "Iteration 705, loss = 0.73285194\n",
      "Iteration 706, loss = 0.73241378\n",
      "Iteration 707, loss = 0.73197702\n",
      "Iteration 708, loss = 0.73153848\n",
      "Iteration 709, loss = 0.73109991\n",
      "Iteration 710, loss = 0.73066494\n",
      "Iteration 711, loss = 0.73023110\n",
      "Iteration 712, loss = 0.72979758\n",
      "Iteration 713, loss = 0.72936478\n",
      "Iteration 714, loss = 0.72893302\n",
      "Iteration 715, loss = 0.72850180\n",
      "Iteration 716, loss = 0.72807272\n",
      "Iteration 717, loss = 0.72764425\n",
      "Iteration 718, loss = 0.72721727\n",
      "Iteration 719, loss = 0.72679151\n",
      "Iteration 720, loss = 0.72636571\n",
      "Iteration 721, loss = 0.72594106\n",
      "Iteration 722, loss = 0.72551728\n",
      "Iteration 723, loss = 0.72509374\n",
      "Iteration 724, loss = 0.72467022\n",
      "Iteration 725, loss = 0.72424844\n",
      "Iteration 726, loss = 0.72382674\n",
      "Iteration 727, loss = 0.72340604\n",
      "Iteration 728, loss = 0.72298486\n",
      "Iteration 729, loss = 0.72256037\n",
      "Iteration 730, loss = 0.72213459\n",
      "Iteration 731, loss = 0.72170844\n",
      "Iteration 732, loss = 0.72128350\n",
      "Iteration 733, loss = 0.72085919\n",
      "Iteration 734, loss = 0.72043443\n",
      "Iteration 735, loss = 0.72001046\n",
      "Iteration 736, loss = 0.71958680\n",
      "Iteration 737, loss = 0.71916445\n",
      "Iteration 738, loss = 0.71874193\n",
      "Iteration 739, loss = 0.71832062\n",
      "Iteration 740, loss = 0.71789924\n",
      "Iteration 741, loss = 0.71747830\n",
      "Iteration 742, loss = 0.71705807\n",
      "Iteration 743, loss = 0.71663882\n",
      "Iteration 744, loss = 0.71621948\n",
      "Iteration 745, loss = 0.71580054\n",
      "Iteration 746, loss = 0.71538296\n",
      "Iteration 747, loss = 0.71496648\n",
      "Iteration 748, loss = 0.71455029\n",
      "Iteration 749, loss = 0.71413403\n",
      "Iteration 750, loss = 0.71371907\n",
      "Iteration 751, loss = 0.71330398\n",
      "Iteration 752, loss = 0.71288693\n",
      "Iteration 753, loss = 0.71246895\n",
      "Iteration 754, loss = 0.71205116\n",
      "Iteration 755, loss = 0.71163299\n",
      "Iteration 756, loss = 0.71121479\n",
      "Iteration 757, loss = 0.71079665\n",
      "Iteration 758, loss = 0.71037881\n",
      "Iteration 759, loss = 0.70996116\n",
      "Iteration 760, loss = 0.70954335\n",
      "Iteration 761, loss = 0.70912577\n",
      "Iteration 762, loss = 0.70870838\n",
      "Iteration 763, loss = 0.70829163\n",
      "Iteration 764, loss = 0.70787583\n",
      "Iteration 765, loss = 0.70746135\n",
      "Iteration 766, loss = 0.70704789\n",
      "Iteration 767, loss = 0.70663425\n",
      "Iteration 768, loss = 0.70622112\n",
      "Iteration 769, loss = 0.70580928\n",
      "Iteration 770, loss = 0.70539866\n",
      "Iteration 771, loss = 0.70498868\n",
      "Iteration 772, loss = 0.70457908\n",
      "Iteration 773, loss = 0.70416995\n",
      "Iteration 774, loss = 0.70376034\n",
      "Iteration 775, loss = 0.70335152\n",
      "Iteration 776, loss = 0.70294279\n",
      "Iteration 777, loss = 0.70253610\n",
      "Iteration 778, loss = 0.70213027\n",
      "Iteration 779, loss = 0.70172595\n",
      "Iteration 780, loss = 0.70132214\n",
      "Iteration 781, loss = 0.70091869\n",
      "Iteration 782, loss = 0.70051714\n",
      "Iteration 783, loss = 0.70011566\n",
      "Iteration 784, loss = 0.69971430\n",
      "Iteration 785, loss = 0.69931357\n",
      "Iteration 786, loss = 0.69891294\n",
      "Iteration 787, loss = 0.69851329\n",
      "Iteration 788, loss = 0.69811416\n",
      "Iteration 789, loss = 0.69771509\n",
      "Iteration 790, loss = 0.69731681\n",
      "Iteration 791, loss = 0.69691860\n",
      "Iteration 792, loss = 0.69652142\n",
      "Iteration 793, loss = 0.69612454\n",
      "Iteration 794, loss = 0.69572797\n",
      "Iteration 795, loss = 0.69533546\n",
      "Iteration 796, loss = 0.69494552\n",
      "Iteration 797, loss = 0.69455525\n",
      "Iteration 798, loss = 0.69416617\n",
      "Iteration 799, loss = 0.69377795\n",
      "Iteration 800, loss = 0.69339066\n",
      "Iteration 801, loss = 0.69300450\n",
      "Iteration 802, loss = 0.69261815\n",
      "Iteration 803, loss = 0.69223145\n",
      "Iteration 804, loss = 0.69184521\n",
      "Iteration 805, loss = 0.69146022\n",
      "Iteration 806, loss = 0.69107610\n",
      "Iteration 807, loss = 0.69068978\n",
      "Iteration 808, loss = 0.69030277\n",
      "Iteration 809, loss = 0.68991475\n",
      "Iteration 810, loss = 0.68952653\n",
      "Iteration 811, loss = 0.68913806\n",
      "Iteration 812, loss = 0.68874931\n",
      "Iteration 813, loss = 0.68836025\n",
      "Iteration 814, loss = 0.68797162\n",
      "Iteration 815, loss = 0.68758308\n",
      "Iteration 816, loss = 0.68719499\n",
      "Iteration 817, loss = 0.68680723\n",
      "Iteration 818, loss = 0.68641951\n",
      "Iteration 819, loss = 0.68603302\n",
      "Iteration 820, loss = 0.68564702\n",
      "Iteration 821, loss = 0.68526221\n",
      "Iteration 822, loss = 0.68487754\n",
      "Iteration 823, loss = 0.68449369\n",
      "Iteration 824, loss = 0.68411054\n",
      "Iteration 825, loss = 0.68372729\n",
      "Iteration 826, loss = 0.68334259\n",
      "Iteration 827, loss = 0.68295765\n",
      "Iteration 828, loss = 0.68257235\n",
      "Iteration 829, loss = 0.68218710\n",
      "Iteration 830, loss = 0.68180311\n",
      "Iteration 831, loss = 0.68142004\n",
      "Iteration 832, loss = 0.68103791\n",
      "Iteration 833, loss = 0.68065667\n",
      "Iteration 834, loss = 0.68027671\n",
      "Iteration 835, loss = 0.67989786\n",
      "Iteration 836, loss = 0.67951859\n",
      "Iteration 837, loss = 0.67914000\n",
      "Iteration 838, loss = 0.67876162\n",
      "Iteration 839, loss = 0.67838361\n",
      "Iteration 840, loss = 0.67800664\n",
      "Iteration 841, loss = 0.67762919\n",
      "Iteration 842, loss = 0.67725148\n",
      "Iteration 843, loss = 0.67687512\n",
      "Iteration 844, loss = 0.67649875\n",
      "Iteration 845, loss = 0.67612248\n",
      "Iteration 846, loss = 0.67574726\n",
      "Iteration 847, loss = 0.67537284\n",
      "Iteration 848, loss = 0.67499887\n",
      "Iteration 849, loss = 0.67462581\n",
      "Iteration 850, loss = 0.67425364\n",
      "Iteration 851, loss = 0.67388178\n",
      "Iteration 852, loss = 0.67351058\n",
      "Iteration 853, loss = 0.67314005\n",
      "Iteration 854, loss = 0.67276991\n",
      "Iteration 855, loss = 0.67240135\n",
      "Iteration 856, loss = 0.67203294\n",
      "Iteration 857, loss = 0.67166470\n",
      "Iteration 858, loss = 0.67129786\n",
      "Iteration 859, loss = 0.67093155\n",
      "Iteration 860, loss = 0.67056555\n",
      "Iteration 861, loss = 0.67020052\n",
      "Iteration 862, loss = 0.66983633\n",
      "Iteration 863, loss = 0.66947350\n",
      "Iteration 864, loss = 0.66911123\n",
      "Iteration 865, loss = 0.66874981\n",
      "Iteration 866, loss = 0.66838878\n",
      "Iteration 867, loss = 0.66802877\n",
      "Iteration 868, loss = 0.66766939\n",
      "Iteration 869, loss = 0.66730908\n",
      "Iteration 870, loss = 0.66694926\n",
      "Iteration 871, loss = 0.66659027\n",
      "Iteration 872, loss = 0.66623145\n",
      "Iteration 873, loss = 0.66587353\n",
      "Iteration 874, loss = 0.66551573\n",
      "Iteration 875, loss = 0.66515849\n",
      "Iteration 876, loss = 0.66480224\n",
      "Iteration 877, loss = 0.66444608\n",
      "Iteration 878, loss = 0.66409082\n",
      "Iteration 879, loss = 0.66373619\n",
      "Iteration 880, loss = 0.66338240\n",
      "Iteration 881, loss = 0.66302864\n",
      "Iteration 882, loss = 0.66267630\n",
      "Iteration 883, loss = 0.66232406\n",
      "Iteration 884, loss = 0.66197303\n",
      "Iteration 885, loss = 0.66162266\n",
      "Iteration 886, loss = 0.66127261\n",
      "Iteration 887, loss = 0.66092327\n",
      "Iteration 888, loss = 0.66057481\n",
      "Iteration 889, loss = 0.66022678\n",
      "Iteration 890, loss = 0.65987997\n",
      "Iteration 891, loss = 0.65953379\n",
      "Iteration 892, loss = 0.65918798\n",
      "Iteration 893, loss = 0.65884348\n",
      "Iteration 894, loss = 0.65849939\n",
      "Iteration 895, loss = 0.65815541\n",
      "Iteration 896, loss = 0.65781189\n",
      "Iteration 897, loss = 0.65746946\n",
      "Iteration 898, loss = 0.65712743\n",
      "Iteration 899, loss = 0.65678801\n",
      "Iteration 900, loss = 0.65644986\n",
      "Iteration 901, loss = 0.65611254\n",
      "Iteration 902, loss = 0.65577533\n",
      "Iteration 903, loss = 0.65543870\n",
      "Iteration 904, loss = 0.65510244\n",
      "Iteration 905, loss = 0.65476676\n",
      "Iteration 906, loss = 0.65443163\n",
      "Iteration 907, loss = 0.65409733\n",
      "Iteration 908, loss = 0.65376333\n",
      "Iteration 909, loss = 0.65343043\n",
      "Iteration 910, loss = 0.65309760\n",
      "Iteration 911, loss = 0.65276526\n",
      "Iteration 912, loss = 0.65243386\n",
      "Iteration 913, loss = 0.65210236\n",
      "Iteration 914, loss = 0.65177131\n",
      "Iteration 915, loss = 0.65143990\n",
      "Iteration 916, loss = 0.65110910\n",
      "Iteration 917, loss = 0.65077874\n",
      "Iteration 918, loss = 0.65044858\n",
      "Iteration 919, loss = 0.65011879\n",
      "Iteration 920, loss = 0.64978948\n",
      "Iteration 921, loss = 0.64946084\n",
      "Iteration 922, loss = 0.64913271\n",
      "Iteration 923, loss = 0.64880505\n",
      "Iteration 924, loss = 0.64847760\n",
      "Iteration 925, loss = 0.64815052\n",
      "Iteration 926, loss = 0.64782411\n",
      "Iteration 927, loss = 0.64749848\n",
      "Iteration 928, loss = 0.64717286\n",
      "Iteration 929, loss = 0.64684773\n",
      "Iteration 930, loss = 0.64652339\n",
      "Iteration 931, loss = 0.64619894\n",
      "Iteration 932, loss = 0.64587391\n",
      "Iteration 933, loss = 0.64554991\n",
      "Iteration 934, loss = 0.64522651\n",
      "Iteration 935, loss = 0.64490298\n",
      "Iteration 936, loss = 0.64458036\n",
      "Iteration 937, loss = 0.64425814\n",
      "Iteration 938, loss = 0.64393636\n",
      "Iteration 939, loss = 0.64361459\n",
      "Iteration 940, loss = 0.64329357\n",
      "Iteration 941, loss = 0.64297308\n",
      "Iteration 942, loss = 0.64265311\n",
      "Iteration 943, loss = 0.64233315\n",
      "Iteration 944, loss = 0.64201347\n",
      "Iteration 945, loss = 0.64169417\n",
      "Iteration 946, loss = 0.64137553\n",
      "Iteration 947, loss = 0.64105768\n",
      "Iteration 948, loss = 0.64074002\n",
      "Iteration 949, loss = 0.64042315\n",
      "Iteration 950, loss = 0.64010658\n",
      "Iteration 951, loss = 0.63979047\n",
      "Iteration 952, loss = 0.63947467\n",
      "Iteration 953, loss = 0.63915906\n",
      "Iteration 954, loss = 0.63884402\n",
      "Iteration 955, loss = 0.63852929\n",
      "Iteration 956, loss = 0.63821506\n",
      "Iteration 957, loss = 0.63790143\n",
      "Iteration 958, loss = 0.63758790\n",
      "Iteration 959, loss = 0.63727487\n",
      "Iteration 960, loss = 0.63696236\n",
      "Iteration 961, loss = 0.63665013\n",
      "Iteration 962, loss = 0.63633836\n",
      "Iteration 963, loss = 0.63602676\n",
      "Iteration 964, loss = 0.63571571\n",
      "Iteration 965, loss = 0.63540488\n",
      "Iteration 966, loss = 0.63509504\n",
      "Iteration 967, loss = 0.63478522\n",
      "Iteration 968, loss = 0.63447563\n",
      "Iteration 969, loss = 0.63416668\n",
      "Iteration 970, loss = 0.63385801\n",
      "Iteration 971, loss = 0.63354980\n",
      "Iteration 972, loss = 0.63324168\n",
      "Iteration 973, loss = 0.63293420\n",
      "Iteration 974, loss = 0.63262697\n",
      "Iteration 975, loss = 0.63232012\n",
      "Iteration 976, loss = 0.63201357\n",
      "Iteration 977, loss = 0.63170770\n",
      "Iteration 978, loss = 0.63140228\n",
      "Iteration 979, loss = 0.63109712\n",
      "Iteration 980, loss = 0.63079210\n",
      "Iteration 981, loss = 0.63048784\n",
      "Iteration 982, loss = 0.63018518\n",
      "Iteration 983, loss = 0.62988301\n",
      "Iteration 984, loss = 0.62958128\n",
      "Iteration 985, loss = 0.62927984\n",
      "Iteration 986, loss = 0.62897889\n",
      "Iteration 987, loss = 0.62867837\n",
      "Iteration 988, loss = 0.62837812\n",
      "Iteration 989, loss = 0.62807832\n",
      "Iteration 990, loss = 0.62777856\n",
      "Iteration 991, loss = 0.62747868\n",
      "Iteration 992, loss = 0.62717909\n",
      "Iteration 993, loss = 0.62688060\n",
      "Iteration 994, loss = 0.62658360\n",
      "Iteration 995, loss = 0.62628707\n",
      "Iteration 996, loss = 0.62599065\n",
      "Iteration 997, loss = 0.62569420\n",
      "Iteration 998, loss = 0.62539844\n",
      "Iteration 999, loss = 0.62510297\n",
      "Iteration 1000, loss = 0.62480784\n",
      "Iteration 1001, loss = 0.62451306\n",
      "Iteration 1002, loss = 0.62421875\n",
      "Iteration 1003, loss = 0.62392548\n",
      "Iteration 1004, loss = 0.62362914\n",
      "Iteration 1005, loss = 0.62333208\n",
      "Iteration 1006, loss = 0.62303416\n",
      "Iteration 1007, loss = 0.62273587\n",
      "Iteration 1008, loss = 0.62243781\n",
      "Iteration 1009, loss = 0.62213965\n",
      "Iteration 1010, loss = 0.62184084\n",
      "Iteration 1011, loss = 0.62154161\n",
      "Iteration 1012, loss = 0.62124244\n",
      "Iteration 1013, loss = 0.62094333\n",
      "Iteration 1014, loss = 0.62064397\n",
      "Iteration 1015, loss = 0.62034479\n",
      "Iteration 1016, loss = 0.62004552\n",
      "Iteration 1017, loss = 0.61974624\n",
      "Iteration 1018, loss = 0.61944754\n",
      "Iteration 1019, loss = 0.61914896\n",
      "Iteration 1020, loss = 0.61885125\n",
      "Iteration 1021, loss = 0.61855354\n",
      "Iteration 1022, loss = 0.61825582\n",
      "Iteration 1023, loss = 0.61795835\n",
      "Iteration 1024, loss = 0.61766099\n",
      "Iteration 1025, loss = 0.61736390\n",
      "Iteration 1026, loss = 0.61706704\n",
      "Iteration 1027, loss = 0.61677084\n",
      "Iteration 1028, loss = 0.61647535\n",
      "Iteration 1029, loss = 0.61617956\n",
      "Iteration 1030, loss = 0.61588434\n",
      "Iteration 1031, loss = 0.61558959\n",
      "Iteration 1032, loss = 0.61529474\n",
      "Iteration 1033, loss = 0.61500059\n",
      "Iteration 1034, loss = 0.61470673\n",
      "Iteration 1035, loss = 0.61441301\n",
      "Iteration 1036, loss = 0.61412004\n",
      "Iteration 1037, loss = 0.61382723\n",
      "Iteration 1038, loss = 0.61353474\n",
      "Iteration 1039, loss = 0.61324276\n",
      "Iteration 1040, loss = 0.61295146\n",
      "Iteration 1041, loss = 0.61266038\n",
      "Iteration 1042, loss = 0.61236979\n",
      "Iteration 1043, loss = 0.61207972\n",
      "Iteration 1044, loss = 0.61179005\n",
      "Iteration 1045, loss = 0.61150090\n",
      "Iteration 1046, loss = 0.61121225\n",
      "Iteration 1047, loss = 0.61092361\n",
      "Iteration 1048, loss = 0.61063559\n",
      "Iteration 1049, loss = 0.61034777\n",
      "Iteration 1050, loss = 0.61006020\n",
      "Iteration 1051, loss = 0.60977311\n",
      "Iteration 1052, loss = 0.60948617\n",
      "Iteration 1053, loss = 0.60919979\n",
      "Iteration 1054, loss = 0.60891347\n",
      "Iteration 1055, loss = 0.60862703\n",
      "Iteration 1056, loss = 0.60834140\n",
      "Iteration 1057, loss = 0.60805580\n",
      "Iteration 1058, loss = 0.60777053\n",
      "Iteration 1059, loss = 0.60748580\n",
      "Iteration 1060, loss = 0.60720125\n",
      "Iteration 1061, loss = 0.60691733\n",
      "Iteration 1062, loss = 0.60663412\n",
      "Iteration 1063, loss = 0.60635129\n",
      "Iteration 1064, loss = 0.60606893\n",
      "Iteration 1065, loss = 0.60578706\n",
      "Iteration 1066, loss = 0.60550542\n",
      "Iteration 1067, loss = 0.60522424\n",
      "Iteration 1068, loss = 0.60494308\n",
      "Iteration 1069, loss = 0.60466254\n",
      "Iteration 1070, loss = 0.60438201\n",
      "Iteration 1071, loss = 0.60410162\n",
      "Iteration 1072, loss = 0.60382222\n",
      "Iteration 1073, loss = 0.60354322\n",
      "Iteration 1074, loss = 0.60326442\n",
      "Iteration 1075, loss = 0.60298597\n",
      "Iteration 1076, loss = 0.60270845\n",
      "Iteration 1077, loss = 0.60243095\n",
      "Iteration 1078, loss = 0.60215369\n",
      "Iteration 1079, loss = 0.60187703\n",
      "Iteration 1080, loss = 0.60160063\n",
      "Iteration 1081, loss = 0.60132458\n",
      "Iteration 1082, loss = 0.60104885\n",
      "Iteration 1083, loss = 0.60077328\n",
      "Iteration 1084, loss = 0.60049826\n",
      "Iteration 1085, loss = 0.60022425\n",
      "Iteration 1086, loss = 0.59995043\n",
      "Iteration 1087, loss = 0.59967733\n",
      "Iteration 1088, loss = 0.59940439\n",
      "Iteration 1089, loss = 0.59913190\n",
      "Iteration 1090, loss = 0.59885966\n",
      "Iteration 1091, loss = 0.59858797\n",
      "Iteration 1092, loss = 0.59831654\n",
      "Iteration 1093, loss = 0.59804495\n",
      "Iteration 1094, loss = 0.59777369\n",
      "Iteration 1095, loss = 0.59750226\n",
      "Iteration 1096, loss = 0.59723047\n",
      "Iteration 1097, loss = 0.59695895\n",
      "Iteration 1098, loss = 0.59668757\n",
      "Iteration 1099, loss = 0.59641666\n",
      "Iteration 1100, loss = 0.59614558\n",
      "Iteration 1101, loss = 0.59587475\n",
      "Iteration 1102, loss = 0.59560392\n",
      "Iteration 1103, loss = 0.59533275\n",
      "Iteration 1104, loss = 0.59506196\n",
      "Iteration 1105, loss = 0.59479093\n",
      "Iteration 1106, loss = 0.59452007\n",
      "Iteration 1107, loss = 0.59424948\n",
      "Iteration 1108, loss = 0.59397932\n",
      "Iteration 1109, loss = 0.59370892\n",
      "Iteration 1110, loss = 0.59343902\n",
      "Iteration 1111, loss = 0.59316936\n",
      "Iteration 1112, loss = 0.59289978\n",
      "Iteration 1113, loss = 0.59263010\n",
      "Iteration 1114, loss = 0.59236076\n",
      "Iteration 1115, loss = 0.59209146\n",
      "Iteration 1116, loss = 0.59182272\n",
      "Iteration 1117, loss = 0.59155422\n",
      "Iteration 1118, loss = 0.59128386\n",
      "Iteration 1119, loss = 0.59101299\n",
      "Iteration 1120, loss = 0.59074241\n",
      "Iteration 1121, loss = 0.59047147\n",
      "Iteration 1122, loss = 0.59020151\n",
      "Iteration 1123, loss = 0.58993174\n",
      "Iteration 1124, loss = 0.58966224\n",
      "Iteration 1125, loss = 0.58939304\n",
      "Iteration 1126, loss = 0.58912401\n",
      "Iteration 1127, loss = 0.58885560\n",
      "Iteration 1128, loss = 0.58858714\n",
      "Iteration 1129, loss = 0.58831882\n",
      "Iteration 1130, loss = 0.58805070\n",
      "Iteration 1131, loss = 0.58778312\n",
      "Iteration 1132, loss = 0.58751597\n",
      "Iteration 1133, loss = 0.58724900\n",
      "Iteration 1134, loss = 0.58698235\n",
      "Iteration 1135, loss = 0.58671600\n",
      "Iteration 1136, loss = 0.58644994\n",
      "Iteration 1137, loss = 0.58618700\n",
      "Iteration 1138, loss = 0.58592482\n",
      "Iteration 1139, loss = 0.58566272\n",
      "Iteration 1140, loss = 0.58540086\n",
      "Iteration 1141, loss = 0.58513938\n",
      "Iteration 1142, loss = 0.58487837\n",
      "Iteration 1143, loss = 0.58461713\n",
      "Iteration 1144, loss = 0.58435597\n",
      "Iteration 1145, loss = 0.58409526\n",
      "Iteration 1146, loss = 0.58383458\n",
      "Iteration 1147, loss = 0.58357408\n",
      "Iteration 1148, loss = 0.58331402\n",
      "Iteration 1149, loss = 0.58305427\n",
      "Iteration 1150, loss = 0.58279456\n",
      "Iteration 1151, loss = 0.58253580\n",
      "Iteration 1152, loss = 0.58227757\n",
      "Iteration 1153, loss = 0.58202047\n",
      "Iteration 1154, loss = 0.58176443\n",
      "Iteration 1155, loss = 0.58150904\n",
      "Iteration 1156, loss = 0.58125367\n",
      "Iteration 1157, loss = 0.58099788\n",
      "Iteration 1158, loss = 0.58074225\n",
      "Iteration 1159, loss = 0.58048739\n",
      "Iteration 1160, loss = 0.58023284\n",
      "Iteration 1161, loss = 0.57997816\n",
      "Iteration 1162, loss = 0.57972364\n",
      "Iteration 1163, loss = 0.57946932\n",
      "Iteration 1164, loss = 0.57921563\n",
      "Iteration 1165, loss = 0.57896216\n",
      "Iteration 1166, loss = 0.57870899\n",
      "Iteration 1167, loss = 0.57845683\n",
      "Iteration 1168, loss = 0.57820487\n",
      "Iteration 1169, loss = 0.57795279\n",
      "Iteration 1170, loss = 0.57770132\n",
      "Iteration 1171, loss = 0.57745012\n",
      "Iteration 1172, loss = 0.57719929\n",
      "Iteration 1173, loss = 0.57694858\n",
      "Iteration 1174, loss = 0.57669834\n",
      "Iteration 1175, loss = 0.57644808\n",
      "Iteration 1176, loss = 0.57619765\n",
      "Iteration 1177, loss = 0.57594764\n",
      "Iteration 1178, loss = 0.57569796\n",
      "Iteration 1179, loss = 0.57544831\n",
      "Iteration 1180, loss = 0.57519857\n",
      "Iteration 1181, loss = 0.57494912\n",
      "Iteration 1182, loss = 0.57470012\n",
      "Iteration 1183, loss = 0.57445136\n",
      "Iteration 1184, loss = 0.57420270\n",
      "Iteration 1185, loss = 0.57395427\n",
      "Iteration 1186, loss = 0.57370618\n",
      "Iteration 1187, loss = 0.57345828\n",
      "Iteration 1188, loss = 0.57321138\n",
      "Iteration 1189, loss = 0.57296462\n",
      "Iteration 1190, loss = 0.57271847\n",
      "Iteration 1191, loss = 0.57247225\n",
      "Iteration 1192, loss = 0.57222602\n",
      "Iteration 1193, loss = 0.57197898\n",
      "Iteration 1194, loss = 0.57173197\n",
      "Iteration 1195, loss = 0.57148518\n",
      "Iteration 1196, loss = 0.57123827\n",
      "Iteration 1197, loss = 0.57099146\n",
      "Iteration 1198, loss = 0.57074472\n",
      "Iteration 1199, loss = 0.57049816\n",
      "Iteration 1200, loss = 0.57025191\n",
      "Iteration 1201, loss = 0.57000548\n",
      "Iteration 1202, loss = 0.56975920\n",
      "Iteration 1203, loss = 0.56951299\n",
      "Iteration 1204, loss = 0.56926695\n",
      "Iteration 1205, loss = 0.56902112\n",
      "Iteration 1206, loss = 0.56877544\n",
      "Iteration 1207, loss = 0.56853001\n",
      "Iteration 1208, loss = 0.56828488\n",
      "Iteration 1209, loss = 0.56804009\n",
      "Iteration 1210, loss = 0.56779575\n",
      "Iteration 1211, loss = 0.56755142\n",
      "Iteration 1212, loss = 0.56730696\n",
      "Iteration 1213, loss = 0.56706246\n",
      "Iteration 1214, loss = 0.56681758\n",
      "Iteration 1215, loss = 0.56657275\n",
      "Iteration 1216, loss = 0.56632786\n",
      "Iteration 1217, loss = 0.56608338\n",
      "Iteration 1218, loss = 0.56583881\n",
      "Iteration 1219, loss = 0.56559425\n",
      "Iteration 1220, loss = 0.56534967\n",
      "Iteration 1221, loss = 0.56510562\n",
      "Iteration 1222, loss = 0.56486156\n",
      "Iteration 1223, loss = 0.56461759\n",
      "Iteration 1224, loss = 0.56437385\n",
      "Iteration 1225, loss = 0.56413055\n",
      "Iteration 1226, loss = 0.56388664\n",
      "Iteration 1227, loss = 0.56364261\n",
      "Iteration 1228, loss = 0.56339833\n",
      "Iteration 1229, loss = 0.56315409\n",
      "Iteration 1230, loss = 0.56291001\n",
      "Iteration 1231, loss = 0.56266614\n",
      "Iteration 1232, loss = 0.56242249\n",
      "Iteration 1233, loss = 0.56218053\n",
      "Iteration 1234, loss = 0.56193746\n",
      "Iteration 1235, loss = 0.56169408\n",
      "Iteration 1236, loss = 0.56145082\n",
      "Iteration 1237, loss = 0.56120763\n",
      "Iteration 1238, loss = 0.56096451\n",
      "Iteration 1239, loss = 0.56072138\n",
      "Iteration 1240, loss = 0.56047837\n",
      "Iteration 1241, loss = 0.56023542\n",
      "Iteration 1242, loss = 0.55999144\n",
      "Iteration 1243, loss = 0.55974884\n",
      "Iteration 1244, loss = 0.55950700\n",
      "Iteration 1245, loss = 0.55926523\n",
      "Iteration 1246, loss = 0.55902397\n",
      "Iteration 1247, loss = 0.55878344\n",
      "Iteration 1248, loss = 0.55854298\n",
      "Iteration 1249, loss = 0.55830283\n",
      "Iteration 1250, loss = 0.55806265\n",
      "Iteration 1251, loss = 0.55782125\n",
      "Iteration 1252, loss = 0.55757863\n",
      "Iteration 1253, loss = 0.55733698\n",
      "Iteration 1254, loss = 0.55709530\n",
      "Iteration 1255, loss = 0.55685454\n",
      "Iteration 1256, loss = 0.55661384\n",
      "Iteration 1257, loss = 0.55637432\n",
      "Iteration 1258, loss = 0.55613629\n",
      "Iteration 1259, loss = 0.55589764\n",
      "Iteration 1260, loss = 0.55565878\n",
      "Iteration 1261, loss = 0.55542029\n",
      "Iteration 1262, loss = 0.55518202\n",
      "Iteration 1263, loss = 0.55494407\n",
      "Iteration 1264, loss = 0.55470694\n",
      "Iteration 1265, loss = 0.55447057\n",
      "Iteration 1266, loss = 0.55423443\n",
      "Iteration 1267, loss = 0.55399881\n",
      "Iteration 1268, loss = 0.55376336\n",
      "Iteration 1269, loss = 0.55352811\n",
      "Iteration 1270, loss = 0.55329396\n",
      "Iteration 1271, loss = 0.55305996\n",
      "Iteration 1272, loss = 0.55282622\n",
      "Iteration 1273, loss = 0.55259286\n",
      "Iteration 1274, loss = 0.55235983\n",
      "Iteration 1275, loss = 0.55212669\n",
      "Iteration 1276, loss = 0.55189437\n",
      "Iteration 1277, loss = 0.55166384\n",
      "Iteration 1278, loss = 0.55143316\n",
      "Iteration 1279, loss = 0.55120307\n",
      "Iteration 1280, loss = 0.55097317\n",
      "Iteration 1281, loss = 0.55074294\n",
      "Iteration 1282, loss = 0.55051256\n",
      "Iteration 1283, loss = 0.55028197\n",
      "Iteration 1284, loss = 0.55005111\n",
      "Iteration 1285, loss = 0.54981932\n",
      "Iteration 1286, loss = 0.54958726\n",
      "Iteration 1287, loss = 0.54935530\n",
      "Iteration 1288, loss = 0.54912331\n",
      "Iteration 1289, loss = 0.54889115\n",
      "Iteration 1290, loss = 0.54865881\n",
      "Iteration 1291, loss = 0.54842655\n",
      "Iteration 1292, loss = 0.54819426\n",
      "Iteration 1293, loss = 0.54796205\n",
      "Iteration 1294, loss = 0.54773034\n",
      "Iteration 1295, loss = 0.54749864\n",
      "Iteration 1296, loss = 0.54726715\n",
      "Iteration 1297, loss = 0.54703583\n",
      "Iteration 1298, loss = 0.54680606\n",
      "Iteration 1299, loss = 0.54657679\n",
      "Iteration 1300, loss = 0.54634767\n",
      "Iteration 1301, loss = 0.54611876\n",
      "Iteration 1302, loss = 0.54589017\n",
      "Iteration 1303, loss = 0.54566181\n",
      "Iteration 1304, loss = 0.54543392\n",
      "Iteration 1305, loss = 0.54520630\n",
      "Iteration 1306, loss = 0.54497902\n",
      "Iteration 1307, loss = 0.54475192\n",
      "Iteration 1308, loss = 0.54452524\n",
      "Iteration 1309, loss = 0.54429899\n",
      "Iteration 1310, loss = 0.54407302\n",
      "Iteration 1311, loss = 0.54384720\n",
      "Iteration 1312, loss = 0.54362165\n",
      "Iteration 1313, loss = 0.54339638\n",
      "Iteration 1314, loss = 0.54317163\n",
      "Iteration 1315, loss = 0.54294691\n",
      "Iteration 1316, loss = 0.54272263\n",
      "Iteration 1317, loss = 0.54249846\n",
      "Iteration 1318, loss = 0.54227445\n",
      "Iteration 1319, loss = 0.54205066\n",
      "Iteration 1320, loss = 0.54182741\n",
      "Iteration 1321, loss = 0.54160439\n",
      "Iteration 1322, loss = 0.54138175\n",
      "Iteration 1323, loss = 0.54115929\n",
      "Iteration 1324, loss = 0.54093711\n",
      "Iteration 1325, loss = 0.54071528\n",
      "Iteration 1326, loss = 0.54049369\n",
      "Iteration 1327, loss = 0.54027210\n",
      "Iteration 1328, loss = 0.54005040\n",
      "Iteration 1329, loss = 0.53982888\n",
      "Iteration 1330, loss = 0.53960784\n",
      "Iteration 1331, loss = 0.53938768\n",
      "Iteration 1332, loss = 0.53916785\n",
      "Iteration 1333, loss = 0.53894878\n",
      "Iteration 1334, loss = 0.53872991\n",
      "Iteration 1335, loss = 0.53851137\n",
      "Iteration 1336, loss = 0.53829294\n",
      "Iteration 1337, loss = 0.53807465\n",
      "Iteration 1338, loss = 0.53785672\n",
      "Iteration 1339, loss = 0.53763895\n",
      "Iteration 1340, loss = 0.53742138\n",
      "Iteration 1341, loss = 0.53720433\n",
      "Iteration 1342, loss = 0.53698735\n",
      "Iteration 1343, loss = 0.53677082\n",
      "Iteration 1344, loss = 0.53655441\n",
      "Iteration 1345, loss = 0.53633818\n",
      "Iteration 1346, loss = 0.53612223\n",
      "Iteration 1347, loss = 0.53590653\n",
      "Iteration 1348, loss = 0.53569097\n",
      "Iteration 1349, loss = 0.53547589\n",
      "Iteration 1350, loss = 0.53526113\n",
      "Iteration 1351, loss = 0.53504635\n",
      "Iteration 1352, loss = 0.53483188\n",
      "Iteration 1353, loss = 0.53461813\n",
      "Iteration 1354, loss = 0.53440424\n",
      "Iteration 1355, loss = 0.53419043\n",
      "Iteration 1356, loss = 0.53397712\n",
      "Iteration 1357, loss = 0.53376426\n",
      "Iteration 1358, loss = 0.53355148\n",
      "Iteration 1359, loss = 0.53333837\n",
      "Iteration 1360, loss = 0.53312565\n",
      "Iteration 1361, loss = 0.53291307\n",
      "Iteration 1362, loss = 0.53270086\n",
      "Iteration 1363, loss = 0.53248843\n",
      "Iteration 1364, loss = 0.53227605\n",
      "Iteration 1365, loss = 0.53206387\n",
      "Iteration 1366, loss = 0.53185132\n",
      "Iteration 1367, loss = 0.53163906\n",
      "Iteration 1368, loss = 0.53142689\n",
      "Iteration 1369, loss = 0.53121485\n",
      "Iteration 1370, loss = 0.53100288\n",
      "Iteration 1371, loss = 0.53079104\n",
      "Iteration 1372, loss = 0.53057936\n",
      "Iteration 1373, loss = 0.53036779\n",
      "Iteration 1374, loss = 0.53015647\n",
      "Iteration 1375, loss = 0.52994532\n",
      "Iteration 1376, loss = 0.52973432\n",
      "Iteration 1377, loss = 0.52952344\n",
      "Iteration 1378, loss = 0.52931283\n",
      "Iteration 1379, loss = 0.52910251\n",
      "Iteration 1380, loss = 0.52889223\n",
      "Iteration 1381, loss = 0.52868222\n",
      "Iteration 1382, loss = 0.52847253\n",
      "Iteration 1383, loss = 0.52826290\n",
      "Iteration 1384, loss = 0.52805345\n",
      "Iteration 1385, loss = 0.52784427\n",
      "Iteration 1386, loss = 0.52763541\n",
      "Iteration 1387, loss = 0.52742674\n",
      "Iteration 1388, loss = 0.52721809\n",
      "Iteration 1389, loss = 0.52700992\n",
      "Iteration 1390, loss = 0.52680188\n",
      "Iteration 1391, loss = 0.52659403\n",
      "Iteration 1392, loss = 0.52638606\n",
      "Iteration 1393, loss = 0.52617843\n",
      "Iteration 1394, loss = 0.52597054\n",
      "Iteration 1395, loss = 0.52576252\n",
      "Iteration 1396, loss = 0.52555505\n",
      "Iteration 1397, loss = 0.52534764\n",
      "Iteration 1398, loss = 0.52513891\n",
      "Iteration 1399, loss = 0.52492915\n",
      "Iteration 1400, loss = 0.52471896\n",
      "Iteration 1401, loss = 0.52450825\n",
      "Iteration 1402, loss = 0.52430120\n",
      "Iteration 1403, loss = 0.52409441\n",
      "Iteration 1404, loss = 0.52388798\n",
      "Iteration 1405, loss = 0.52368191\n",
      "Iteration 1406, loss = 0.52347579\n",
      "Iteration 1407, loss = 0.52327046\n",
      "Iteration 1408, loss = 0.52306545\n",
      "Iteration 1409, loss = 0.52285979\n",
      "Iteration 1410, loss = 0.52265420\n",
      "Iteration 1411, loss = 0.52244898\n",
      "Iteration 1412, loss = 0.52224352\n",
      "Iteration 1413, loss = 0.52203830\n",
      "Iteration 1414, loss = 0.52183333\n",
      "Iteration 1415, loss = 0.52162853\n",
      "Iteration 1416, loss = 0.52142408\n",
      "Iteration 1417, loss = 0.52122017\n",
      "Iteration 1418, loss = 0.52101623\n",
      "Iteration 1419, loss = 0.52081275\n",
      "Iteration 1420, loss = 0.52060956\n",
      "Iteration 1421, loss = 0.52040645\n",
      "Iteration 1422, loss = 0.52020338\n",
      "Iteration 1423, loss = 0.52000091\n",
      "Iteration 1424, loss = 0.51979855\n",
      "Iteration 1425, loss = 0.51959621\n",
      "Iteration 1426, loss = 0.51939414\n",
      "Iteration 1427, loss = 0.51919233\n",
      "Iteration 1428, loss = 0.51899082\n",
      "Iteration 1429, loss = 0.51878935\n",
      "Iteration 1430, loss = 0.51858794\n",
      "Iteration 1431, loss = 0.51838675\n",
      "Iteration 1432, loss = 0.51818532\n",
      "Iteration 1433, loss = 0.51798399\n",
      "Iteration 1434, loss = 0.51778298\n",
      "Iteration 1435, loss = 0.51758213\n",
      "Iteration 1436, loss = 0.51738098\n",
      "Iteration 1437, loss = 0.51717960\n",
      "Iteration 1438, loss = 0.51697851\n",
      "Iteration 1439, loss = 0.51677744\n",
      "Iteration 1440, loss = 0.51657643\n",
      "Iteration 1441, loss = 0.51637588\n",
      "Iteration 1442, loss = 0.51617534\n",
      "Iteration 1443, loss = 0.51597492\n",
      "Iteration 1444, loss = 0.51577436\n",
      "Iteration 1445, loss = 0.51557406\n",
      "Iteration 1446, loss = 0.51537409\n",
      "Iteration 1447, loss = 0.51517422\n",
      "Iteration 1448, loss = 0.51497431\n",
      "Iteration 1449, loss = 0.51477388\n",
      "Iteration 1450, loss = 0.51457365\n",
      "Iteration 1451, loss = 0.51437330\n",
      "Iteration 1452, loss = 0.51417302\n",
      "Iteration 1453, loss = 0.51397329\n",
      "Iteration 1454, loss = 0.51377368\n",
      "Iteration 1455, loss = 0.51357425\n",
      "Iteration 1456, loss = 0.51337481\n",
      "Iteration 1457, loss = 0.51317554\n",
      "Iteration 1458, loss = 0.51297616\n",
      "Iteration 1459, loss = 0.51277720\n",
      "Iteration 1460, loss = 0.51257933\n",
      "Iteration 1461, loss = 0.51238178\n",
      "Iteration 1462, loss = 0.51218464\n",
      "Iteration 1463, loss = 0.51198822\n",
      "Iteration 1464, loss = 0.51179179\n",
      "Iteration 1465, loss = 0.51159511\n",
      "Iteration 1466, loss = 0.51139786\n",
      "Iteration 1467, loss = 0.51120151\n",
      "Iteration 1468, loss = 0.51100496\n",
      "Iteration 1469, loss = 0.51080841\n",
      "Iteration 1470, loss = 0.51061202\n",
      "Iteration 1471, loss = 0.51041558\n",
      "Iteration 1472, loss = 0.51021906\n",
      "Iteration 1473, loss = 0.51002305\n",
      "Iteration 1474, loss = 0.50982733\n",
      "Iteration 1475, loss = 0.50963192\n",
      "Iteration 1476, loss = 0.50943701\n",
      "Iteration 1477, loss = 0.50924238\n",
      "Iteration 1478, loss = 0.50904764\n",
      "Iteration 1479, loss = 0.50885352\n",
      "Iteration 1480, loss = 0.50865960\n",
      "Iteration 1481, loss = 0.50846573\n",
      "Iteration 1482, loss = 0.50827202\n",
      "Iteration 1483, loss = 0.50807852\n",
      "Iteration 1484, loss = 0.50788500\n",
      "Iteration 1485, loss = 0.50769176\n",
      "Iteration 1486, loss = 0.50749909\n",
      "Iteration 1487, loss = 0.50730642\n",
      "Iteration 1488, loss = 0.50711384\n",
      "Iteration 1489, loss = 0.50692183\n",
      "Iteration 1490, loss = 0.50673046\n",
      "Iteration 1491, loss = 0.50653977\n",
      "Iteration 1492, loss = 0.50634938\n",
      "Iteration 1493, loss = 0.50615953\n",
      "Iteration 1494, loss = 0.50596972\n",
      "Iteration 1495, loss = 0.50578023\n",
      "Iteration 1496, loss = 0.50559086\n",
      "Iteration 1497, loss = 0.50540164\n",
      "Iteration 1498, loss = 0.50521248\n",
      "Iteration 1499, loss = 0.50502376\n",
      "Iteration 1500, loss = 0.50483487\n",
      "Iteration 1501, loss = 0.50464635\n",
      "Iteration 1502, loss = 0.50445792\n",
      "Iteration 1503, loss = 0.50426950\n",
      "Iteration 1504, loss = 0.50408153\n",
      "Iteration 1505, loss = 0.50389378\n",
      "Iteration 1506, loss = 0.50370657\n",
      "Iteration 1507, loss = 0.50351966\n",
      "Iteration 1508, loss = 0.50333247\n",
      "Iteration 1509, loss = 0.50314565\n",
      "Iteration 1510, loss = 0.50295930\n",
      "Iteration 1511, loss = 0.50277276\n",
      "Iteration 1512, loss = 0.50258627\n",
      "Iteration 1513, loss = 0.50240026\n",
      "Iteration 1514, loss = 0.50221477\n",
      "Iteration 1515, loss = 0.50202923\n",
      "Iteration 1516, loss = 0.50184385\n",
      "Iteration 1517, loss = 0.50165881\n",
      "Iteration 1518, loss = 0.50147372\n",
      "Iteration 1519, loss = 0.50128887\n",
      "Iteration 1520, loss = 0.50110427\n",
      "Iteration 1521, loss = 0.50091997\n",
      "Iteration 1522, loss = 0.50073585\n",
      "Iteration 1523, loss = 0.50055211\n",
      "Iteration 1524, loss = 0.50036847\n",
      "Iteration 1525, loss = 0.50018518\n",
      "Iteration 1526, loss = 0.50000181\n",
      "Iteration 1527, loss = 0.49981877\n",
      "Iteration 1528, loss = 0.49963607\n",
      "Iteration 1529, loss = 0.49945328\n",
      "Iteration 1530, loss = 0.49927117\n",
      "Iteration 1531, loss = 0.49908891\n",
      "Iteration 1532, loss = 0.49890690\n",
      "Iteration 1533, loss = 0.49872497\n",
      "Iteration 1534, loss = 0.49854327\n",
      "Iteration 1535, loss = 0.49836160\n",
      "Iteration 1536, loss = 0.49817992\n",
      "Iteration 1537, loss = 0.49799856\n",
      "Iteration 1538, loss = 0.49781746\n",
      "Iteration 1539, loss = 0.49763648\n",
      "Iteration 1540, loss = 0.49745560\n",
      "Iteration 1541, loss = 0.49727493\n",
      "Iteration 1542, loss = 0.49709434\n",
      "Iteration 1543, loss = 0.49691412\n",
      "Iteration 1544, loss = 0.49673388\n",
      "Iteration 1545, loss = 0.49655381\n",
      "Iteration 1546, loss = 0.49637385\n",
      "Iteration 1547, loss = 0.49619394\n",
      "Iteration 1548, loss = 0.49601444\n",
      "Iteration 1549, loss = 0.49583513\n",
      "Iteration 1550, loss = 0.49565581\n",
      "Iteration 1551, loss = 0.49547668\n",
      "Iteration 1552, loss = 0.49529773\n",
      "Iteration 1553, loss = 0.49511912\n",
      "Iteration 1554, loss = 0.49494060\n",
      "Iteration 1555, loss = 0.49476211\n",
      "Iteration 1556, loss = 0.49458404\n",
      "Iteration 1557, loss = 0.49440610\n",
      "Iteration 1558, loss = 0.49422825\n",
      "Iteration 1559, loss = 0.49405063\n",
      "Iteration 1560, loss = 0.49387318\n",
      "Iteration 1561, loss = 0.49369582\n",
      "Iteration 1562, loss = 0.49351866\n",
      "Iteration 1563, loss = 0.49334166\n",
      "Iteration 1564, loss = 0.49316508\n",
      "Iteration 1565, loss = 0.49298856\n",
      "Iteration 1566, loss = 0.49281209\n",
      "Iteration 1567, loss = 0.49263577\n",
      "Iteration 1568, loss = 0.49245975\n",
      "Iteration 1569, loss = 0.49228409\n",
      "Iteration 1570, loss = 0.49210830\n",
      "Iteration 1571, loss = 0.49193248\n",
      "Iteration 1572, loss = 0.49175685\n",
      "Iteration 1573, loss = 0.49158142\n",
      "Iteration 1574, loss = 0.49140636\n",
      "Iteration 1575, loss = 0.49123124\n",
      "Iteration 1576, loss = 0.49105619\n",
      "Iteration 1577, loss = 0.49088145\n",
      "Iteration 1578, loss = 0.49070680\n",
      "Iteration 1579, loss = 0.49053237\n",
      "Iteration 1580, loss = 0.49035811\n",
      "Iteration 1581, loss = 0.49018415\n",
      "Iteration 1582, loss = 0.49001012\n",
      "Iteration 1583, loss = 0.48983624\n",
      "Iteration 1584, loss = 0.48966268\n",
      "Iteration 1585, loss = 0.48948916\n",
      "Iteration 1586, loss = 0.48931589\n",
      "Iteration 1587, loss = 0.48914289\n",
      "Iteration 1588, loss = 0.48896997\n",
      "Iteration 1589, loss = 0.48879778\n",
      "Iteration 1590, loss = 0.48862590\n",
      "Iteration 1591, loss = 0.48845432\n",
      "Iteration 1592, loss = 0.48828268\n",
      "Iteration 1593, loss = 0.48811124\n",
      "Iteration 1594, loss = 0.48794012\n",
      "Iteration 1595, loss = 0.48776932\n",
      "Iteration 1596, loss = 0.48759861\n",
      "Iteration 1597, loss = 0.48742806\n",
      "Iteration 1598, loss = 0.48725758\n",
      "Iteration 1599, loss = 0.48708770\n",
      "Iteration 1600, loss = 0.48691776\n",
      "Iteration 1601, loss = 0.48674795\n",
      "Iteration 1602, loss = 0.48657812\n",
      "Iteration 1603, loss = 0.48640870\n",
      "Iteration 1604, loss = 0.48623966\n",
      "Iteration 1605, loss = 0.48607095\n",
      "Iteration 1606, loss = 0.48590215\n",
      "Iteration 1607, loss = 0.48573332\n",
      "Iteration 1608, loss = 0.48556467\n",
      "Iteration 1609, loss = 0.48539578\n",
      "Iteration 1610, loss = 0.48522667\n",
      "Iteration 1611, loss = 0.48505773\n",
      "Iteration 1612, loss = 0.48488871\n",
      "Iteration 1613, loss = 0.48472008\n",
      "Iteration 1614, loss = 0.48455147\n",
      "Iteration 1615, loss = 0.48438285\n",
      "Iteration 1616, loss = 0.48421442\n",
      "Iteration 1617, loss = 0.48404614\n",
      "Iteration 1618, loss = 0.48387796\n",
      "Iteration 1619, loss = 0.48370981\n",
      "Iteration 1620, loss = 0.48354194\n",
      "Iteration 1621, loss = 0.48337414\n",
      "Iteration 1622, loss = 0.48320652\n",
      "Iteration 1623, loss = 0.48303906\n",
      "Iteration 1624, loss = 0.48287169\n",
      "Iteration 1625, loss = 0.48270444\n",
      "Iteration 1626, loss = 0.48253716\n",
      "Iteration 1627, loss = 0.48237002\n",
      "Iteration 1628, loss = 0.48220308\n",
      "Iteration 1629, loss = 0.48203641\n",
      "Iteration 1630, loss = 0.48186989\n",
      "Iteration 1631, loss = 0.48170352\n",
      "Iteration 1632, loss = 0.48153734\n",
      "Iteration 1633, loss = 0.48137111\n",
      "Iteration 1634, loss = 0.48120508\n",
      "Iteration 1635, loss = 0.48103948\n",
      "Iteration 1636, loss = 0.48087410\n",
      "Iteration 1637, loss = 0.48070870\n",
      "Iteration 1638, loss = 0.48054365\n",
      "Iteration 1639, loss = 0.48037864\n",
      "Iteration 1640, loss = 0.48021385\n",
      "Iteration 1641, loss = 0.48004929\n",
      "Iteration 1642, loss = 0.47988481\n",
      "Iteration 1643, loss = 0.47972048\n",
      "Iteration 1644, loss = 0.47955652\n",
      "Iteration 1645, loss = 0.47939332\n",
      "Iteration 1646, loss = 0.47922995\n",
      "Iteration 1647, loss = 0.47906694\n",
      "Iteration 1648, loss = 0.47890406\n",
      "Iteration 1649, loss = 0.47874130\n",
      "Iteration 1650, loss = 0.47857859\n",
      "Iteration 1651, loss = 0.47841596\n",
      "Iteration 1652, loss = 0.47825355\n",
      "Iteration 1653, loss = 0.47809113\n",
      "Iteration 1654, loss = 0.47792885\n",
      "Iteration 1655, loss = 0.47776692\n",
      "Iteration 1656, loss = 0.47760540\n",
      "Iteration 1657, loss = 0.47744384\n",
      "Iteration 1658, loss = 0.47728248\n",
      "Iteration 1659, loss = 0.47712118\n",
      "Iteration 1660, loss = 0.47696030\n",
      "Iteration 1661, loss = 0.47679945\n",
      "Iteration 1662, loss = 0.47663877\n",
      "Iteration 1663, loss = 0.47647822\n",
      "Iteration 1664, loss = 0.47631793\n",
      "Iteration 1665, loss = 0.47615790\n",
      "Iteration 1666, loss = 0.47599786\n",
      "Iteration 1667, loss = 0.47583808\n",
      "Iteration 1668, loss = 0.47567858\n",
      "Iteration 1669, loss = 0.47551928\n",
      "Iteration 1670, loss = 0.47535994\n",
      "Iteration 1671, loss = 0.47520086\n",
      "Iteration 1672, loss = 0.47504220\n",
      "Iteration 1673, loss = 0.47488353\n",
      "Iteration 1674, loss = 0.47472498\n",
      "Iteration 1675, loss = 0.47456668\n",
      "Iteration 1676, loss = 0.47440858\n",
      "Iteration 1677, loss = 0.47425055\n",
      "Iteration 1678, loss = 0.47409283\n",
      "Iteration 1679, loss = 0.47393497\n",
      "Iteration 1680, loss = 0.47377757\n",
      "Iteration 1681, loss = 0.47362043\n",
      "Iteration 1682, loss = 0.47346328\n",
      "Iteration 1683, loss = 0.47330633\n",
      "Iteration 1684, loss = 0.47314945\n",
      "Iteration 1685, loss = 0.47299285\n",
      "Iteration 1686, loss = 0.47283642\n",
      "Iteration 1687, loss = 0.47268016\n",
      "Iteration 1688, loss = 0.47252387\n",
      "Iteration 1689, loss = 0.47236741\n",
      "Iteration 1690, loss = 0.47221095\n",
      "Iteration 1691, loss = 0.47205451\n",
      "Iteration 1692, loss = 0.47189852\n",
      "Iteration 1693, loss = 0.47174280\n",
      "Iteration 1694, loss = 0.47158721\n",
      "Iteration 1695, loss = 0.47143180\n",
      "Iteration 1696, loss = 0.47127644\n",
      "Iteration 1697, loss = 0.47112105\n",
      "Iteration 1698, loss = 0.47096571\n",
      "Iteration 1699, loss = 0.47081088\n",
      "Iteration 1700, loss = 0.47065610\n",
      "Iteration 1701, loss = 0.47050142\n",
      "Iteration 1702, loss = 0.47034676\n",
      "Iteration 1703, loss = 0.47019248\n",
      "Iteration 1704, loss = 0.47003831\n",
      "Iteration 1705, loss = 0.46988429\n",
      "Iteration 1706, loss = 0.46973038\n",
      "Iteration 1707, loss = 0.46957665\n",
      "Iteration 1708, loss = 0.46942302\n",
      "Iteration 1709, loss = 0.46926963\n",
      "Iteration 1710, loss = 0.46911658\n",
      "Iteration 1711, loss = 0.46896350\n",
      "Iteration 1712, loss = 0.46881051\n",
      "Iteration 1713, loss = 0.46865754\n",
      "Iteration 1714, loss = 0.46850510\n",
      "Iteration 1715, loss = 0.46835265\n",
      "Iteration 1716, loss = 0.46820009\n",
      "Iteration 1717, loss = 0.46804752\n",
      "Iteration 1718, loss = 0.46789514\n",
      "Iteration 1719, loss = 0.46774313\n",
      "Iteration 1720, loss = 0.46759149\n",
      "Iteration 1721, loss = 0.46744010\n",
      "Iteration 1722, loss = 0.46728874\n",
      "Iteration 1723, loss = 0.46713745\n",
      "Iteration 1724, loss = 0.46698639\n",
      "Iteration 1725, loss = 0.46683562\n",
      "Iteration 1726, loss = 0.46668429\n",
      "Iteration 1727, loss = 0.46653281\n",
      "Iteration 1728, loss = 0.46638132\n",
      "Iteration 1729, loss = 0.46622980\n",
      "Iteration 1730, loss = 0.46607835\n",
      "Iteration 1731, loss = 0.46592702\n",
      "Iteration 1732, loss = 0.46577562\n",
      "Iteration 1733, loss = 0.46562441\n",
      "Iteration 1734, loss = 0.46547345\n",
      "Iteration 1735, loss = 0.46532277\n",
      "Iteration 1736, loss = 0.46517234\n",
      "Iteration 1737, loss = 0.46502224\n",
      "Iteration 1738, loss = 0.46487219\n",
      "Iteration 1739, loss = 0.46472217\n",
      "Iteration 1740, loss = 0.46457245\n",
      "Iteration 1741, loss = 0.46442284\n",
      "Iteration 1742, loss = 0.46427333\n",
      "Iteration 1743, loss = 0.46412358\n",
      "Iteration 1744, loss = 0.46397454\n",
      "Iteration 1745, loss = 0.46382568\n",
      "Iteration 1746, loss = 0.46367660\n",
      "Iteration 1747, loss = 0.46352755\n",
      "Iteration 1748, loss = 0.46337857\n",
      "Iteration 1749, loss = 0.46323001\n",
      "Iteration 1750, loss = 0.46308187\n",
      "Iteration 1751, loss = 0.46293367\n",
      "Iteration 1752, loss = 0.46278546\n",
      "Iteration 1753, loss = 0.46263748\n",
      "Iteration 1754, loss = 0.46248987\n",
      "Iteration 1755, loss = 0.46234251\n",
      "Iteration 1756, loss = 0.46219529\n",
      "Iteration 1757, loss = 0.46204824\n",
      "Iteration 1758, loss = 0.46190124\n",
      "Iteration 1759, loss = 0.46175418\n",
      "Iteration 1760, loss = 0.46160703\n",
      "Iteration 1761, loss = 0.46146007\n",
      "Iteration 1762, loss = 0.46131323\n",
      "Iteration 1763, loss = 0.46116655\n",
      "Iteration 1764, loss = 0.46102000\n",
      "Iteration 1765, loss = 0.46087356\n",
      "Iteration 1766, loss = 0.46072722\n",
      "Iteration 1767, loss = 0.46058115\n",
      "Iteration 1768, loss = 0.46043504\n",
      "Iteration 1769, loss = 0.46028904\n",
      "Iteration 1770, loss = 0.46014351\n",
      "Iteration 1771, loss = 0.45999811\n",
      "Iteration 1772, loss = 0.45985269\n",
      "Iteration 1773, loss = 0.45970782\n",
      "Iteration 1774, loss = 0.45956330\n",
      "Iteration 1775, loss = 0.45941894\n",
      "Iteration 1776, loss = 0.45927479\n",
      "Iteration 1777, loss = 0.45913050\n",
      "Iteration 1778, loss = 0.45898639\n",
      "Iteration 1779, loss = 0.45884232\n",
      "Iteration 1780, loss = 0.45869842\n",
      "Iteration 1781, loss = 0.45855456\n",
      "Iteration 1782, loss = 0.45841062\n",
      "Iteration 1783, loss = 0.45826691\n",
      "Iteration 1784, loss = 0.45812349\n",
      "Iteration 1785, loss = 0.45798009\n",
      "Iteration 1786, loss = 0.45783683\n",
      "Iteration 1787, loss = 0.45769364\n",
      "Iteration 1788, loss = 0.45755073\n",
      "Iteration 1789, loss = 0.45740839\n",
      "Iteration 1790, loss = 0.45726607\n",
      "Iteration 1791, loss = 0.45712363\n",
      "Iteration 1792, loss = 0.45698140\n",
      "Iteration 1793, loss = 0.45683910\n",
      "Iteration 1794, loss = 0.45669709\n",
      "Iteration 1795, loss = 0.45655520\n",
      "Iteration 1796, loss = 0.45641319\n",
      "Iteration 1797, loss = 0.45627153\n",
      "Iteration 1798, loss = 0.45613014\n",
      "Iteration 1799, loss = 0.45598882\n",
      "Iteration 1800, loss = 0.45584758\n",
      "Iteration 1801, loss = 0.45570674\n",
      "Iteration 1802, loss = 0.45556586\n",
      "Iteration 1803, loss = 0.45542508\n",
      "Iteration 1804, loss = 0.45528435\n",
      "Iteration 1805, loss = 0.45514382\n",
      "Iteration 1806, loss = 0.45500388\n",
      "Iteration 1807, loss = 0.45486384\n",
      "Iteration 1808, loss = 0.45472380\n",
      "Iteration 1809, loss = 0.45458377\n",
      "Iteration 1810, loss = 0.45444391\n",
      "Iteration 1811, loss = 0.45430454\n",
      "Iteration 1812, loss = 0.45416518\n",
      "Iteration 1813, loss = 0.45402596\n",
      "Iteration 1814, loss = 0.45388695\n",
      "Iteration 1815, loss = 0.45374784\n",
      "Iteration 1816, loss = 0.45360889\n",
      "Iteration 1817, loss = 0.45347042\n",
      "Iteration 1818, loss = 0.45333179\n",
      "Iteration 1819, loss = 0.45319342\n",
      "Iteration 1820, loss = 0.45305527\n",
      "Iteration 1821, loss = 0.45291710\n",
      "Iteration 1822, loss = 0.45277916\n",
      "Iteration 1823, loss = 0.45264143\n",
      "Iteration 1824, loss = 0.45250364\n",
      "Iteration 1825, loss = 0.45236607\n",
      "Iteration 1826, loss = 0.45222867\n",
      "Iteration 1827, loss = 0.45209163\n",
      "Iteration 1828, loss = 0.45195447\n",
      "Iteration 1829, loss = 0.45181720\n",
      "Iteration 1830, loss = 0.45168055\n",
      "Iteration 1831, loss = 0.45154430\n",
      "Iteration 1832, loss = 0.45140811\n",
      "Iteration 1833, loss = 0.45127193\n",
      "Iteration 1834, loss = 0.45113576\n",
      "Iteration 1835, loss = 0.45100001\n",
      "Iteration 1836, loss = 0.45086439\n",
      "Iteration 1837, loss = 0.45072890\n",
      "Iteration 1838, loss = 0.45059331\n",
      "Iteration 1839, loss = 0.45045789\n",
      "Iteration 1840, loss = 0.45032281\n",
      "Iteration 1841, loss = 0.45018620\n",
      "Iteration 1842, loss = 0.45004926\n",
      "Iteration 1843, loss = 0.44991444\n",
      "Iteration 1844, loss = 0.44978007\n",
      "Iteration 1845, loss = 0.44964633\n",
      "Iteration 1846, loss = 0.44951275\n",
      "Iteration 1847, loss = 0.44937929\n",
      "Iteration 1848, loss = 0.44924600\n",
      "Iteration 1849, loss = 0.44911288\n",
      "Iteration 1850, loss = 0.44898006\n",
      "Iteration 1851, loss = 0.44884746\n",
      "Iteration 1852, loss = 0.44871504\n",
      "Iteration 1853, loss = 0.44858301\n",
      "Iteration 1854, loss = 0.44845009\n",
      "Iteration 1855, loss = 0.44831847\n",
      "Iteration 1856, loss = 0.44818691\n",
      "Iteration 1857, loss = 0.44805555\n",
      "Iteration 1858, loss = 0.44792409\n",
      "Iteration 1859, loss = 0.44779279\n",
      "Iteration 1860, loss = 0.44766140\n",
      "Iteration 1861, loss = 0.44753060\n",
      "Iteration 1862, loss = 0.44739988\n",
      "Iteration 1863, loss = 0.44726895\n",
      "Iteration 1864, loss = 0.44713817\n",
      "Iteration 1865, loss = 0.44700812\n",
      "Iteration 1866, loss = 0.44687780\n",
      "Iteration 1867, loss = 0.44674742\n",
      "Iteration 1868, loss = 0.44661702\n",
      "Iteration 1869, loss = 0.44648675\n",
      "Iteration 1870, loss = 0.44635608\n",
      "Iteration 1871, loss = 0.44622382\n",
      "Iteration 1872, loss = 0.44609136\n",
      "Iteration 1873, loss = 0.44595877\n",
      "Iteration 1874, loss = 0.44582777\n",
      "Iteration 1875, loss = 0.44569654\n",
      "Iteration 1876, loss = 0.44556342\n",
      "Iteration 1877, loss = 0.44542882\n",
      "Iteration 1878, loss = 0.44529473\n",
      "Iteration 1879, loss = 0.44516204\n",
      "Iteration 1880, loss = 0.44502892\n",
      "Iteration 1881, loss = 0.44489382\n",
      "Iteration 1882, loss = 0.44475869\n",
      "Iteration 1883, loss = 0.44462337\n",
      "Iteration 1884, loss = 0.44448789\n",
      "Iteration 1885, loss = 0.44435195\n",
      "Iteration 1886, loss = 0.44421608\n",
      "Iteration 1887, loss = 0.44408017\n",
      "Iteration 1888, loss = 0.44394415\n",
      "Iteration 1889, loss = 0.44381064\n",
      "Iteration 1890, loss = 0.44367851\n",
      "Iteration 1891, loss = 0.44354683\n",
      "Iteration 1892, loss = 0.44341524\n",
      "Iteration 1893, loss = 0.44328389\n",
      "Iteration 1894, loss = 0.44315275\n",
      "Iteration 1895, loss = 0.44302195\n",
      "Iteration 1896, loss = 0.44289141\n",
      "Iteration 1897, loss = 0.44276079\n",
      "Iteration 1898, loss = 0.44263049\n",
      "Iteration 1899, loss = 0.44250034\n",
      "Iteration 1900, loss = 0.44237050\n",
      "Iteration 1901, loss = 0.44224082\n",
      "Iteration 1902, loss = 0.44211148\n",
      "Iteration 1903, loss = 0.44198273\n",
      "Iteration 1904, loss = 0.44185402\n",
      "Iteration 1905, loss = 0.44172543\n",
      "Iteration 1906, loss = 0.44159710\n",
      "Iteration 1907, loss = 0.44146899\n",
      "Iteration 1908, loss = 0.44134094\n",
      "Iteration 1909, loss = 0.44121334\n",
      "Iteration 1910, loss = 0.44108579\n",
      "Iteration 1911, loss = 0.44095831\n",
      "Iteration 1912, loss = 0.44083117\n",
      "Iteration 1913, loss = 0.44070410\n",
      "Iteration 1914, loss = 0.44057720\n",
      "Iteration 1915, loss = 0.44045043\n",
      "Iteration 1916, loss = 0.44032381\n",
      "Iteration 1917, loss = 0.44019730\n",
      "Iteration 1918, loss = 0.44007092\n",
      "Iteration 1919, loss = 0.43994465\n",
      "Iteration 1920, loss = 0.43981869\n",
      "Iteration 1921, loss = 0.43969282\n",
      "Iteration 1922, loss = 0.43956729\n",
      "Iteration 1923, loss = 0.43944314\n",
      "Iteration 1924, loss = 0.43931912\n",
      "Iteration 1925, loss = 0.43919516\n",
      "Iteration 1926, loss = 0.43907148\n",
      "Iteration 1927, loss = 0.43894774\n",
      "Iteration 1928, loss = 0.43882425\n",
      "Iteration 1929, loss = 0.43870067\n",
      "Iteration 1930, loss = 0.43857751\n",
      "Iteration 1931, loss = 0.43845434\n",
      "Iteration 1932, loss = 0.43833117\n",
      "Iteration 1933, loss = 0.43820839\n",
      "Iteration 1934, loss = 0.43808563\n",
      "Iteration 1935, loss = 0.43796296\n",
      "Iteration 1936, loss = 0.43784027\n",
      "Iteration 1937, loss = 0.43771790\n",
      "Iteration 1938, loss = 0.43759578\n",
      "Iteration 1939, loss = 0.43747353\n",
      "Iteration 1940, loss = 0.43735171\n",
      "Iteration 1941, loss = 0.43723006\n",
      "Iteration 1942, loss = 0.43710835\n",
      "Iteration 1943, loss = 0.43698673\n",
      "Iteration 1944, loss = 0.43686534\n",
      "Iteration 1945, loss = 0.43674420\n",
      "Iteration 1946, loss = 0.43662289\n",
      "Iteration 1947, loss = 0.43650192\n",
      "Iteration 1948, loss = 0.43638099\n",
      "Iteration 1949, loss = 0.43626016\n",
      "Iteration 1950, loss = 0.43613948\n",
      "Iteration 1951, loss = 0.43601881\n",
      "Iteration 1952, loss = 0.43589848\n",
      "Iteration 1953, loss = 0.43577819\n",
      "Iteration 1954, loss = 0.43565801\n",
      "Iteration 1955, loss = 0.43553786\n",
      "Iteration 1956, loss = 0.43541784\n",
      "Iteration 1957, loss = 0.43529799\n",
      "Iteration 1958, loss = 0.43517828\n",
      "Iteration 1959, loss = 0.43505868\n",
      "Iteration 1960, loss = 0.43493927\n",
      "Iteration 1961, loss = 0.43482002\n",
      "Iteration 1962, loss = 0.43470064\n",
      "Iteration 1963, loss = 0.43458155\n",
      "Iteration 1964, loss = 0.43446256\n",
      "Iteration 1965, loss = 0.43434384\n",
      "Iteration 1966, loss = 0.43422504\n",
      "Iteration 1967, loss = 0.43410634\n",
      "Iteration 1968, loss = 0.43398801\n",
      "Iteration 1969, loss = 0.43386961\n",
      "Iteration 1970, loss = 0.43375147\n",
      "Iteration 1971, loss = 0.43363350\n",
      "Iteration 1972, loss = 0.43351547\n",
      "Iteration 1973, loss = 0.43339763\n",
      "Iteration 1974, loss = 0.43328005\n",
      "Iteration 1975, loss = 0.43316252\n",
      "Iteration 1976, loss = 0.43304493\n",
      "Iteration 1977, loss = 0.43292770\n",
      "Iteration 1978, loss = 0.43281069\n",
      "Iteration 1979, loss = 0.43269387\n",
      "Iteration 1980, loss = 0.43257705\n",
      "Iteration 1981, loss = 0.43246004\n",
      "Iteration 1982, loss = 0.43234348\n",
      "Iteration 1983, loss = 0.43222703\n",
      "Iteration 1984, loss = 0.43211050\n",
      "Iteration 1985, loss = 0.43199421\n",
      "Iteration 1986, loss = 0.43187812\n",
      "Iteration 1987, loss = 0.43176214\n",
      "Iteration 1988, loss = 0.43164614\n",
      "Iteration 1989, loss = 0.43153038\n",
      "Iteration 1990, loss = 0.43141484\n",
      "Iteration 1991, loss = 0.43129920\n",
      "Iteration 1992, loss = 0.43118377\n",
      "Iteration 1993, loss = 0.43106831\n",
      "Iteration 1994, loss = 0.43095307\n",
      "Iteration 1995, loss = 0.43083787\n",
      "Iteration 1996, loss = 0.43072296\n",
      "Iteration 1997, loss = 0.43060821\n",
      "Iteration 1998, loss = 0.43049347\n",
      "Iteration 1999, loss = 0.43037893\n",
      "Iteration 2000, loss = 0.43026449\n",
      "Iteration 2001, loss = 0.43015035\n",
      "Iteration 2002, loss = 0.43003607\n",
      "Iteration 2003, loss = 0.42992192\n",
      "Iteration 2004, loss = 0.42980794\n",
      "Iteration 2005, loss = 0.42969411\n",
      "Iteration 2006, loss = 0.42958041\n",
      "Iteration 2007, loss = 0.42946673\n",
      "Iteration 2008, loss = 0.42935311\n",
      "Iteration 2009, loss = 0.42923958\n",
      "Iteration 2010, loss = 0.42912643\n",
      "Iteration 2011, loss = 0.42901340\n",
      "Iteration 2012, loss = 0.42890034\n",
      "Iteration 2013, loss = 0.42878758\n",
      "Iteration 2014, loss = 0.42867476\n",
      "Iteration 2015, loss = 0.42856191\n",
      "Iteration 2016, loss = 0.42844941\n",
      "Iteration 2017, loss = 0.42833697\n",
      "Iteration 2018, loss = 0.42822456\n",
      "Iteration 2019, loss = 0.42811238\n",
      "Iteration 2020, loss = 0.42800019\n",
      "Iteration 2021, loss = 0.42788819\n",
      "Iteration 2022, loss = 0.42777647\n",
      "Iteration 2023, loss = 0.42766474\n",
      "Iteration 2024, loss = 0.42755299\n",
      "Iteration 2025, loss = 0.42744142\n",
      "Iteration 2026, loss = 0.42732999\n",
      "Iteration 2027, loss = 0.42721858\n",
      "Iteration 2028, loss = 0.42710739\n",
      "Iteration 2029, loss = 0.42699605\n",
      "Iteration 2030, loss = 0.42688514\n",
      "Iteration 2031, loss = 0.42677435\n",
      "Iteration 2032, loss = 0.42666370\n",
      "Iteration 2033, loss = 0.42655296\n",
      "Iteration 2034, loss = 0.42644241\n",
      "Iteration 2035, loss = 0.42633195\n",
      "Iteration 2036, loss = 0.42622169\n",
      "Iteration 2037, loss = 0.42611154\n",
      "Iteration 2038, loss = 0.42600161\n",
      "Iteration 2039, loss = 0.42589147\n",
      "Iteration 2040, loss = 0.42578114\n",
      "Iteration 2041, loss = 0.42567106\n",
      "Iteration 2042, loss = 0.42556124\n",
      "Iteration 2043, loss = 0.42545172\n",
      "Iteration 2044, loss = 0.42534224\n",
      "Iteration 2045, loss = 0.42523255\n",
      "Iteration 2046, loss = 0.42512323\n",
      "Iteration 2047, loss = 0.42501384\n",
      "Iteration 2048, loss = 0.42490471\n",
      "Iteration 2049, loss = 0.42479544\n",
      "Iteration 2050, loss = 0.42468648\n",
      "Iteration 2051, loss = 0.42457747\n",
      "Iteration 2052, loss = 0.42446848\n",
      "Iteration 2053, loss = 0.42435984\n",
      "Iteration 2054, loss = 0.42425100\n",
      "Iteration 2055, loss = 0.42414233\n",
      "Iteration 2056, loss = 0.42403368\n",
      "Iteration 2057, loss = 0.42392550\n",
      "Iteration 2058, loss = 0.42381695\n",
      "Iteration 2059, loss = 0.42370862\n",
      "Iteration 2060, loss = 0.42360050\n",
      "Iteration 2061, loss = 0.42349254\n",
      "Iteration 2062, loss = 0.42338467\n",
      "Iteration 2063, loss = 0.42327685\n",
      "Iteration 2064, loss = 0.42316910\n",
      "Iteration 2065, loss = 0.42306153\n",
      "Iteration 2066, loss = 0.42295397\n",
      "Iteration 2067, loss = 0.42284664\n",
      "Iteration 2068, loss = 0.42273945\n",
      "Iteration 2069, loss = 0.42263221\n",
      "Iteration 2070, loss = 0.42252525\n",
      "Iteration 2071, loss = 0.42241828\n",
      "Iteration 2072, loss = 0.42231157\n",
      "Iteration 2073, loss = 0.42220488\n",
      "Iteration 2074, loss = 0.42209827\n",
      "Iteration 2075, loss = 0.42199175\n",
      "Iteration 2076, loss = 0.42188545\n",
      "Iteration 2077, loss = 0.42177915\n",
      "Iteration 2078, loss = 0.42167291\n",
      "Iteration 2079, loss = 0.42156679\n",
      "Iteration 2080, loss = 0.42146093\n",
      "Iteration 2081, loss = 0.42135513\n",
      "Iteration 2082, loss = 0.42124936\n",
      "Iteration 2083, loss = 0.42114362\n",
      "Iteration 2084, loss = 0.42103795\n",
      "Iteration 2085, loss = 0.42093253\n",
      "Iteration 2086, loss = 0.42082721\n",
      "Iteration 2087, loss = 0.42072179\n",
      "Iteration 2088, loss = 0.42061659\n",
      "Iteration 2089, loss = 0.42051162\n",
      "Iteration 2090, loss = 0.42040664\n",
      "Iteration 2091, loss = 0.42030175\n",
      "Iteration 2092, loss = 0.42019698\n",
      "Iteration 2093, loss = 0.42009241\n",
      "Iteration 2094, loss = 0.41998771\n",
      "Iteration 2095, loss = 0.41988352\n",
      "Iteration 2096, loss = 0.41977909\n",
      "Iteration 2097, loss = 0.41967498\n",
      "Iteration 2098, loss = 0.41957075\n",
      "Iteration 2099, loss = 0.41946654\n",
      "Iteration 2100, loss = 0.41936289\n",
      "Iteration 2101, loss = 0.41925907\n",
      "Iteration 2102, loss = 0.41915515\n",
      "Iteration 2103, loss = 0.41905146\n",
      "Iteration 2104, loss = 0.41894811\n",
      "Iteration 2105, loss = 0.41884426\n",
      "Iteration 2106, loss = 0.41874012\n",
      "Iteration 2107, loss = 0.41863587\n",
      "Iteration 2108, loss = 0.41853158\n",
      "Iteration 2109, loss = 0.41842720\n",
      "Iteration 2110, loss = 0.41832273\n",
      "Iteration 2111, loss = 0.41821820\n",
      "Iteration 2112, loss = 0.41811363\n",
      "Iteration 2113, loss = 0.41800916\n",
      "Iteration 2114, loss = 0.41790462\n",
      "Iteration 2115, loss = 0.41780006\n",
      "Iteration 2116, loss = 0.41769540\n",
      "Iteration 2117, loss = 0.41759082\n",
      "Iteration 2118, loss = 0.41748660\n",
      "Iteration 2119, loss = 0.41738223\n",
      "Iteration 2120, loss = 0.41727778\n",
      "Iteration 2121, loss = 0.41717325\n",
      "Iteration 2122, loss = 0.41706996\n",
      "Iteration 2123, loss = 0.41696782\n",
      "Iteration 2124, loss = 0.41686568\n",
      "Iteration 2125, loss = 0.41676378\n",
      "Iteration 2126, loss = 0.41666188\n",
      "Iteration 2127, loss = 0.41656011\n",
      "Iteration 2128, loss = 0.41645856\n",
      "Iteration 2129, loss = 0.41635679\n",
      "Iteration 2130, loss = 0.41625510\n",
      "Iteration 2131, loss = 0.41615347\n",
      "Iteration 2132, loss = 0.41605193\n",
      "Iteration 2133, loss = 0.41595011\n",
      "Iteration 2134, loss = 0.41584729\n",
      "Iteration 2135, loss = 0.41574424\n",
      "Iteration 2136, loss = 0.41564191\n",
      "Iteration 2137, loss = 0.41553998\n",
      "Iteration 2138, loss = 0.41543854\n",
      "Iteration 2139, loss = 0.41533766\n",
      "Iteration 2140, loss = 0.41523675\n",
      "Iteration 2141, loss = 0.41513666\n",
      "Iteration 2142, loss = 0.41503678\n",
      "Iteration 2143, loss = 0.41493720\n",
      "Iteration 2144, loss = 0.41483741\n",
      "Iteration 2145, loss = 0.41473788\n",
      "Iteration 2146, loss = 0.41463834\n",
      "Iteration 2147, loss = 0.41453887\n",
      "Iteration 2148, loss = 0.41443924\n",
      "Iteration 2149, loss = 0.41433940\n",
      "Iteration 2150, loss = 0.41423941\n",
      "Iteration 2151, loss = 0.41413942\n",
      "Iteration 2152, loss = 0.41403973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32511651\n",
      "Iteration 2, loss = 1.32289374\n",
      "Iteration 3, loss = 1.32068688\n",
      "Iteration 4, loss = 1.31849136\n",
      "Iteration 5, loss = 1.31630836\n",
      "Iteration 6, loss = 1.31413153\n",
      "Iteration 7, loss = 1.31195712\n",
      "Iteration 8, loss = 1.30978864\n",
      "Iteration 9, loss = 1.30762960\n",
      "Iteration 10, loss = 1.30547897\n",
      "Iteration 11, loss = 1.30333712\n",
      "Iteration 12, loss = 1.30120877\n",
      "Iteration 13, loss = 1.29908822\n",
      "Iteration 14, loss = 1.29697440\n",
      "Iteration 15, loss = 1.29486797\n",
      "Iteration 16, loss = 1.29277271\n",
      "Iteration 17, loss = 1.29068933\n",
      "Iteration 18, loss = 1.28861909\n",
      "Iteration 19, loss = 1.28655883\n",
      "Iteration 20, loss = 1.28450815\n",
      "Iteration 21, loss = 1.28246752\n",
      "Iteration 22, loss = 1.28043912\n",
      "Iteration 23, loss = 1.27842748\n",
      "Iteration 24, loss = 1.27642689\n",
      "Iteration 25, loss = 1.27443591\n",
      "Iteration 26, loss = 1.27245422\n",
      "Iteration 27, loss = 1.27047091\n",
      "Iteration 28, loss = 1.26849452\n",
      "Iteration 29, loss = 1.26654966\n",
      "Iteration 30, loss = 1.26462018\n",
      "Iteration 31, loss = 1.26270155\n",
      "Iteration 32, loss = 1.26080018\n",
      "Iteration 33, loss = 1.25891842\n",
      "Iteration 34, loss = 1.25704438\n",
      "Iteration 35, loss = 1.25518128\n",
      "Iteration 36, loss = 1.25332853\n",
      "Iteration 37, loss = 1.25148713\n",
      "Iteration 38, loss = 1.24965487\n",
      "Iteration 39, loss = 1.24783221\n",
      "Iteration 40, loss = 1.24601716\n",
      "Iteration 41, loss = 1.24419754\n",
      "Iteration 42, loss = 1.24236792\n",
      "Iteration 43, loss = 1.24054190\n",
      "Iteration 44, loss = 1.23872846\n",
      "Iteration 45, loss = 1.23692805\n",
      "Iteration 46, loss = 1.23513547\n",
      "Iteration 47, loss = 1.23333817\n",
      "Iteration 48, loss = 1.23153573\n",
      "Iteration 49, loss = 1.22973471\n",
      "Iteration 50, loss = 1.22793768\n",
      "Iteration 51, loss = 1.22615758\n",
      "Iteration 52, loss = 1.22438097\n",
      "Iteration 53, loss = 1.22260512\n",
      "Iteration 54, loss = 1.22083407\n",
      "Iteration 55, loss = 1.21907736\n",
      "Iteration 56, loss = 1.21732796\n",
      "Iteration 57, loss = 1.21558857\n",
      "Iteration 58, loss = 1.21386447\n",
      "Iteration 59, loss = 1.21214700\n",
      "Iteration 60, loss = 1.21043698\n",
      "Iteration 61, loss = 1.20873669\n",
      "Iteration 62, loss = 1.20704521\n",
      "Iteration 63, loss = 1.20536314\n",
      "Iteration 64, loss = 1.20371309\n",
      "Iteration 65, loss = 1.20208214\n",
      "Iteration 66, loss = 1.20045398\n",
      "Iteration 67, loss = 1.19883051\n",
      "Iteration 68, loss = 1.19722884\n",
      "Iteration 69, loss = 1.19564020\n",
      "Iteration 70, loss = 1.19405892\n",
      "Iteration 71, loss = 1.19248656\n",
      "Iteration 72, loss = 1.19092351\n",
      "Iteration 73, loss = 1.18937246\n",
      "Iteration 74, loss = 1.18783356\n",
      "Iteration 75, loss = 1.18630348\n",
      "Iteration 76, loss = 1.18478454\n",
      "Iteration 77, loss = 1.18327736\n",
      "Iteration 78, loss = 1.18178106\n",
      "Iteration 79, loss = 1.18029132\n",
      "Iteration 80, loss = 1.17880860\n",
      "Iteration 81, loss = 1.17733398\n",
      "Iteration 82, loss = 1.17586879\n",
      "Iteration 83, loss = 1.17441747\n",
      "Iteration 84, loss = 1.17297635\n",
      "Iteration 85, loss = 1.17153663\n",
      "Iteration 86, loss = 1.17010323\n",
      "Iteration 87, loss = 1.16867726\n",
      "Iteration 88, loss = 1.16725310\n",
      "Iteration 89, loss = 1.16583652\n",
      "Iteration 90, loss = 1.16443207\n",
      "Iteration 91, loss = 1.16303411\n",
      "Iteration 92, loss = 1.16164376\n",
      "Iteration 93, loss = 1.16026217\n",
      "Iteration 94, loss = 1.15888601\n",
      "Iteration 95, loss = 1.15751837\n",
      "Iteration 96, loss = 1.15615196\n",
      "Iteration 97, loss = 1.15478543\n",
      "Iteration 98, loss = 1.15342778\n",
      "Iteration 99, loss = 1.15208041\n",
      "Iteration 100, loss = 1.15074301\n",
      "Iteration 101, loss = 1.14941118\n",
      "Iteration 102, loss = 1.14808427\n",
      "Iteration 103, loss = 1.14676388\n",
      "Iteration 104, loss = 1.14544665\n",
      "Iteration 105, loss = 1.14413232\n",
      "Iteration 106, loss = 1.14282004\n",
      "Iteration 107, loss = 1.14151219\n",
      "Iteration 108, loss = 1.14020894\n",
      "Iteration 109, loss = 1.13892403\n",
      "Iteration 110, loss = 1.13764246\n",
      "Iteration 111, loss = 1.13636570\n",
      "Iteration 112, loss = 1.13508991\n",
      "Iteration 113, loss = 1.13381650\n",
      "Iteration 114, loss = 1.13254513\n",
      "Iteration 115, loss = 1.13128169\n",
      "Iteration 116, loss = 1.13001900\n",
      "Iteration 117, loss = 1.12876648\n",
      "Iteration 118, loss = 1.12752982\n",
      "Iteration 119, loss = 1.12629284\n",
      "Iteration 120, loss = 1.12506157\n",
      "Iteration 121, loss = 1.12383162\n",
      "Iteration 122, loss = 1.12261073\n",
      "Iteration 123, loss = 1.12139165\n",
      "Iteration 124, loss = 1.12017717\n",
      "Iteration 125, loss = 1.11896563\n",
      "Iteration 126, loss = 1.11775471\n",
      "Iteration 127, loss = 1.11655098\n",
      "Iteration 128, loss = 1.11534715\n",
      "Iteration 129, loss = 1.11414606\n",
      "Iteration 130, loss = 1.11294949\n",
      "Iteration 131, loss = 1.11175980\n",
      "Iteration 132, loss = 1.11056970\n",
      "Iteration 133, loss = 1.10938632\n",
      "Iteration 134, loss = 1.10820592\n",
      "Iteration 135, loss = 1.10702842\n",
      "Iteration 136, loss = 1.10585551\n",
      "Iteration 137, loss = 1.10468139\n",
      "Iteration 138, loss = 1.10350797\n",
      "Iteration 139, loss = 1.10233889\n",
      "Iteration 140, loss = 1.10117702\n",
      "Iteration 141, loss = 1.10002200\n",
      "Iteration 142, loss = 1.09886743\n",
      "Iteration 143, loss = 1.09771744\n",
      "Iteration 144, loss = 1.09656990\n",
      "Iteration 145, loss = 1.09542781\n",
      "Iteration 146, loss = 1.09428942\n",
      "Iteration 147, loss = 1.09315116\n",
      "Iteration 148, loss = 1.09201568\n",
      "Iteration 149, loss = 1.09088636\n",
      "Iteration 150, loss = 1.08976574\n",
      "Iteration 151, loss = 1.08865026\n",
      "Iteration 152, loss = 1.08753457\n",
      "Iteration 153, loss = 1.08641980\n",
      "Iteration 154, loss = 1.08530809\n",
      "Iteration 155, loss = 1.08419905\n",
      "Iteration 156, loss = 1.08310135\n",
      "Iteration 157, loss = 1.08200730\n",
      "Iteration 158, loss = 1.08091615\n",
      "Iteration 159, loss = 1.07982263\n",
      "Iteration 160, loss = 1.07872948\n",
      "Iteration 161, loss = 1.07764267\n",
      "Iteration 162, loss = 1.07655413\n",
      "Iteration 163, loss = 1.07547065\n",
      "Iteration 164, loss = 1.07438395\n",
      "Iteration 165, loss = 1.07329986\n",
      "Iteration 166, loss = 1.07221630\n",
      "Iteration 167, loss = 1.07113382\n",
      "Iteration 168, loss = 1.07004959\n",
      "Iteration 169, loss = 1.06896462\n",
      "Iteration 170, loss = 1.06788253\n",
      "Iteration 171, loss = 1.06680781\n",
      "Iteration 172, loss = 1.06573218\n",
      "Iteration 173, loss = 1.06465687\n",
      "Iteration 174, loss = 1.06358465\n",
      "Iteration 175, loss = 1.06251380\n",
      "Iteration 176, loss = 1.06144441\n",
      "Iteration 177, loss = 1.06037720\n",
      "Iteration 178, loss = 1.05930901\n",
      "Iteration 179, loss = 1.05824308\n",
      "Iteration 180, loss = 1.05717903\n",
      "Iteration 181, loss = 1.05611875\n",
      "Iteration 182, loss = 1.05506069\n",
      "Iteration 183, loss = 1.05401047\n",
      "Iteration 184, loss = 1.05296336\n",
      "Iteration 185, loss = 1.05192025\n",
      "Iteration 186, loss = 1.05087934\n",
      "Iteration 187, loss = 1.04984329\n",
      "Iteration 188, loss = 1.04880523\n",
      "Iteration 189, loss = 1.04776961\n",
      "Iteration 190, loss = 1.04674685\n",
      "Iteration 191, loss = 1.04572466\n",
      "Iteration 192, loss = 1.04470070\n",
      "Iteration 193, loss = 1.04367555\n",
      "Iteration 194, loss = 1.04265503\n",
      "Iteration 195, loss = 1.04163755\n",
      "Iteration 196, loss = 1.04061795\n",
      "Iteration 197, loss = 1.03960454\n",
      "Iteration 198, loss = 1.03859444\n",
      "Iteration 199, loss = 1.03758888\n",
      "Iteration 200, loss = 1.03658253\n",
      "Iteration 201, loss = 1.03557384\n",
      "Iteration 202, loss = 1.03456917\n",
      "Iteration 203, loss = 1.03356811\n",
      "Iteration 204, loss = 1.03257087\n",
      "Iteration 205, loss = 1.03157736\n",
      "Iteration 206, loss = 1.03058426\n",
      "Iteration 207, loss = 1.02959201\n",
      "Iteration 208, loss = 1.02860486\n",
      "Iteration 209, loss = 1.02761577\n",
      "Iteration 210, loss = 1.02662900\n",
      "Iteration 211, loss = 1.02564432\n",
      "Iteration 212, loss = 1.02465913\n",
      "Iteration 213, loss = 1.02367336\n",
      "Iteration 214, loss = 1.02269223\n",
      "Iteration 215, loss = 1.02170896\n",
      "Iteration 216, loss = 1.02072577\n",
      "Iteration 217, loss = 1.01974493\n",
      "Iteration 218, loss = 1.01876629\n",
      "Iteration 219, loss = 1.01778420\n",
      "Iteration 220, loss = 1.01680860\n",
      "Iteration 221, loss = 1.01583299\n",
      "Iteration 222, loss = 1.01485907\n",
      "Iteration 223, loss = 1.01388463\n",
      "Iteration 224, loss = 1.01291165\n",
      "Iteration 225, loss = 1.01194044\n",
      "Iteration 226, loss = 1.01097598\n",
      "Iteration 227, loss = 1.01001437\n",
      "Iteration 228, loss = 1.00905249\n",
      "Iteration 229, loss = 1.00809308\n",
      "Iteration 230, loss = 1.00713152\n",
      "Iteration 231, loss = 1.00617504\n",
      "Iteration 232, loss = 1.00522385\n",
      "Iteration 233, loss = 1.00427516\n",
      "Iteration 234, loss = 1.00332515\n",
      "Iteration 235, loss = 1.00237648\n",
      "Iteration 236, loss = 1.00143333\n",
      "Iteration 237, loss = 1.00049049\n",
      "Iteration 238, loss = 0.99955000\n",
      "Iteration 239, loss = 0.99861400\n",
      "Iteration 240, loss = 0.99768644\n",
      "Iteration 241, loss = 0.99676713\n",
      "Iteration 242, loss = 0.99584881\n",
      "Iteration 243, loss = 0.99493469\n",
      "Iteration 244, loss = 0.99402363\n",
      "Iteration 245, loss = 0.99311186\n",
      "Iteration 246, loss = 0.99220007\n",
      "Iteration 247, loss = 0.99129392\n",
      "Iteration 248, loss = 0.99038688\n",
      "Iteration 249, loss = 0.98947955\n",
      "Iteration 250, loss = 0.98857282\n",
      "Iteration 251, loss = 0.98766782\n",
      "Iteration 252, loss = 0.98676282\n",
      "Iteration 253, loss = 0.98585681\n",
      "Iteration 254, loss = 0.98494860\n",
      "Iteration 255, loss = 0.98404636\n",
      "Iteration 256, loss = 0.98314744\n",
      "Iteration 257, loss = 0.98224960\n",
      "Iteration 258, loss = 0.98134343\n",
      "Iteration 259, loss = 0.98044078\n",
      "Iteration 260, loss = 0.97953895\n",
      "Iteration 261, loss = 0.97863755\n",
      "Iteration 262, loss = 0.97773856\n",
      "Iteration 263, loss = 0.97683884\n",
      "Iteration 264, loss = 0.97594066\n",
      "Iteration 265, loss = 0.97504497\n",
      "Iteration 266, loss = 0.97414771\n",
      "Iteration 267, loss = 0.97325443\n",
      "Iteration 268, loss = 0.97236488\n",
      "Iteration 269, loss = 0.97147536\n",
      "Iteration 270, loss = 0.97058720\n",
      "Iteration 271, loss = 0.96969941\n",
      "Iteration 272, loss = 0.96881504\n",
      "Iteration 273, loss = 0.96792591\n",
      "Iteration 274, loss = 0.96703697\n",
      "Iteration 275, loss = 0.96614941\n",
      "Iteration 276, loss = 0.96526604\n",
      "Iteration 277, loss = 0.96438348\n",
      "Iteration 278, loss = 0.96349739\n",
      "Iteration 279, loss = 0.96261369\n",
      "Iteration 280, loss = 0.96173101\n",
      "Iteration 281, loss = 0.96084788\n",
      "Iteration 282, loss = 0.95996279\n",
      "Iteration 283, loss = 0.95907504\n",
      "Iteration 284, loss = 0.95818359\n",
      "Iteration 285, loss = 0.95728917\n",
      "Iteration 286, loss = 0.95639886\n",
      "Iteration 287, loss = 0.95550783\n",
      "Iteration 288, loss = 0.95461540\n",
      "Iteration 289, loss = 0.95372508\n",
      "Iteration 290, loss = 0.95283503\n",
      "Iteration 291, loss = 0.95194916\n",
      "Iteration 292, loss = 0.95106559\n",
      "Iteration 293, loss = 0.95017987\n",
      "Iteration 294, loss = 0.94929770\n",
      "Iteration 295, loss = 0.94842265\n",
      "Iteration 296, loss = 0.94754783\n",
      "Iteration 297, loss = 0.94667267\n",
      "Iteration 298, loss = 0.94579923\n",
      "Iteration 299, loss = 0.94492675\n",
      "Iteration 300, loss = 0.94405725\n",
      "Iteration 301, loss = 0.94318758\n",
      "Iteration 302, loss = 0.94231456\n",
      "Iteration 303, loss = 0.94144830\n",
      "Iteration 304, loss = 0.94059904\n",
      "Iteration 305, loss = 0.93975095\n",
      "Iteration 306, loss = 0.93890382\n",
      "Iteration 307, loss = 0.93805590\n",
      "Iteration 308, loss = 0.93720769\n",
      "Iteration 309, loss = 0.93636188\n",
      "Iteration 310, loss = 0.93551807\n",
      "Iteration 311, loss = 0.93467460\n",
      "Iteration 312, loss = 0.93383138\n",
      "Iteration 313, loss = 0.93299004\n",
      "Iteration 314, loss = 0.93214826\n",
      "Iteration 315, loss = 0.93131009\n",
      "Iteration 316, loss = 0.93047484\n",
      "Iteration 317, loss = 0.92963905\n",
      "Iteration 318, loss = 0.92880452\n",
      "Iteration 319, loss = 0.92797124\n",
      "Iteration 320, loss = 0.92713760\n",
      "Iteration 321, loss = 0.92630418\n",
      "Iteration 322, loss = 0.92547129\n",
      "Iteration 323, loss = 0.92463728\n",
      "Iteration 324, loss = 0.92380739\n",
      "Iteration 325, loss = 0.92297598\n",
      "Iteration 326, loss = 0.92214775\n",
      "Iteration 327, loss = 0.92132127\n",
      "Iteration 328, loss = 0.92050136\n",
      "Iteration 329, loss = 0.91968474\n",
      "Iteration 330, loss = 0.91886796\n",
      "Iteration 331, loss = 0.91805211\n",
      "Iteration 332, loss = 0.91723662\n",
      "Iteration 333, loss = 0.91642177\n",
      "Iteration 334, loss = 0.91560793\n",
      "Iteration 335, loss = 0.91479354\n",
      "Iteration 336, loss = 0.91398173\n",
      "Iteration 337, loss = 0.91317170\n",
      "Iteration 338, loss = 0.91236211\n",
      "Iteration 339, loss = 0.91155155\n",
      "Iteration 340, loss = 0.91074166\n",
      "Iteration 341, loss = 0.90992908\n",
      "Iteration 342, loss = 0.90911403\n",
      "Iteration 343, loss = 0.90829963\n",
      "Iteration 344, loss = 0.90748585\n",
      "Iteration 345, loss = 0.90667383\n",
      "Iteration 346, loss = 0.90586109\n",
      "Iteration 347, loss = 0.90505173\n",
      "Iteration 348, loss = 0.90423791\n",
      "Iteration 349, loss = 0.90342463\n",
      "Iteration 350, loss = 0.90261347\n",
      "Iteration 351, loss = 0.90180290\n",
      "Iteration 352, loss = 0.90099230\n",
      "Iteration 353, loss = 0.90018434\n",
      "Iteration 354, loss = 0.89937672\n",
      "Iteration 355, loss = 0.89857339\n",
      "Iteration 356, loss = 0.89777298\n",
      "Iteration 357, loss = 0.89697295\n",
      "Iteration 358, loss = 0.89617478\n",
      "Iteration 359, loss = 0.89537698\n",
      "Iteration 360, loss = 0.89458034\n",
      "Iteration 361, loss = 0.89378453\n",
      "Iteration 362, loss = 0.89299343\n",
      "Iteration 363, loss = 0.89220550\n",
      "Iteration 364, loss = 0.89141956\n",
      "Iteration 365, loss = 0.89063610\n",
      "Iteration 366, loss = 0.88985515\n",
      "Iteration 367, loss = 0.88907603\n",
      "Iteration 368, loss = 0.88830020\n",
      "Iteration 369, loss = 0.88752687\n",
      "Iteration 370, loss = 0.88675616\n",
      "Iteration 371, loss = 0.88598889\n",
      "Iteration 372, loss = 0.88522124\n",
      "Iteration 373, loss = 0.88445470\n",
      "Iteration 374, loss = 0.88368857\n",
      "Iteration 375, loss = 0.88292626\n",
      "Iteration 376, loss = 0.88216470\n",
      "Iteration 377, loss = 0.88140571\n",
      "Iteration 378, loss = 0.88064766\n",
      "Iteration 379, loss = 0.87989254\n",
      "Iteration 380, loss = 0.87914181\n",
      "Iteration 381, loss = 0.87838896\n",
      "Iteration 382, loss = 0.87763929\n",
      "Iteration 383, loss = 0.87689353\n",
      "Iteration 384, loss = 0.87614952\n",
      "Iteration 385, loss = 0.87540654\n",
      "Iteration 386, loss = 0.87466484\n",
      "Iteration 387, loss = 0.87392551\n",
      "Iteration 388, loss = 0.87318833\n",
      "Iteration 389, loss = 0.87245130\n",
      "Iteration 390, loss = 0.87171408\n",
      "Iteration 391, loss = 0.87097841\n",
      "Iteration 392, loss = 0.87024162\n",
      "Iteration 393, loss = 0.86950710\n",
      "Iteration 394, loss = 0.86877595\n",
      "Iteration 395, loss = 0.86804530\n",
      "Iteration 396, loss = 0.86731195\n",
      "Iteration 397, loss = 0.86657768\n",
      "Iteration 398, loss = 0.86584533\n",
      "Iteration 399, loss = 0.86511178\n",
      "Iteration 400, loss = 0.86437920\n",
      "Iteration 401, loss = 0.86364993\n",
      "Iteration 402, loss = 0.86292121\n",
      "Iteration 403, loss = 0.86219238\n",
      "Iteration 404, loss = 0.86146643\n",
      "Iteration 405, loss = 0.86073815\n",
      "Iteration 406, loss = 0.86000977\n",
      "Iteration 407, loss = 0.85928122\n",
      "Iteration 408, loss = 0.85855564\n",
      "Iteration 409, loss = 0.85783049\n",
      "Iteration 410, loss = 0.85710317\n",
      "Iteration 411, loss = 0.85637602\n",
      "Iteration 412, loss = 0.85565164\n",
      "Iteration 413, loss = 0.85492792\n",
      "Iteration 414, loss = 0.85420648\n",
      "Iteration 415, loss = 0.85348586\n",
      "Iteration 416, loss = 0.85276433\n",
      "Iteration 417, loss = 0.85204336\n",
      "Iteration 418, loss = 0.85132398\n",
      "Iteration 419, loss = 0.85060877\n",
      "Iteration 420, loss = 0.84989416\n",
      "Iteration 421, loss = 0.84917805\n",
      "Iteration 422, loss = 0.84846291\n",
      "Iteration 423, loss = 0.84775089\n",
      "Iteration 424, loss = 0.84704100\n",
      "Iteration 425, loss = 0.84633254\n",
      "Iteration 426, loss = 0.84562400\n",
      "Iteration 427, loss = 0.84491725\n",
      "Iteration 428, loss = 0.84421324\n",
      "Iteration 429, loss = 0.84351040\n",
      "Iteration 430, loss = 0.84280795\n",
      "Iteration 431, loss = 0.84210924\n",
      "Iteration 432, loss = 0.84141095\n",
      "Iteration 433, loss = 0.84071408\n",
      "Iteration 434, loss = 0.84001976\n",
      "Iteration 435, loss = 0.83932653\n",
      "Iteration 436, loss = 0.83863639\n",
      "Iteration 437, loss = 0.83794578\n",
      "Iteration 438, loss = 0.83725804\n",
      "Iteration 439, loss = 0.83657072\n",
      "Iteration 440, loss = 0.83588528\n",
      "Iteration 441, loss = 0.83520121\n",
      "Iteration 442, loss = 0.83451847\n",
      "Iteration 443, loss = 0.83383926\n",
      "Iteration 444, loss = 0.83316199\n",
      "Iteration 445, loss = 0.83248507\n",
      "Iteration 446, loss = 0.83181122\n",
      "Iteration 447, loss = 0.83114060\n",
      "Iteration 448, loss = 0.83046905\n",
      "Iteration 449, loss = 0.82979941\n",
      "Iteration 450, loss = 0.82913111\n",
      "Iteration 451, loss = 0.82846651\n",
      "Iteration 452, loss = 0.82780534\n",
      "Iteration 453, loss = 0.82714634\n",
      "Iteration 454, loss = 0.82648732\n",
      "Iteration 455, loss = 0.82583336\n",
      "Iteration 456, loss = 0.82518097\n",
      "Iteration 457, loss = 0.82452699\n",
      "Iteration 458, loss = 0.82387536\n",
      "Iteration 459, loss = 0.82322695\n",
      "Iteration 460, loss = 0.82257726\n",
      "Iteration 461, loss = 0.82192868\n",
      "Iteration 462, loss = 0.82128486\n",
      "Iteration 463, loss = 0.82064264\n",
      "Iteration 464, loss = 0.82000003\n",
      "Iteration 465, loss = 0.81935909\n",
      "Iteration 466, loss = 0.81871998\n",
      "Iteration 467, loss = 0.81808422\n",
      "Iteration 468, loss = 0.81744974\n",
      "Iteration 469, loss = 0.81681532\n",
      "Iteration 470, loss = 0.81618054\n",
      "Iteration 471, loss = 0.81554911\n",
      "Iteration 472, loss = 0.81491922\n",
      "Iteration 473, loss = 0.81429214\n",
      "Iteration 474, loss = 0.81366669\n",
      "Iteration 475, loss = 0.81304251\n",
      "Iteration 476, loss = 0.81241971\n",
      "Iteration 477, loss = 0.81179957\n",
      "Iteration 478, loss = 0.81118140\n",
      "Iteration 479, loss = 0.81056290\n",
      "Iteration 480, loss = 0.80994373\n",
      "Iteration 481, loss = 0.80932652\n",
      "Iteration 482, loss = 0.80870811\n",
      "Iteration 483, loss = 0.80809039\n",
      "Iteration 484, loss = 0.80747409\n",
      "Iteration 485, loss = 0.80685922\n",
      "Iteration 486, loss = 0.80624580\n",
      "Iteration 487, loss = 0.80563273\n",
      "Iteration 488, loss = 0.80502042\n",
      "Iteration 489, loss = 0.80441019\n",
      "Iteration 490, loss = 0.80379992\n",
      "Iteration 491, loss = 0.80319047\n",
      "Iteration 492, loss = 0.80258230\n",
      "Iteration 493, loss = 0.80197478\n",
      "Iteration 494, loss = 0.80136977\n",
      "Iteration 495, loss = 0.80076593\n",
      "Iteration 496, loss = 0.80016360\n",
      "Iteration 497, loss = 0.79956200\n",
      "Iteration 498, loss = 0.79896268\n",
      "Iteration 499, loss = 0.79836338\n",
      "Iteration 500, loss = 0.79776755\n",
      "Iteration 501, loss = 0.79717232\n",
      "Iteration 502, loss = 0.79658139\n",
      "Iteration 503, loss = 0.79599109\n",
      "Iteration 504, loss = 0.79540132\n",
      "Iteration 505, loss = 0.79481350\n",
      "Iteration 506, loss = 0.79422709\n",
      "Iteration 507, loss = 0.79364091\n",
      "Iteration 508, loss = 0.79305333\n",
      "Iteration 509, loss = 0.79246576\n",
      "Iteration 510, loss = 0.79188095\n",
      "Iteration 511, loss = 0.79129613\n",
      "Iteration 512, loss = 0.79071139\n",
      "Iteration 513, loss = 0.79012670\n",
      "Iteration 514, loss = 0.78954505\n",
      "Iteration 515, loss = 0.78896644\n",
      "Iteration 516, loss = 0.78838670\n",
      "Iteration 517, loss = 0.78780657\n",
      "Iteration 518, loss = 0.78722670\n",
      "Iteration 519, loss = 0.78664898\n",
      "Iteration 520, loss = 0.78607309\n",
      "Iteration 521, loss = 0.78549750\n",
      "Iteration 522, loss = 0.78492168\n",
      "Iteration 523, loss = 0.78434810\n",
      "Iteration 524, loss = 0.78377747\n",
      "Iteration 525, loss = 0.78320763\n",
      "Iteration 526, loss = 0.78263907\n",
      "Iteration 527, loss = 0.78207042\n",
      "Iteration 528, loss = 0.78150292\n",
      "Iteration 529, loss = 0.78093729\n",
      "Iteration 530, loss = 0.78037220\n",
      "Iteration 531, loss = 0.77980790\n",
      "Iteration 532, loss = 0.77924644\n",
      "Iteration 533, loss = 0.77868543\n",
      "Iteration 534, loss = 0.77812518\n",
      "Iteration 535, loss = 0.77756619\n",
      "Iteration 536, loss = 0.77700953\n",
      "Iteration 537, loss = 0.77645410\n",
      "Iteration 538, loss = 0.77589883\n",
      "Iteration 539, loss = 0.77534606\n",
      "Iteration 540, loss = 0.77479330\n",
      "Iteration 541, loss = 0.77424240\n",
      "Iteration 542, loss = 0.77369177\n",
      "Iteration 543, loss = 0.77314279\n",
      "Iteration 544, loss = 0.77259633\n",
      "Iteration 545, loss = 0.77204917\n",
      "Iteration 546, loss = 0.77150159\n",
      "Iteration 547, loss = 0.77095718\n",
      "Iteration 548, loss = 0.77041513\n",
      "Iteration 549, loss = 0.76987267\n",
      "Iteration 550, loss = 0.76933045\n",
      "Iteration 551, loss = 0.76879208\n",
      "Iteration 552, loss = 0.76825327\n",
      "Iteration 553, loss = 0.76771523\n",
      "Iteration 554, loss = 0.76717869\n",
      "Iteration 555, loss = 0.76664299\n",
      "Iteration 556, loss = 0.76610905\n",
      "Iteration 557, loss = 0.76557494\n",
      "Iteration 558, loss = 0.76503909\n",
      "Iteration 559, loss = 0.76449949\n",
      "Iteration 560, loss = 0.76396151\n",
      "Iteration 561, loss = 0.76342201\n",
      "Iteration 562, loss = 0.76288035\n",
      "Iteration 563, loss = 0.76233940\n",
      "Iteration 564, loss = 0.76179729\n",
      "Iteration 565, loss = 0.76125580\n",
      "Iteration 566, loss = 0.76071644\n",
      "Iteration 567, loss = 0.76017663\n",
      "Iteration 568, loss = 0.75963701\n",
      "Iteration 569, loss = 0.75909827\n",
      "Iteration 570, loss = 0.75856021\n",
      "Iteration 571, loss = 0.75802401\n",
      "Iteration 572, loss = 0.75748811\n",
      "Iteration 573, loss = 0.75695185\n",
      "Iteration 574, loss = 0.75641801\n",
      "Iteration 575, loss = 0.75588325\n",
      "Iteration 576, loss = 0.75534961\n",
      "Iteration 577, loss = 0.75481845\n",
      "Iteration 578, loss = 0.75428785\n",
      "Iteration 579, loss = 0.75375883\n",
      "Iteration 580, loss = 0.75322939\n",
      "Iteration 581, loss = 0.75270229\n",
      "Iteration 582, loss = 0.75217577\n",
      "Iteration 583, loss = 0.75165042\n",
      "Iteration 584, loss = 0.75112674\n",
      "Iteration 585, loss = 0.75060403\n",
      "Iteration 586, loss = 0.75008184\n",
      "Iteration 587, loss = 0.74956182\n",
      "Iteration 588, loss = 0.74904210\n",
      "Iteration 589, loss = 0.74852230\n",
      "Iteration 590, loss = 0.74800458\n",
      "Iteration 591, loss = 0.74748863\n",
      "Iteration 592, loss = 0.74697316\n",
      "Iteration 593, loss = 0.74645860\n",
      "Iteration 594, loss = 0.74594468\n",
      "Iteration 595, loss = 0.74543473\n",
      "Iteration 596, loss = 0.74492638\n",
      "Iteration 597, loss = 0.74441910\n",
      "Iteration 598, loss = 0.74390978\n",
      "Iteration 599, loss = 0.74339972\n",
      "Iteration 600, loss = 0.74288979\n",
      "Iteration 601, loss = 0.74238120\n",
      "Iteration 602, loss = 0.74187242\n",
      "Iteration 603, loss = 0.74136405\n",
      "Iteration 604, loss = 0.74085603\n",
      "Iteration 605, loss = 0.74034863\n",
      "Iteration 606, loss = 0.73984298\n",
      "Iteration 607, loss = 0.73933862\n",
      "Iteration 608, loss = 0.73883329\n",
      "Iteration 609, loss = 0.73832937\n",
      "Iteration 610, loss = 0.73782659\n",
      "Iteration 611, loss = 0.73732435\n",
      "Iteration 612, loss = 0.73682404\n",
      "Iteration 613, loss = 0.73632472\n",
      "Iteration 614, loss = 0.73582509\n",
      "Iteration 615, loss = 0.73532546\n",
      "Iteration 616, loss = 0.73482691\n",
      "Iteration 617, loss = 0.73432902\n",
      "Iteration 618, loss = 0.73383206\n",
      "Iteration 619, loss = 0.73333629\n",
      "Iteration 620, loss = 0.73284115\n",
      "Iteration 621, loss = 0.73234639\n",
      "Iteration 622, loss = 0.73185273\n",
      "Iteration 623, loss = 0.73135777\n",
      "Iteration 624, loss = 0.73086435\n",
      "Iteration 625, loss = 0.73037287\n",
      "Iteration 626, loss = 0.72988194\n",
      "Iteration 627, loss = 0.72939206\n",
      "Iteration 628, loss = 0.72890269\n",
      "Iteration 629, loss = 0.72841586\n",
      "Iteration 630, loss = 0.72792903\n",
      "Iteration 631, loss = 0.72744233\n",
      "Iteration 632, loss = 0.72695749\n",
      "Iteration 633, loss = 0.72647318\n",
      "Iteration 634, loss = 0.72599009\n",
      "Iteration 635, loss = 0.72550760\n",
      "Iteration 636, loss = 0.72502562\n",
      "Iteration 637, loss = 0.72454409\n",
      "Iteration 638, loss = 0.72406354\n",
      "Iteration 639, loss = 0.72358441\n",
      "Iteration 640, loss = 0.72310511\n",
      "Iteration 641, loss = 0.72262633\n",
      "Iteration 642, loss = 0.72215004\n",
      "Iteration 643, loss = 0.72167377\n",
      "Iteration 644, loss = 0.72119815\n",
      "Iteration 645, loss = 0.72072384\n",
      "Iteration 646, loss = 0.72025026\n",
      "Iteration 647, loss = 0.71977750\n",
      "Iteration 648, loss = 0.71930527\n",
      "Iteration 649, loss = 0.71883438\n",
      "Iteration 650, loss = 0.71836447\n",
      "Iteration 651, loss = 0.71789529\n",
      "Iteration 652, loss = 0.71742588\n",
      "Iteration 653, loss = 0.71695751\n",
      "Iteration 654, loss = 0.71648978\n",
      "Iteration 655, loss = 0.71602299\n",
      "Iteration 656, loss = 0.71555665\n",
      "Iteration 657, loss = 0.71509008\n",
      "Iteration 658, loss = 0.71462558\n",
      "Iteration 659, loss = 0.71416178\n",
      "Iteration 660, loss = 0.71369900\n",
      "Iteration 661, loss = 0.71323707\n",
      "Iteration 662, loss = 0.71277532\n",
      "Iteration 663, loss = 0.71231451\n",
      "Iteration 664, loss = 0.71185463\n",
      "Iteration 665, loss = 0.71139462\n",
      "Iteration 666, loss = 0.71093571\n",
      "Iteration 667, loss = 0.71047883\n",
      "Iteration 668, loss = 0.71002224\n",
      "Iteration 669, loss = 0.70956564\n",
      "Iteration 670, loss = 0.70910994\n",
      "Iteration 671, loss = 0.70865590\n",
      "Iteration 672, loss = 0.70820265\n",
      "Iteration 673, loss = 0.70774877\n",
      "Iteration 674, loss = 0.70729703\n",
      "Iteration 675, loss = 0.70684766\n",
      "Iteration 676, loss = 0.70639886\n",
      "Iteration 677, loss = 0.70595052\n",
      "Iteration 678, loss = 0.70550205\n",
      "Iteration 679, loss = 0.70505501\n",
      "Iteration 680, loss = 0.70460827\n",
      "Iteration 681, loss = 0.70416198\n",
      "Iteration 682, loss = 0.70371724\n",
      "Iteration 683, loss = 0.70327206\n",
      "Iteration 684, loss = 0.70282764\n",
      "Iteration 685, loss = 0.70238435\n",
      "Iteration 686, loss = 0.70194122\n",
      "Iteration 687, loss = 0.70149852\n",
      "Iteration 688, loss = 0.70105730\n",
      "Iteration 689, loss = 0.70061702\n",
      "Iteration 690, loss = 0.70017675\n",
      "Iteration 691, loss = 0.69973774\n",
      "Iteration 692, loss = 0.69929886\n",
      "Iteration 693, loss = 0.69886185\n",
      "Iteration 694, loss = 0.69842554\n",
      "Iteration 695, loss = 0.69798935\n",
      "Iteration 696, loss = 0.69755389\n",
      "Iteration 697, loss = 0.69711956\n",
      "Iteration 698, loss = 0.69668622\n",
      "Iteration 699, loss = 0.69625326\n",
      "Iteration 700, loss = 0.69582102\n",
      "Iteration 701, loss = 0.69538945\n",
      "Iteration 702, loss = 0.69495913\n",
      "Iteration 703, loss = 0.69452985\n",
      "Iteration 704, loss = 0.69410182\n",
      "Iteration 705, loss = 0.69367473\n",
      "Iteration 706, loss = 0.69324796\n",
      "Iteration 707, loss = 0.69282185\n",
      "Iteration 708, loss = 0.69239592\n",
      "Iteration 709, loss = 0.69197092\n",
      "Iteration 710, loss = 0.69154707\n",
      "Iteration 711, loss = 0.69112344\n",
      "Iteration 712, loss = 0.69069997\n",
      "Iteration 713, loss = 0.69027808\n",
      "Iteration 714, loss = 0.68985641\n",
      "Iteration 715, loss = 0.68943487\n",
      "Iteration 716, loss = 0.68901480\n",
      "Iteration 717, loss = 0.68859560\n",
      "Iteration 718, loss = 0.68817660\n",
      "Iteration 719, loss = 0.68775859\n",
      "Iteration 720, loss = 0.68734094\n",
      "Iteration 721, loss = 0.68692227\n",
      "Iteration 722, loss = 0.68650503\n",
      "Iteration 723, loss = 0.68608812\n",
      "Iteration 724, loss = 0.68567136\n",
      "Iteration 725, loss = 0.68525527\n",
      "Iteration 726, loss = 0.68483848\n",
      "Iteration 727, loss = 0.68442171\n",
      "Iteration 728, loss = 0.68400642\n",
      "Iteration 729, loss = 0.68359090\n",
      "Iteration 730, loss = 0.68317574\n",
      "Iteration 731, loss = 0.68276147\n",
      "Iteration 732, loss = 0.68234735\n",
      "Iteration 733, loss = 0.68193361\n",
      "Iteration 734, loss = 0.68152046\n",
      "Iteration 735, loss = 0.68110738\n",
      "Iteration 736, loss = 0.68069585\n",
      "Iteration 737, loss = 0.68028616\n",
      "Iteration 738, loss = 0.67987680\n",
      "Iteration 739, loss = 0.67946794\n",
      "Iteration 740, loss = 0.67905996\n",
      "Iteration 741, loss = 0.67865251\n",
      "Iteration 742, loss = 0.67824433\n",
      "Iteration 743, loss = 0.67783602\n",
      "Iteration 744, loss = 0.67742795\n",
      "Iteration 745, loss = 0.67702083\n",
      "Iteration 746, loss = 0.67661518\n",
      "Iteration 747, loss = 0.67620954\n",
      "Iteration 748, loss = 0.67580362\n",
      "Iteration 749, loss = 0.67539773\n",
      "Iteration 750, loss = 0.67499294\n",
      "Iteration 751, loss = 0.67458803\n",
      "Iteration 752, loss = 0.67418514\n",
      "Iteration 753, loss = 0.67378240\n",
      "Iteration 754, loss = 0.67338048\n",
      "Iteration 755, loss = 0.67297880\n",
      "Iteration 756, loss = 0.67257800\n",
      "Iteration 757, loss = 0.67217811\n",
      "Iteration 758, loss = 0.67177870\n",
      "Iteration 759, loss = 0.67138018\n",
      "Iteration 760, loss = 0.67098207\n",
      "Iteration 761, loss = 0.67058511\n",
      "Iteration 762, loss = 0.67018833\n",
      "Iteration 763, loss = 0.66979171\n",
      "Iteration 764, loss = 0.66939608\n",
      "Iteration 765, loss = 0.66900042\n",
      "Iteration 766, loss = 0.66860533\n",
      "Iteration 767, loss = 0.66821093\n",
      "Iteration 768, loss = 0.66781759\n",
      "Iteration 769, loss = 0.66742483\n",
      "Iteration 770, loss = 0.66703232\n",
      "Iteration 771, loss = 0.66664015\n",
      "Iteration 772, loss = 0.66624864\n",
      "Iteration 773, loss = 0.66585785\n",
      "Iteration 774, loss = 0.66546786\n",
      "Iteration 775, loss = 0.66507756\n",
      "Iteration 776, loss = 0.66468770\n",
      "Iteration 777, loss = 0.66429810\n",
      "Iteration 778, loss = 0.66390909\n",
      "Iteration 779, loss = 0.66352045\n",
      "Iteration 780, loss = 0.66313303\n",
      "Iteration 781, loss = 0.66274652\n",
      "Iteration 782, loss = 0.66236023\n",
      "Iteration 783, loss = 0.66197421\n",
      "Iteration 784, loss = 0.66158903\n",
      "Iteration 785, loss = 0.66120455\n",
      "Iteration 786, loss = 0.66082035\n",
      "Iteration 787, loss = 0.66043705\n",
      "Iteration 788, loss = 0.66005510\n",
      "Iteration 789, loss = 0.65967405\n",
      "Iteration 790, loss = 0.65929309\n",
      "Iteration 791, loss = 0.65891290\n",
      "Iteration 792, loss = 0.65853320\n",
      "Iteration 793, loss = 0.65815382\n",
      "Iteration 794, loss = 0.65777539\n",
      "Iteration 795, loss = 0.65739694\n",
      "Iteration 796, loss = 0.65701916\n",
      "Iteration 797, loss = 0.65664282\n",
      "Iteration 798, loss = 0.65626633\n",
      "Iteration 799, loss = 0.65589093\n",
      "Iteration 800, loss = 0.65551583\n",
      "Iteration 801, loss = 0.65514108\n",
      "Iteration 802, loss = 0.65476719\n",
      "Iteration 803, loss = 0.65439391\n",
      "Iteration 804, loss = 0.65402123\n",
      "Iteration 805, loss = 0.65364865\n",
      "Iteration 806, loss = 0.65327670\n",
      "Iteration 807, loss = 0.65290540\n",
      "Iteration 808, loss = 0.65253431\n",
      "Iteration 809, loss = 0.65216410\n",
      "Iteration 810, loss = 0.65179432\n",
      "Iteration 811, loss = 0.65142513\n",
      "Iteration 812, loss = 0.65105659\n",
      "Iteration 813, loss = 0.65068872\n",
      "Iteration 814, loss = 0.65032134\n",
      "Iteration 815, loss = 0.64995450\n",
      "Iteration 816, loss = 0.64958806\n",
      "Iteration 817, loss = 0.64922234\n",
      "Iteration 818, loss = 0.64885675\n",
      "Iteration 819, loss = 0.64849127\n",
      "Iteration 820, loss = 0.64812577\n",
      "Iteration 821, loss = 0.64776060\n",
      "Iteration 822, loss = 0.64739548\n",
      "Iteration 823, loss = 0.64702998\n",
      "Iteration 824, loss = 0.64666468\n",
      "Iteration 825, loss = 0.64629970\n",
      "Iteration 826, loss = 0.64593474\n",
      "Iteration 827, loss = 0.64557032\n",
      "Iteration 828, loss = 0.64520653\n",
      "Iteration 829, loss = 0.64484247\n",
      "Iteration 830, loss = 0.64447903\n",
      "Iteration 831, loss = 0.64411610\n",
      "Iteration 832, loss = 0.64375346\n",
      "Iteration 833, loss = 0.64339091\n",
      "Iteration 834, loss = 0.64302967\n",
      "Iteration 835, loss = 0.64266842\n",
      "Iteration 836, loss = 0.64230767\n",
      "Iteration 837, loss = 0.64194750\n",
      "Iteration 838, loss = 0.64158769\n",
      "Iteration 839, loss = 0.64122843\n",
      "Iteration 840, loss = 0.64086982\n",
      "Iteration 841, loss = 0.64051184\n",
      "Iteration 842, loss = 0.64015455\n",
      "Iteration 843, loss = 0.63979786\n",
      "Iteration 844, loss = 0.63944182\n",
      "Iteration 845, loss = 0.63908690\n",
      "Iteration 846, loss = 0.63873243\n",
      "Iteration 847, loss = 0.63837813\n",
      "Iteration 848, loss = 0.63802413\n",
      "Iteration 849, loss = 0.63767088\n",
      "Iteration 850, loss = 0.63731816\n",
      "Iteration 851, loss = 0.63696567\n",
      "Iteration 852, loss = 0.63661377\n",
      "Iteration 853, loss = 0.63626197\n",
      "Iteration 854, loss = 0.63591109\n",
      "Iteration 855, loss = 0.63556117\n",
      "Iteration 856, loss = 0.63521101\n",
      "Iteration 857, loss = 0.63485986\n",
      "Iteration 858, loss = 0.63450954\n",
      "Iteration 859, loss = 0.63415955\n",
      "Iteration 860, loss = 0.63380905\n",
      "Iteration 861, loss = 0.63345781\n",
      "Iteration 862, loss = 0.63310731\n",
      "Iteration 863, loss = 0.63275766\n",
      "Iteration 864, loss = 0.63240812\n",
      "Iteration 865, loss = 0.63205855\n",
      "Iteration 866, loss = 0.63171020\n",
      "Iteration 867, loss = 0.63136179\n",
      "Iteration 868, loss = 0.63101400\n",
      "Iteration 869, loss = 0.63066636\n",
      "Iteration 870, loss = 0.63031883\n",
      "Iteration 871, loss = 0.62997183\n",
      "Iteration 872, loss = 0.62962495\n",
      "Iteration 873, loss = 0.62927942\n",
      "Iteration 874, loss = 0.62893414\n",
      "Iteration 875, loss = 0.62858919\n",
      "Iteration 876, loss = 0.62824454\n",
      "Iteration 877, loss = 0.62790036\n",
      "Iteration 878, loss = 0.62755660\n",
      "Iteration 879, loss = 0.62721312\n",
      "Iteration 880, loss = 0.62687008\n",
      "Iteration 881, loss = 0.62652615\n",
      "Iteration 882, loss = 0.62618333\n",
      "Iteration 883, loss = 0.62584044\n",
      "Iteration 884, loss = 0.62549809\n",
      "Iteration 885, loss = 0.62515577\n",
      "Iteration 886, loss = 0.62481407\n",
      "Iteration 887, loss = 0.62447349\n",
      "Iteration 888, loss = 0.62413285\n",
      "Iteration 889, loss = 0.62379126\n",
      "Iteration 890, loss = 0.62345015\n",
      "Iteration 891, loss = 0.62310912\n",
      "Iteration 892, loss = 0.62276829\n",
      "Iteration 893, loss = 0.62242786\n",
      "Iteration 894, loss = 0.62208920\n",
      "Iteration 895, loss = 0.62175171\n",
      "Iteration 896, loss = 0.62141547\n",
      "Iteration 897, loss = 0.62108059\n",
      "Iteration 898, loss = 0.62074631\n",
      "Iteration 899, loss = 0.62041161\n",
      "Iteration 900, loss = 0.62007737\n",
      "Iteration 901, loss = 0.61974413\n",
      "Iteration 902, loss = 0.61941084\n",
      "Iteration 903, loss = 0.61907792\n",
      "Iteration 904, loss = 0.61874542\n",
      "Iteration 905, loss = 0.61841196\n",
      "Iteration 906, loss = 0.61807734\n",
      "Iteration 907, loss = 0.61774230\n",
      "Iteration 908, loss = 0.61740663\n",
      "Iteration 909, loss = 0.61707130\n",
      "Iteration 910, loss = 0.61673567\n",
      "Iteration 911, loss = 0.61639894\n",
      "Iteration 912, loss = 0.61605994\n",
      "Iteration 913, loss = 0.61572047\n",
      "Iteration 914, loss = 0.61538057\n",
      "Iteration 915, loss = 0.61504037\n",
      "Iteration 916, loss = 0.61469981\n",
      "Iteration 917, loss = 0.61435941\n",
      "Iteration 918, loss = 0.61401922\n",
      "Iteration 919, loss = 0.61367929\n",
      "Iteration 920, loss = 0.61333932\n",
      "Iteration 921, loss = 0.61299915\n",
      "Iteration 922, loss = 0.61265863\n",
      "Iteration 923, loss = 0.61231812\n",
      "Iteration 924, loss = 0.61197776\n",
      "Iteration 925, loss = 0.61163809\n",
      "Iteration 926, loss = 0.61129888\n",
      "Iteration 927, loss = 0.61095953\n",
      "Iteration 928, loss = 0.61062057\n",
      "Iteration 929, loss = 0.61028235\n",
      "Iteration 930, loss = 0.60994494\n",
      "Iteration 931, loss = 0.60960730\n",
      "Iteration 932, loss = 0.60926980\n",
      "Iteration 933, loss = 0.60893282\n",
      "Iteration 934, loss = 0.60859629\n",
      "Iteration 935, loss = 0.60826061\n",
      "Iteration 936, loss = 0.60792537\n",
      "Iteration 937, loss = 0.60759035\n",
      "Iteration 938, loss = 0.60725589\n",
      "Iteration 939, loss = 0.60692199\n",
      "Iteration 940, loss = 0.60658852\n",
      "Iteration 941, loss = 0.60625536\n",
      "Iteration 942, loss = 0.60592279\n",
      "Iteration 943, loss = 0.60559151\n",
      "Iteration 944, loss = 0.60526077\n",
      "Iteration 945, loss = 0.60493062\n",
      "Iteration 946, loss = 0.60460088\n",
      "Iteration 947, loss = 0.60427078\n",
      "Iteration 948, loss = 0.60394010\n",
      "Iteration 949, loss = 0.60360992\n",
      "Iteration 950, loss = 0.60328046\n",
      "Iteration 951, loss = 0.60295238\n",
      "Iteration 952, loss = 0.60262484\n",
      "Iteration 953, loss = 0.60229829\n",
      "Iteration 954, loss = 0.60197301\n",
      "Iteration 955, loss = 0.60164802\n",
      "Iteration 956, loss = 0.60132296\n",
      "Iteration 957, loss = 0.60099644\n",
      "Iteration 958, loss = 0.60067022\n",
      "Iteration 959, loss = 0.60034501\n",
      "Iteration 960, loss = 0.60002116\n",
      "Iteration 961, loss = 0.59969792\n",
      "Iteration 962, loss = 0.59937608\n",
      "Iteration 963, loss = 0.59905473\n",
      "Iteration 964, loss = 0.59873354\n",
      "Iteration 965, loss = 0.59841117\n",
      "Iteration 966, loss = 0.59808874\n",
      "Iteration 967, loss = 0.59776842\n",
      "Iteration 968, loss = 0.59744925\n",
      "Iteration 969, loss = 0.59713053\n",
      "Iteration 970, loss = 0.59681198\n",
      "Iteration 971, loss = 0.59649452\n",
      "Iteration 972, loss = 0.59617958\n",
      "Iteration 973, loss = 0.59586470\n",
      "Iteration 974, loss = 0.59555042\n",
      "Iteration 975, loss = 0.59523639\n",
      "Iteration 976, loss = 0.59492237\n",
      "Iteration 977, loss = 0.59460902\n",
      "Iteration 978, loss = 0.59429616\n",
      "Iteration 979, loss = 0.59398374\n",
      "Iteration 980, loss = 0.59367270\n",
      "Iteration 981, loss = 0.59336253\n",
      "Iteration 982, loss = 0.59305211\n",
      "Iteration 983, loss = 0.59274234\n",
      "Iteration 984, loss = 0.59243248\n",
      "Iteration 985, loss = 0.59212266\n",
      "Iteration 986, loss = 0.59181398\n",
      "Iteration 987, loss = 0.59150543\n",
      "Iteration 988, loss = 0.59119725\n",
      "Iteration 989, loss = 0.59088869\n",
      "Iteration 990, loss = 0.59058130\n",
      "Iteration 991, loss = 0.59027461\n",
      "Iteration 992, loss = 0.58996830\n",
      "Iteration 993, loss = 0.58966143\n",
      "Iteration 994, loss = 0.58935399\n",
      "Iteration 995, loss = 0.58904602\n",
      "Iteration 996, loss = 0.58873827\n",
      "Iteration 997, loss = 0.58843024\n",
      "Iteration 998, loss = 0.58812239\n",
      "Iteration 999, loss = 0.58781401\n",
      "Iteration 1000, loss = 0.58750525\n",
      "Iteration 1001, loss = 0.58719622\n",
      "Iteration 1002, loss = 0.58688739\n",
      "Iteration 1003, loss = 0.58657823\n",
      "Iteration 1004, loss = 0.58626874\n",
      "Iteration 1005, loss = 0.58595937\n",
      "Iteration 1006, loss = 0.58565019\n",
      "Iteration 1007, loss = 0.58534134\n",
      "Iteration 1008, loss = 0.58503280\n",
      "Iteration 1009, loss = 0.58472434\n",
      "Iteration 1010, loss = 0.58441693\n",
      "Iteration 1011, loss = 0.58411049\n",
      "Iteration 1012, loss = 0.58380509\n",
      "Iteration 1013, loss = 0.58349975\n",
      "Iteration 1014, loss = 0.58319442\n",
      "Iteration 1015, loss = 0.58288902\n",
      "Iteration 1016, loss = 0.58258427\n",
      "Iteration 1017, loss = 0.58228009\n",
      "Iteration 1018, loss = 0.58197562\n",
      "Iteration 1019, loss = 0.58167100\n",
      "Iteration 1020, loss = 0.58136684\n",
      "Iteration 1021, loss = 0.58106260\n",
      "Iteration 1022, loss = 0.58075882\n",
      "Iteration 1023, loss = 0.58045721\n",
      "Iteration 1024, loss = 0.58015599\n",
      "Iteration 1025, loss = 0.57985504\n",
      "Iteration 1026, loss = 0.57955232\n",
      "Iteration 1027, loss = 0.57924940\n",
      "Iteration 1028, loss = 0.57894641\n",
      "Iteration 1029, loss = 0.57864387\n",
      "Iteration 1030, loss = 0.57834162\n",
      "Iteration 1031, loss = 0.57803963\n",
      "Iteration 1032, loss = 0.57773758\n",
      "Iteration 1033, loss = 0.57743546\n",
      "Iteration 1034, loss = 0.57713250\n",
      "Iteration 1035, loss = 0.57682966\n",
      "Iteration 1036, loss = 0.57652703\n",
      "Iteration 1037, loss = 0.57622508\n",
      "Iteration 1038, loss = 0.57592326\n",
      "Iteration 1039, loss = 0.57562172\n",
      "Iteration 1040, loss = 0.57532067\n",
      "Iteration 1041, loss = 0.57501928\n",
      "Iteration 1042, loss = 0.57471717\n",
      "Iteration 1043, loss = 0.57441559\n",
      "Iteration 1044, loss = 0.57411320\n",
      "Iteration 1045, loss = 0.57381076\n",
      "Iteration 1046, loss = 0.57350861\n",
      "Iteration 1047, loss = 0.57320630\n",
      "Iteration 1048, loss = 0.57290458\n",
      "Iteration 1049, loss = 0.57260440\n",
      "Iteration 1050, loss = 0.57230469\n",
      "Iteration 1051, loss = 0.57200540\n",
      "Iteration 1052, loss = 0.57170555\n",
      "Iteration 1053, loss = 0.57140544\n",
      "Iteration 1054, loss = 0.57110585\n",
      "Iteration 1055, loss = 0.57080582\n",
      "Iteration 1056, loss = 0.57050654\n",
      "Iteration 1057, loss = 0.57020754\n",
      "Iteration 1058, loss = 0.56990868\n",
      "Iteration 1059, loss = 0.56961063\n",
      "Iteration 1060, loss = 0.56931303\n",
      "Iteration 1061, loss = 0.56901578\n",
      "Iteration 1062, loss = 0.56871872\n",
      "Iteration 1063, loss = 0.56842270\n",
      "Iteration 1064, loss = 0.56812696\n",
      "Iteration 1065, loss = 0.56783164\n",
      "Iteration 1066, loss = 0.56753678\n",
      "Iteration 1067, loss = 0.56724308\n",
      "Iteration 1068, loss = 0.56695159\n",
      "Iteration 1069, loss = 0.56666044\n",
      "Iteration 1070, loss = 0.56637003\n",
      "Iteration 1071, loss = 0.56608010\n",
      "Iteration 1072, loss = 0.56579005\n",
      "Iteration 1073, loss = 0.56549945\n",
      "Iteration 1074, loss = 0.56520928\n",
      "Iteration 1075, loss = 0.56491946\n",
      "Iteration 1076, loss = 0.56462969\n",
      "Iteration 1077, loss = 0.56434073\n",
      "Iteration 1078, loss = 0.56405247\n",
      "Iteration 1079, loss = 0.56376458\n",
      "Iteration 1080, loss = 0.56347744\n",
      "Iteration 1081, loss = 0.56319064\n",
      "Iteration 1082, loss = 0.56290410\n",
      "Iteration 1083, loss = 0.56261842\n",
      "Iteration 1084, loss = 0.56233317\n",
      "Iteration 1085, loss = 0.56204801\n",
      "Iteration 1086, loss = 0.56176373\n",
      "Iteration 1087, loss = 0.56147976\n",
      "Iteration 1088, loss = 0.56119611\n",
      "Iteration 1089, loss = 0.56091319\n",
      "Iteration 1090, loss = 0.56063066\n",
      "Iteration 1091, loss = 0.56034795\n",
      "Iteration 1092, loss = 0.56006620\n",
      "Iteration 1093, loss = 0.55978494\n",
      "Iteration 1094, loss = 0.55950394\n",
      "Iteration 1095, loss = 0.55922304\n",
      "Iteration 1096, loss = 0.55894258\n",
      "Iteration 1097, loss = 0.55866232\n",
      "Iteration 1098, loss = 0.55838219\n",
      "Iteration 1099, loss = 0.55810260\n",
      "Iteration 1100, loss = 0.55782323\n",
      "Iteration 1101, loss = 0.55754448\n",
      "Iteration 1102, loss = 0.55726597\n",
      "Iteration 1103, loss = 0.55698823\n",
      "Iteration 1104, loss = 0.55671102\n",
      "Iteration 1105, loss = 0.55643414\n",
      "Iteration 1106, loss = 0.55615761\n",
      "Iteration 1107, loss = 0.55588116\n",
      "Iteration 1108, loss = 0.55560503\n",
      "Iteration 1109, loss = 0.55532908\n",
      "Iteration 1110, loss = 0.55505350\n",
      "Iteration 1111, loss = 0.55477833\n",
      "Iteration 1112, loss = 0.55450355\n",
      "Iteration 1113, loss = 0.55422908\n",
      "Iteration 1114, loss = 0.55395506\n",
      "Iteration 1115, loss = 0.55368150\n",
      "Iteration 1116, loss = 0.55340840\n",
      "Iteration 1117, loss = 0.55313562\n",
      "Iteration 1118, loss = 0.55286334\n",
      "Iteration 1119, loss = 0.55259136\n",
      "Iteration 1120, loss = 0.55231980\n",
      "Iteration 1121, loss = 0.55204880\n",
      "Iteration 1122, loss = 0.55177785\n",
      "Iteration 1123, loss = 0.55150714\n",
      "Iteration 1124, loss = 0.55123684\n",
      "Iteration 1125, loss = 0.55096664\n",
      "Iteration 1126, loss = 0.55069680\n",
      "Iteration 1127, loss = 0.55042759\n",
      "Iteration 1128, loss = 0.55015903\n",
      "Iteration 1129, loss = 0.54989060\n",
      "Iteration 1130, loss = 0.54962247\n",
      "Iteration 1131, loss = 0.54935481\n",
      "Iteration 1132, loss = 0.54908735\n",
      "Iteration 1133, loss = 0.54882028\n",
      "Iteration 1134, loss = 0.54855375\n",
      "Iteration 1135, loss = 0.54828787\n",
      "Iteration 1136, loss = 0.54802368\n",
      "Iteration 1137, loss = 0.54775986\n",
      "Iteration 1138, loss = 0.54749636\n",
      "Iteration 1139, loss = 0.54723365\n",
      "Iteration 1140, loss = 0.54697112\n",
      "Iteration 1141, loss = 0.54670896\n",
      "Iteration 1142, loss = 0.54644733\n",
      "Iteration 1143, loss = 0.54618612\n",
      "Iteration 1144, loss = 0.54592513\n",
      "Iteration 1145, loss = 0.54566464\n",
      "Iteration 1146, loss = 0.54540418\n",
      "Iteration 1147, loss = 0.54514426\n",
      "Iteration 1148, loss = 0.54488470\n",
      "Iteration 1149, loss = 0.54462560\n",
      "Iteration 1150, loss = 0.54436690\n",
      "Iteration 1151, loss = 0.54410834\n",
      "Iteration 1152, loss = 0.54385009\n",
      "Iteration 1153, loss = 0.54359203\n",
      "Iteration 1154, loss = 0.54333450\n",
      "Iteration 1155, loss = 0.54307720\n",
      "Iteration 1156, loss = 0.54281957\n",
      "Iteration 1157, loss = 0.54256216\n",
      "Iteration 1158, loss = 0.54230500\n",
      "Iteration 1159, loss = 0.54204851\n",
      "Iteration 1160, loss = 0.54179240\n",
      "Iteration 1161, loss = 0.54153751\n",
      "Iteration 1162, loss = 0.54128277\n",
      "Iteration 1163, loss = 0.54102853\n",
      "Iteration 1164, loss = 0.54077478\n",
      "Iteration 1165, loss = 0.54052135\n",
      "Iteration 1166, loss = 0.54026812\n",
      "Iteration 1167, loss = 0.54001483\n",
      "Iteration 1168, loss = 0.53976173\n",
      "Iteration 1169, loss = 0.53950928\n",
      "Iteration 1170, loss = 0.53925707\n",
      "Iteration 1171, loss = 0.53900483\n",
      "Iteration 1172, loss = 0.53875282\n",
      "Iteration 1173, loss = 0.53850130\n",
      "Iteration 1174, loss = 0.53824953\n",
      "Iteration 1175, loss = 0.53799777\n",
      "Iteration 1176, loss = 0.53774605\n",
      "Iteration 1177, loss = 0.53749451\n",
      "Iteration 1178, loss = 0.53724325\n",
      "Iteration 1179, loss = 0.53699217\n",
      "Iteration 1180, loss = 0.53674119\n",
      "Iteration 1181, loss = 0.53649084\n",
      "Iteration 1182, loss = 0.53624081\n",
      "Iteration 1183, loss = 0.53599062\n",
      "Iteration 1184, loss = 0.53574085\n",
      "Iteration 1185, loss = 0.53549167\n",
      "Iteration 1186, loss = 0.53524251\n",
      "Iteration 1187, loss = 0.53499378\n",
      "Iteration 1188, loss = 0.53474529\n",
      "Iteration 1189, loss = 0.53449700\n",
      "Iteration 1190, loss = 0.53424924\n",
      "Iteration 1191, loss = 0.53400160\n",
      "Iteration 1192, loss = 0.53375660\n",
      "Iteration 1193, loss = 0.53351271\n",
      "Iteration 1194, loss = 0.53326918\n",
      "Iteration 1195, loss = 0.53302575\n",
      "Iteration 1196, loss = 0.53278276\n",
      "Iteration 1197, loss = 0.53254027\n",
      "Iteration 1198, loss = 0.53229803\n",
      "Iteration 1199, loss = 0.53205597\n",
      "Iteration 1200, loss = 0.53181408\n",
      "Iteration 1201, loss = 0.53157291\n",
      "Iteration 1202, loss = 0.53133200\n",
      "Iteration 1203, loss = 0.53109127\n",
      "Iteration 1204, loss = 0.53085075\n",
      "Iteration 1205, loss = 0.53061086\n",
      "Iteration 1206, loss = 0.53037173\n",
      "Iteration 1207, loss = 0.53013283\n",
      "Iteration 1208, loss = 0.52989411\n",
      "Iteration 1209, loss = 0.52965575\n",
      "Iteration 1210, loss = 0.52941767\n",
      "Iteration 1211, loss = 0.52917992\n",
      "Iteration 1212, loss = 0.52894272\n",
      "Iteration 1213, loss = 0.52870549\n",
      "Iteration 1214, loss = 0.52846868\n",
      "Iteration 1215, loss = 0.52823250\n",
      "Iteration 1216, loss = 0.52799670\n",
      "Iteration 1217, loss = 0.52776121\n",
      "Iteration 1218, loss = 0.52752579\n",
      "Iteration 1219, loss = 0.52729090\n",
      "Iteration 1220, loss = 0.52705611\n",
      "Iteration 1221, loss = 0.52682190\n",
      "Iteration 1222, loss = 0.52658789\n",
      "Iteration 1223, loss = 0.52635396\n",
      "Iteration 1224, loss = 0.52612050\n",
      "Iteration 1225, loss = 0.52588748\n",
      "Iteration 1226, loss = 0.52565449\n",
      "Iteration 1227, loss = 0.52542211\n",
      "Iteration 1228, loss = 0.52518957\n",
      "Iteration 1229, loss = 0.52495750\n",
      "Iteration 1230, loss = 0.52472557\n",
      "Iteration 1231, loss = 0.52449406\n",
      "Iteration 1232, loss = 0.52426274\n",
      "Iteration 1233, loss = 0.52403171\n",
      "Iteration 1234, loss = 0.52380096\n",
      "Iteration 1235, loss = 0.52357046\n",
      "Iteration 1236, loss = 0.52334012\n",
      "Iteration 1237, loss = 0.52311013\n",
      "Iteration 1238, loss = 0.52288016\n",
      "Iteration 1239, loss = 0.52265034\n",
      "Iteration 1240, loss = 0.52242117\n",
      "Iteration 1241, loss = 0.52219224\n",
      "Iteration 1242, loss = 0.52196321\n",
      "Iteration 1243, loss = 0.52173446\n",
      "Iteration 1244, loss = 0.52150601\n",
      "Iteration 1245, loss = 0.52127801\n",
      "Iteration 1246, loss = 0.52105041\n",
      "Iteration 1247, loss = 0.52082264\n",
      "Iteration 1248, loss = 0.52059516\n",
      "Iteration 1249, loss = 0.52036795\n",
      "Iteration 1250, loss = 0.52014110\n",
      "Iteration 1251, loss = 0.51991465\n",
      "Iteration 1252, loss = 0.51968808\n",
      "Iteration 1253, loss = 0.51946197\n",
      "Iteration 1254, loss = 0.51923651\n",
      "Iteration 1255, loss = 0.51901121\n",
      "Iteration 1256, loss = 0.51878612\n",
      "Iteration 1257, loss = 0.51856137\n",
      "Iteration 1258, loss = 0.51833694\n",
      "Iteration 1259, loss = 0.51811258\n",
      "Iteration 1260, loss = 0.51788836\n",
      "Iteration 1261, loss = 0.51766457\n",
      "Iteration 1262, loss = 0.51744098\n",
      "Iteration 1263, loss = 0.51721748\n",
      "Iteration 1264, loss = 0.51699411\n",
      "Iteration 1265, loss = 0.51677122\n",
      "Iteration 1266, loss = 0.51654841\n",
      "Iteration 1267, loss = 0.51632590\n",
      "Iteration 1268, loss = 0.51610352\n",
      "Iteration 1269, loss = 0.51588142\n",
      "Iteration 1270, loss = 0.51565948\n",
      "Iteration 1271, loss = 0.51543791\n",
      "Iteration 1272, loss = 0.51521657\n",
      "Iteration 1273, loss = 0.51499544\n",
      "Iteration 1274, loss = 0.51477446\n",
      "Iteration 1275, loss = 0.51455380\n",
      "Iteration 1276, loss = 0.51433345\n",
      "Iteration 1277, loss = 0.51411336\n",
      "Iteration 1278, loss = 0.51389349\n",
      "Iteration 1279, loss = 0.51367352\n",
      "Iteration 1280, loss = 0.51345396\n",
      "Iteration 1281, loss = 0.51323459\n",
      "Iteration 1282, loss = 0.51301565\n",
      "Iteration 1283, loss = 0.51279700\n",
      "Iteration 1284, loss = 0.51257854\n",
      "Iteration 1285, loss = 0.51236023\n",
      "Iteration 1286, loss = 0.51214211\n",
      "Iteration 1287, loss = 0.51192422\n",
      "Iteration 1288, loss = 0.51170627\n",
      "Iteration 1289, loss = 0.51148869\n",
      "Iteration 1290, loss = 0.51127155\n",
      "Iteration 1291, loss = 0.51105446\n",
      "Iteration 1292, loss = 0.51083735\n",
      "Iteration 1293, loss = 0.51062060\n",
      "Iteration 1294, loss = 0.51040416\n",
      "Iteration 1295, loss = 0.51018797\n",
      "Iteration 1296, loss = 0.50997181\n",
      "Iteration 1297, loss = 0.50975610\n",
      "Iteration 1298, loss = 0.50954027\n",
      "Iteration 1299, loss = 0.50932471\n",
      "Iteration 1300, loss = 0.50910936\n",
      "Iteration 1301, loss = 0.50889441\n",
      "Iteration 1302, loss = 0.50867963\n",
      "Iteration 1303, loss = 0.50846491\n",
      "Iteration 1304, loss = 0.50825055\n",
      "Iteration 1305, loss = 0.50803639\n",
      "Iteration 1306, loss = 0.50782239\n",
      "Iteration 1307, loss = 0.50760859\n",
      "Iteration 1308, loss = 0.50739446\n",
      "Iteration 1309, loss = 0.50718040\n",
      "Iteration 1310, loss = 0.50696627\n",
      "Iteration 1311, loss = 0.50675228\n",
      "Iteration 1312, loss = 0.50653843\n",
      "Iteration 1313, loss = 0.50632453\n",
      "Iteration 1314, loss = 0.50611063\n",
      "Iteration 1315, loss = 0.50589667\n",
      "Iteration 1316, loss = 0.50568263\n",
      "Iteration 1317, loss = 0.50546908\n",
      "Iteration 1318, loss = 0.50525556\n",
      "Iteration 1319, loss = 0.50504236\n",
      "Iteration 1320, loss = 0.50482924\n",
      "Iteration 1321, loss = 0.50461673\n",
      "Iteration 1322, loss = 0.50440427\n",
      "Iteration 1323, loss = 0.50419188\n",
      "Iteration 1324, loss = 0.50397961\n",
      "Iteration 1325, loss = 0.50376702\n",
      "Iteration 1326, loss = 0.50355492\n",
      "Iteration 1327, loss = 0.50334295\n",
      "Iteration 1328, loss = 0.50313095\n",
      "Iteration 1329, loss = 0.50291965\n",
      "Iteration 1330, loss = 0.50270866\n",
      "Iteration 1331, loss = 0.50249783\n",
      "Iteration 1332, loss = 0.50228690\n",
      "Iteration 1333, loss = 0.50207620\n",
      "Iteration 1334, loss = 0.50186588\n",
      "Iteration 1335, loss = 0.50165566\n",
      "Iteration 1336, loss = 0.50144542\n",
      "Iteration 1337, loss = 0.50123424\n",
      "Iteration 1338, loss = 0.50102309\n",
      "Iteration 1339, loss = 0.50081192\n",
      "Iteration 1340, loss = 0.50060090\n",
      "Iteration 1341, loss = 0.50038980\n",
      "Iteration 1342, loss = 0.50017862\n",
      "Iteration 1343, loss = 0.49996732\n",
      "Iteration 1344, loss = 0.49975612\n",
      "Iteration 1345, loss = 0.49954492\n",
      "Iteration 1346, loss = 0.49933391\n",
      "Iteration 1347, loss = 0.49912311\n",
      "Iteration 1348, loss = 0.49891239\n",
      "Iteration 1349, loss = 0.49870180\n",
      "Iteration 1350, loss = 0.49849049\n",
      "Iteration 1351, loss = 0.49827848\n",
      "Iteration 1352, loss = 0.49806610\n",
      "Iteration 1353, loss = 0.49785321\n",
      "Iteration 1354, loss = 0.49764035\n",
      "Iteration 1355, loss = 0.49742756\n",
      "Iteration 1356, loss = 0.49721479\n",
      "Iteration 1357, loss = 0.49700205\n",
      "Iteration 1358, loss = 0.49678995\n",
      "Iteration 1359, loss = 0.49657788\n",
      "Iteration 1360, loss = 0.49636600\n",
      "Iteration 1361, loss = 0.49615423\n",
      "Iteration 1362, loss = 0.49594252\n",
      "Iteration 1363, loss = 0.49573092\n",
      "Iteration 1364, loss = 0.49551903\n",
      "Iteration 1365, loss = 0.49530722\n",
      "Iteration 1366, loss = 0.49509592\n",
      "Iteration 1367, loss = 0.49488499\n",
      "Iteration 1368, loss = 0.49467391\n",
      "Iteration 1369, loss = 0.49446426\n",
      "Iteration 1370, loss = 0.49425491\n",
      "Iteration 1371, loss = 0.49404555\n",
      "Iteration 1372, loss = 0.49383625\n",
      "Iteration 1373, loss = 0.49362699\n",
      "Iteration 1374, loss = 0.49341815\n",
      "Iteration 1375, loss = 0.49320941\n",
      "Iteration 1376, loss = 0.49300091\n",
      "Iteration 1377, loss = 0.49279246\n",
      "Iteration 1378, loss = 0.49258397\n",
      "Iteration 1379, loss = 0.49237645\n",
      "Iteration 1380, loss = 0.49216787\n",
      "Iteration 1381, loss = 0.49195917\n",
      "Iteration 1382, loss = 0.49174954\n",
      "Iteration 1383, loss = 0.49153926\n",
      "Iteration 1384, loss = 0.49132883\n",
      "Iteration 1385, loss = 0.49111938\n",
      "Iteration 1386, loss = 0.49091141\n",
      "Iteration 1387, loss = 0.49070405\n",
      "Iteration 1388, loss = 0.49049786\n",
      "Iteration 1389, loss = 0.49029318\n",
      "Iteration 1390, loss = 0.49008938\n",
      "Iteration 1391, loss = 0.48988604\n",
      "Iteration 1392, loss = 0.48968275\n",
      "Iteration 1393, loss = 0.48947960\n",
      "Iteration 1394, loss = 0.48927691\n",
      "Iteration 1395, loss = 0.48907424\n",
      "Iteration 1396, loss = 0.48887173\n",
      "Iteration 1397, loss = 0.48866916\n",
      "Iteration 1398, loss = 0.48846625\n",
      "Iteration 1399, loss = 0.48826349\n",
      "Iteration 1400, loss = 0.48806135\n",
      "Iteration 1401, loss = 0.48785972\n",
      "Iteration 1402, loss = 0.48765821\n",
      "Iteration 1403, loss = 0.48745698\n",
      "Iteration 1404, loss = 0.48725612\n",
      "Iteration 1405, loss = 0.48705533\n",
      "Iteration 1406, loss = 0.48685434\n",
      "Iteration 1407, loss = 0.48665390\n",
      "Iteration 1408, loss = 0.48645386\n",
      "Iteration 1409, loss = 0.48625359\n",
      "Iteration 1410, loss = 0.48605379\n",
      "Iteration 1411, loss = 0.48585427\n",
      "Iteration 1412, loss = 0.48565483\n",
      "Iteration 1413, loss = 0.48545595\n",
      "Iteration 1414, loss = 0.48525692\n",
      "Iteration 1415, loss = 0.48505798\n",
      "Iteration 1416, loss = 0.48485969\n",
      "Iteration 1417, loss = 0.48466172\n",
      "Iteration 1418, loss = 0.48446405\n",
      "Iteration 1419, loss = 0.48426631\n",
      "Iteration 1420, loss = 0.48406871\n",
      "Iteration 1421, loss = 0.48387151\n",
      "Iteration 1422, loss = 0.48367447\n",
      "Iteration 1423, loss = 0.48347755\n",
      "Iteration 1424, loss = 0.48328116\n",
      "Iteration 1425, loss = 0.48308508\n",
      "Iteration 1426, loss = 0.48288894\n",
      "Iteration 1427, loss = 0.48269298\n",
      "Iteration 1428, loss = 0.48249756\n",
      "Iteration 1429, loss = 0.48230219\n",
      "Iteration 1430, loss = 0.48210685\n",
      "Iteration 1431, loss = 0.48191148\n",
      "Iteration 1432, loss = 0.48171751\n",
      "Iteration 1433, loss = 0.48152479\n",
      "Iteration 1434, loss = 0.48133225\n",
      "Iteration 1435, loss = 0.48114022\n",
      "Iteration 1436, loss = 0.48094809\n",
      "Iteration 1437, loss = 0.48075619\n",
      "Iteration 1438, loss = 0.48056469\n",
      "Iteration 1439, loss = 0.48037333\n",
      "Iteration 1440, loss = 0.48018226\n",
      "Iteration 1441, loss = 0.47999131\n",
      "Iteration 1442, loss = 0.47980059\n",
      "Iteration 1443, loss = 0.47960952\n",
      "Iteration 1444, loss = 0.47941871\n",
      "Iteration 1445, loss = 0.47922773\n",
      "Iteration 1446, loss = 0.47903682\n",
      "Iteration 1447, loss = 0.47884631\n",
      "Iteration 1448, loss = 0.47865615\n",
      "Iteration 1449, loss = 0.47846602\n",
      "Iteration 1450, loss = 0.47827568\n",
      "Iteration 1451, loss = 0.47808559\n",
      "Iteration 1452, loss = 0.47789597\n",
      "Iteration 1453, loss = 0.47770634\n",
      "Iteration 1454, loss = 0.47751671\n",
      "Iteration 1455, loss = 0.47732697\n",
      "Iteration 1456, loss = 0.47713757\n",
      "Iteration 1457, loss = 0.47694850\n",
      "Iteration 1458, loss = 0.47675937\n",
      "Iteration 1459, loss = 0.47657022\n",
      "Iteration 1460, loss = 0.47638138\n",
      "Iteration 1461, loss = 0.47619260\n",
      "Iteration 1462, loss = 0.47600400\n",
      "Iteration 1463, loss = 0.47581582\n",
      "Iteration 1464, loss = 0.47562784\n",
      "Iteration 1465, loss = 0.47543944\n",
      "Iteration 1466, loss = 0.47525063\n",
      "Iteration 1467, loss = 0.47506166\n",
      "Iteration 1468, loss = 0.47487274\n",
      "Iteration 1469, loss = 0.47468426\n",
      "Iteration 1470, loss = 0.47449391\n",
      "Iteration 1471, loss = 0.47430282\n",
      "Iteration 1472, loss = 0.47411229\n",
      "Iteration 1473, loss = 0.47392276\n",
      "Iteration 1474, loss = 0.47373349\n",
      "Iteration 1475, loss = 0.47354472\n",
      "Iteration 1476, loss = 0.47335703\n",
      "Iteration 1477, loss = 0.47317008\n",
      "Iteration 1478, loss = 0.47298408\n",
      "Iteration 1479, loss = 0.47279729\n",
      "Iteration 1480, loss = 0.47260932\n",
      "Iteration 1481, loss = 0.47242266\n",
      "Iteration 1482, loss = 0.47223739\n",
      "Iteration 1483, loss = 0.47205270\n",
      "Iteration 1484, loss = 0.47186829\n",
      "Iteration 1485, loss = 0.47168381\n",
      "Iteration 1486, loss = 0.47149954\n",
      "Iteration 1487, loss = 0.47131583\n",
      "Iteration 1488, loss = 0.47113223\n",
      "Iteration 1489, loss = 0.47094861\n",
      "Iteration 1490, loss = 0.47076552\n",
      "Iteration 1491, loss = 0.47058265\n",
      "Iteration 1492, loss = 0.47040002\n",
      "Iteration 1493, loss = 0.47021748\n",
      "Iteration 1494, loss = 0.47003498\n",
      "Iteration 1495, loss = 0.46985242\n",
      "Iteration 1496, loss = 0.46966981\n",
      "Iteration 1497, loss = 0.46948707\n",
      "Iteration 1498, loss = 0.46930413\n",
      "Iteration 1499, loss = 0.46912140\n",
      "Iteration 1500, loss = 0.46893881\n",
      "Iteration 1501, loss = 0.46875592\n",
      "Iteration 1502, loss = 0.46857310\n",
      "Iteration 1503, loss = 0.46839028\n",
      "Iteration 1504, loss = 0.46820774\n",
      "Iteration 1505, loss = 0.46802528\n",
      "Iteration 1506, loss = 0.46784271\n",
      "Iteration 1507, loss = 0.46766037\n",
      "Iteration 1508, loss = 0.46747802\n",
      "Iteration 1509, loss = 0.46729552\n",
      "Iteration 1510, loss = 0.46711304\n",
      "Iteration 1511, loss = 0.46693043\n",
      "Iteration 1512, loss = 0.46674776\n",
      "Iteration 1513, loss = 0.46656527\n",
      "Iteration 1514, loss = 0.46638259\n",
      "Iteration 1515, loss = 0.46620013\n",
      "Iteration 1516, loss = 0.46601780\n",
      "Iteration 1517, loss = 0.46583523\n",
      "Iteration 1518, loss = 0.46565294\n",
      "Iteration 1519, loss = 0.46547059\n",
      "Iteration 1520, loss = 0.46528868\n",
      "Iteration 1521, loss = 0.46510753\n",
      "Iteration 1522, loss = 0.46492655\n",
      "Iteration 1523, loss = 0.46474477\n",
      "Iteration 1524, loss = 0.46456189\n",
      "Iteration 1525, loss = 0.46437887\n",
      "Iteration 1526, loss = 0.46419802\n",
      "Iteration 1527, loss = 0.46401813\n",
      "Iteration 1528, loss = 0.46383899\n",
      "Iteration 1529, loss = 0.46366071\n",
      "Iteration 1530, loss = 0.46348256\n",
      "Iteration 1531, loss = 0.46330447\n",
      "Iteration 1532, loss = 0.46312644\n",
      "Iteration 1533, loss = 0.46294836\n",
      "Iteration 1534, loss = 0.46277034\n",
      "Iteration 1535, loss = 0.46259276\n",
      "Iteration 1536, loss = 0.46241553\n",
      "Iteration 1537, loss = 0.46223865\n",
      "Iteration 1538, loss = 0.46206162\n",
      "Iteration 1539, loss = 0.46188487\n",
      "Iteration 1540, loss = 0.46170810\n",
      "Iteration 1541, loss = 0.46153145\n",
      "Iteration 1542, loss = 0.46135539\n",
      "Iteration 1543, loss = 0.46118000\n",
      "Iteration 1544, loss = 0.46100471\n",
      "Iteration 1545, loss = 0.46082995\n",
      "Iteration 1546, loss = 0.46065532\n",
      "Iteration 1547, loss = 0.46048078\n",
      "Iteration 1548, loss = 0.46030638\n",
      "Iteration 1549, loss = 0.46013213\n",
      "Iteration 1550, loss = 0.45995795\n",
      "Iteration 1551, loss = 0.45978422\n",
      "Iteration 1552, loss = 0.45961065\n",
      "Iteration 1553, loss = 0.45943717\n",
      "Iteration 1554, loss = 0.45926375\n",
      "Iteration 1555, loss = 0.45909073\n",
      "Iteration 1556, loss = 0.45891809\n",
      "Iteration 1557, loss = 0.45874562\n",
      "Iteration 1558, loss = 0.45857319\n",
      "Iteration 1559, loss = 0.45840112\n",
      "Iteration 1560, loss = 0.45822917\n",
      "Iteration 1561, loss = 0.45805763\n",
      "Iteration 1562, loss = 0.45788594\n",
      "Iteration 1563, loss = 0.45771450\n",
      "Iteration 1564, loss = 0.45754336\n",
      "Iteration 1565, loss = 0.45737235\n",
      "Iteration 1566, loss = 0.45720131\n",
      "Iteration 1567, loss = 0.45703051\n",
      "Iteration 1568, loss = 0.45685963\n",
      "Iteration 1569, loss = 0.45668898\n",
      "Iteration 1570, loss = 0.45651873\n",
      "Iteration 1571, loss = 0.45634844\n",
      "Iteration 1572, loss = 0.45617813\n",
      "Iteration 1573, loss = 0.45600778\n",
      "Iteration 1574, loss = 0.45583766\n",
      "Iteration 1575, loss = 0.45566747\n",
      "Iteration 1576, loss = 0.45549761\n",
      "Iteration 1577, loss = 0.45532763\n",
      "Iteration 1578, loss = 0.45515781\n",
      "Iteration 1579, loss = 0.45498819\n",
      "Iteration 1580, loss = 0.45481856\n",
      "Iteration 1581, loss = 0.45464938\n",
      "Iteration 1582, loss = 0.45448018\n",
      "Iteration 1583, loss = 0.45431109\n",
      "Iteration 1584, loss = 0.45414218\n",
      "Iteration 1585, loss = 0.45397357\n",
      "Iteration 1586, loss = 0.45380502\n",
      "Iteration 1587, loss = 0.45363675\n",
      "Iteration 1588, loss = 0.45346833\n",
      "Iteration 1589, loss = 0.45330055\n",
      "Iteration 1590, loss = 0.45313274\n",
      "Iteration 1591, loss = 0.45296496\n",
      "Iteration 1592, loss = 0.45279749\n",
      "Iteration 1593, loss = 0.45263035\n",
      "Iteration 1594, loss = 0.45246326\n",
      "Iteration 1595, loss = 0.45229616\n",
      "Iteration 1596, loss = 0.45212925\n",
      "Iteration 1597, loss = 0.45196253\n",
      "Iteration 1598, loss = 0.45179611\n",
      "Iteration 1599, loss = 0.45162972\n",
      "Iteration 1600, loss = 0.45146362\n",
      "Iteration 1601, loss = 0.45129743\n",
      "Iteration 1602, loss = 0.45113164\n",
      "Iteration 1603, loss = 0.45096600\n",
      "Iteration 1604, loss = 0.45080058\n",
      "Iteration 1605, loss = 0.45063528\n",
      "Iteration 1606, loss = 0.45046996\n",
      "Iteration 1607, loss = 0.45030450\n",
      "Iteration 1608, loss = 0.45013897\n",
      "Iteration 1609, loss = 0.44997334\n",
      "Iteration 1610, loss = 0.44980756\n",
      "Iteration 1611, loss = 0.44964167\n",
      "Iteration 1612, loss = 0.44947575\n",
      "Iteration 1613, loss = 0.44930998\n",
      "Iteration 1614, loss = 0.44914334\n",
      "Iteration 1615, loss = 0.44897603\n",
      "Iteration 1616, loss = 0.44880856\n",
      "Iteration 1617, loss = 0.44864091\n",
      "Iteration 1618, loss = 0.44847304\n",
      "Iteration 1619, loss = 0.44830586\n",
      "Iteration 1620, loss = 0.44813774\n",
      "Iteration 1621, loss = 0.44797030\n",
      "Iteration 1622, loss = 0.44780299\n",
      "Iteration 1623, loss = 0.44763595\n",
      "Iteration 1624, loss = 0.44746927\n",
      "Iteration 1625, loss = 0.44730373\n",
      "Iteration 1626, loss = 0.44713856\n",
      "Iteration 1627, loss = 0.44697375\n",
      "Iteration 1628, loss = 0.44681004\n",
      "Iteration 1629, loss = 0.44664677\n",
      "Iteration 1630, loss = 0.44648380\n",
      "Iteration 1631, loss = 0.44632109\n",
      "Iteration 1632, loss = 0.44615855\n",
      "Iteration 1633, loss = 0.44599607\n",
      "Iteration 1634, loss = 0.44583404\n",
      "Iteration 1635, loss = 0.44567214\n",
      "Iteration 1636, loss = 0.44551011\n",
      "Iteration 1637, loss = 0.44534865\n",
      "Iteration 1638, loss = 0.44518741\n",
      "Iteration 1639, loss = 0.44502632\n",
      "Iteration 1640, loss = 0.44486533\n",
      "Iteration 1641, loss = 0.44470447\n",
      "Iteration 1642, loss = 0.44454396\n",
      "Iteration 1643, loss = 0.44438400\n",
      "Iteration 1644, loss = 0.44422406\n",
      "Iteration 1645, loss = 0.44406420\n",
      "Iteration 1646, loss = 0.44390468\n",
      "Iteration 1647, loss = 0.44374531\n",
      "Iteration 1648, loss = 0.44358627\n",
      "Iteration 1649, loss = 0.44342719\n",
      "Iteration 1650, loss = 0.44326831\n",
      "Iteration 1651, loss = 0.44310970\n",
      "Iteration 1652, loss = 0.44295112\n",
      "Iteration 1653, loss = 0.44279282\n",
      "Iteration 1654, loss = 0.44263477\n",
      "Iteration 1655, loss = 0.44247655\n",
      "Iteration 1656, loss = 0.44231860\n",
      "Iteration 1657, loss = 0.44216064\n",
      "Iteration 1658, loss = 0.44200292\n",
      "Iteration 1659, loss = 0.44184540\n",
      "Iteration 1660, loss = 0.44168815\n",
      "Iteration 1661, loss = 0.44153096\n",
      "Iteration 1662, loss = 0.44137395\n",
      "Iteration 1663, loss = 0.44121702\n",
      "Iteration 1664, loss = 0.44106018\n",
      "Iteration 1665, loss = 0.44090346\n",
      "Iteration 1666, loss = 0.44074696\n",
      "Iteration 1667, loss = 0.44059093\n",
      "Iteration 1668, loss = 0.44043494\n",
      "Iteration 1669, loss = 0.44027919\n",
      "Iteration 1670, loss = 0.44012358\n",
      "Iteration 1671, loss = 0.43996802\n",
      "Iteration 1672, loss = 0.43981255\n",
      "Iteration 1673, loss = 0.43965745\n",
      "Iteration 1674, loss = 0.43950286\n",
      "Iteration 1675, loss = 0.43934852\n",
      "Iteration 1676, loss = 0.43919434\n",
      "Iteration 1677, loss = 0.43904033\n",
      "Iteration 1678, loss = 0.43888632\n",
      "Iteration 1679, loss = 0.43873246\n",
      "Iteration 1680, loss = 0.43857865\n",
      "Iteration 1681, loss = 0.43842485\n",
      "Iteration 1682, loss = 0.43827128\n",
      "Iteration 1683, loss = 0.43811761\n",
      "Iteration 1684, loss = 0.43796392\n",
      "Iteration 1685, loss = 0.43781073\n",
      "Iteration 1686, loss = 0.43765745\n",
      "Iteration 1687, loss = 0.43750418\n",
      "Iteration 1688, loss = 0.43735127\n",
      "Iteration 1689, loss = 0.43719859\n",
      "Iteration 1690, loss = 0.43704571\n",
      "Iteration 1691, loss = 0.43689311\n",
      "Iteration 1692, loss = 0.43674057\n",
      "Iteration 1693, loss = 0.43658822\n",
      "Iteration 1694, loss = 0.43643598\n",
      "Iteration 1695, loss = 0.43628397\n",
      "Iteration 1696, loss = 0.43613231\n",
      "Iteration 1697, loss = 0.43598061\n",
      "Iteration 1698, loss = 0.43582905\n",
      "Iteration 1699, loss = 0.43567771\n",
      "Iteration 1700, loss = 0.43552655\n",
      "Iteration 1701, loss = 0.43537551\n",
      "Iteration 1702, loss = 0.43522465\n",
      "Iteration 1703, loss = 0.43507356\n",
      "Iteration 1704, loss = 0.43492248\n",
      "Iteration 1705, loss = 0.43477168\n",
      "Iteration 1706, loss = 0.43462087\n",
      "Iteration 1707, loss = 0.43447027\n",
      "Iteration 1708, loss = 0.43431987\n",
      "Iteration 1709, loss = 0.43416971\n",
      "Iteration 1710, loss = 0.43401950\n",
      "Iteration 1711, loss = 0.43386940\n",
      "Iteration 1712, loss = 0.43371947\n",
      "Iteration 1713, loss = 0.43356969\n",
      "Iteration 1714, loss = 0.43342019\n",
      "Iteration 1715, loss = 0.43327046\n",
      "Iteration 1716, loss = 0.43312122\n",
      "Iteration 1717, loss = 0.43297206\n",
      "Iteration 1718, loss = 0.43282306\n",
      "Iteration 1719, loss = 0.43267435\n",
      "Iteration 1720, loss = 0.43252557\n",
      "Iteration 1721, loss = 0.43237665\n",
      "Iteration 1722, loss = 0.43222750\n",
      "Iteration 1723, loss = 0.43207835\n",
      "Iteration 1724, loss = 0.43192924\n",
      "Iteration 1725, loss = 0.43178006\n",
      "Iteration 1726, loss = 0.43163091\n",
      "Iteration 1727, loss = 0.43148190\n",
      "Iteration 1728, loss = 0.43133294\n",
      "Iteration 1729, loss = 0.43118413\n",
      "Iteration 1730, loss = 0.43103554\n",
      "Iteration 1731, loss = 0.43088673\n",
      "Iteration 1732, loss = 0.43073828\n",
      "Iteration 1733, loss = 0.43058976\n",
      "Iteration 1734, loss = 0.43044128\n",
      "Iteration 1735, loss = 0.43029294\n",
      "Iteration 1736, loss = 0.43014460\n",
      "Iteration 1737, loss = 0.42999639\n",
      "Iteration 1738, loss = 0.42984822\n",
      "Iteration 1739, loss = 0.42970020\n",
      "Iteration 1740, loss = 0.42955228\n",
      "Iteration 1741, loss = 0.42940453\n",
      "Iteration 1742, loss = 0.42925678\n",
      "Iteration 1743, loss = 0.42910922\n",
      "Iteration 1744, loss = 0.42896180\n",
      "Iteration 1745, loss = 0.42881456\n",
      "Iteration 1746, loss = 0.42866753\n",
      "Iteration 1747, loss = 0.42852046\n",
      "Iteration 1748, loss = 0.42837379\n",
      "Iteration 1749, loss = 0.42822725\n",
      "Iteration 1750, loss = 0.42808092\n",
      "Iteration 1751, loss = 0.42793465\n",
      "Iteration 1752, loss = 0.42778834\n",
      "Iteration 1753, loss = 0.42764229\n",
      "Iteration 1754, loss = 0.42749634\n",
      "Iteration 1755, loss = 0.42735026\n",
      "Iteration 1756, loss = 0.42720442\n",
      "Iteration 1757, loss = 0.42705854\n",
      "Iteration 1758, loss = 0.42691303\n",
      "Iteration 1759, loss = 0.42676743\n",
      "Iteration 1760, loss = 0.42662195\n",
      "Iteration 1761, loss = 0.42647648\n",
      "Iteration 1762, loss = 0.42633150\n",
      "Iteration 1763, loss = 0.42618647\n",
      "Iteration 1764, loss = 0.42604159\n",
      "Iteration 1765, loss = 0.42589682\n",
      "Iteration 1766, loss = 0.42575205\n",
      "Iteration 1767, loss = 0.42560752\n",
      "Iteration 1768, loss = 0.42546323\n",
      "Iteration 1769, loss = 0.42531891\n",
      "Iteration 1770, loss = 0.42517487\n",
      "Iteration 1771, loss = 0.42503114\n",
      "Iteration 1772, loss = 0.42488723\n",
      "Iteration 1773, loss = 0.42474371\n",
      "Iteration 1774, loss = 0.42460008\n",
      "Iteration 1775, loss = 0.42445668\n",
      "Iteration 1776, loss = 0.42431358\n",
      "Iteration 1777, loss = 0.42417058\n",
      "Iteration 1778, loss = 0.42402763\n",
      "Iteration 1779, loss = 0.42388484\n",
      "Iteration 1780, loss = 0.42374203\n",
      "Iteration 1781, loss = 0.42359950\n",
      "Iteration 1782, loss = 0.42345719\n",
      "Iteration 1783, loss = 0.42331494\n",
      "Iteration 1784, loss = 0.42317301\n",
      "Iteration 1785, loss = 0.42303116\n",
      "Iteration 1786, loss = 0.42288931\n",
      "Iteration 1787, loss = 0.42274783\n",
      "Iteration 1788, loss = 0.42260620\n",
      "Iteration 1789, loss = 0.42246466\n",
      "Iteration 1790, loss = 0.42232334\n",
      "Iteration 1791, loss = 0.42218202\n",
      "Iteration 1792, loss = 0.42204073\n",
      "Iteration 1793, loss = 0.42189964\n",
      "Iteration 1794, loss = 0.42175886\n",
      "Iteration 1795, loss = 0.42161808\n",
      "Iteration 1796, loss = 0.42147728\n",
      "Iteration 1797, loss = 0.42133668\n",
      "Iteration 1798, loss = 0.42119630\n",
      "Iteration 1799, loss = 0.42105589\n",
      "Iteration 1800, loss = 0.42091569\n",
      "Iteration 1801, loss = 0.42077556\n",
      "Iteration 1802, loss = 0.42063622\n",
      "Iteration 1803, loss = 0.42049679\n",
      "Iteration 1804, loss = 0.42035735\n",
      "Iteration 1805, loss = 0.42021811\n",
      "Iteration 1806, loss = 0.42007880\n",
      "Iteration 1807, loss = 0.41993935\n",
      "Iteration 1808, loss = 0.41980053\n",
      "Iteration 1809, loss = 0.41966190\n",
      "Iteration 1810, loss = 0.41952345\n",
      "Iteration 1811, loss = 0.41938504\n",
      "Iteration 1812, loss = 0.41924659\n",
      "Iteration 1813, loss = 0.41910837\n",
      "Iteration 1814, loss = 0.41897022\n",
      "Iteration 1815, loss = 0.41883219\n",
      "Iteration 1816, loss = 0.41869418\n",
      "Iteration 1817, loss = 0.41855662\n",
      "Iteration 1818, loss = 0.41841914\n",
      "Iteration 1819, loss = 0.41828153\n",
      "Iteration 1820, loss = 0.41814437\n",
      "Iteration 1821, loss = 0.41800746\n",
      "Iteration 1822, loss = 0.41787041\n",
      "Iteration 1823, loss = 0.41773365\n",
      "Iteration 1824, loss = 0.41759698\n",
      "Iteration 1825, loss = 0.41746050\n",
      "Iteration 1826, loss = 0.41732428\n",
      "Iteration 1827, loss = 0.41718808\n",
      "Iteration 1828, loss = 0.41705211\n",
      "Iteration 1829, loss = 0.41691628\n",
      "Iteration 1830, loss = 0.41678040\n",
      "Iteration 1831, loss = 0.41664476\n",
      "Iteration 1832, loss = 0.41650933\n",
      "Iteration 1833, loss = 0.41637488\n",
      "Iteration 1834, loss = 0.41624079\n",
      "Iteration 1835, loss = 0.41610692\n",
      "Iteration 1836, loss = 0.41597329\n",
      "Iteration 1837, loss = 0.41583931\n",
      "Iteration 1838, loss = 0.41570571\n",
      "Iteration 1839, loss = 0.41557219\n",
      "Iteration 1840, loss = 0.41543890\n",
      "Iteration 1841, loss = 0.41530552\n",
      "Iteration 1842, loss = 0.41517235\n",
      "Iteration 1843, loss = 0.41503931\n",
      "Iteration 1844, loss = 0.41490631\n",
      "Iteration 1845, loss = 0.41477350\n",
      "Iteration 1846, loss = 0.41464077\n",
      "Iteration 1847, loss = 0.41450808\n",
      "Iteration 1848, loss = 0.41437553\n",
      "Iteration 1849, loss = 0.41424312\n",
      "Iteration 1850, loss = 0.41411110\n",
      "Iteration 1851, loss = 0.41397910\n",
      "Iteration 1852, loss = 0.41384701\n",
      "Iteration 1853, loss = 0.41371524\n",
      "Iteration 1854, loss = 0.41358371\n",
      "Iteration 1855, loss = 0.41345208\n",
      "Iteration 1856, loss = 0.41332058\n",
      "Iteration 1857, loss = 0.41318925\n",
      "Iteration 1858, loss = 0.41305805\n",
      "Iteration 1859, loss = 0.41292699\n",
      "Iteration 1860, loss = 0.41279626\n",
      "Iteration 1861, loss = 0.41266549\n",
      "Iteration 1862, loss = 0.41253470\n",
      "Iteration 1863, loss = 0.41240409\n",
      "Iteration 1864, loss = 0.41227374\n",
      "Iteration 1865, loss = 0.41214355\n",
      "Iteration 1866, loss = 0.41201345\n",
      "Iteration 1867, loss = 0.41188342\n",
      "Iteration 1868, loss = 0.41175347\n",
      "Iteration 1869, loss = 0.41162387\n",
      "Iteration 1870, loss = 0.41149411\n",
      "Iteration 1871, loss = 0.41136446\n",
      "Iteration 1872, loss = 0.41123514\n",
      "Iteration 1873, loss = 0.41110585\n",
      "Iteration 1874, loss = 0.41097669\n",
      "Iteration 1875, loss = 0.41084762\n",
      "Iteration 1876, loss = 0.41071867\n",
      "Iteration 1877, loss = 0.41058980\n",
      "Iteration 1878, loss = 0.41046106\n",
      "Iteration 1879, loss = 0.41033236\n",
      "Iteration 1880, loss = 0.41020392\n",
      "Iteration 1881, loss = 0.41007563\n",
      "Iteration 1882, loss = 0.40994752\n",
      "Iteration 1883, loss = 0.40981949\n",
      "Iteration 1884, loss = 0.40969153\n",
      "Iteration 1885, loss = 0.40956391\n",
      "Iteration 1886, loss = 0.40943634\n",
      "Iteration 1887, loss = 0.40930876\n",
      "Iteration 1888, loss = 0.40918151\n",
      "Iteration 1889, loss = 0.40905430\n",
      "Iteration 1890, loss = 0.40892725\n",
      "Iteration 1891, loss = 0.40880023\n",
      "Iteration 1892, loss = 0.40867326\n",
      "Iteration 1893, loss = 0.40854654\n",
      "Iteration 1894, loss = 0.40841983\n",
      "Iteration 1895, loss = 0.40829369\n",
      "Iteration 1896, loss = 0.40816725\n",
      "Iteration 1897, loss = 0.40804101\n",
      "Iteration 1898, loss = 0.40791514\n",
      "Iteration 1899, loss = 0.40778925\n",
      "Iteration 1900, loss = 0.40766315\n",
      "Iteration 1901, loss = 0.40753735\n",
      "Iteration 1902, loss = 0.40741185\n",
      "Iteration 1903, loss = 0.40728634\n",
      "Iteration 1904, loss = 0.40716093\n",
      "Iteration 1905, loss = 0.40703572\n",
      "Iteration 1906, loss = 0.40691051\n",
      "Iteration 1907, loss = 0.40678552\n",
      "Iteration 1908, loss = 0.40666067\n",
      "Iteration 1909, loss = 0.40653588\n",
      "Iteration 1910, loss = 0.40641128\n",
      "Iteration 1911, loss = 0.40628673\n",
      "Iteration 1912, loss = 0.40616223\n",
      "Iteration 1913, loss = 0.40603803\n",
      "Iteration 1914, loss = 0.40591411\n",
      "Iteration 1915, loss = 0.40578997\n",
      "Iteration 1916, loss = 0.40566596\n",
      "Iteration 1917, loss = 0.40554213\n",
      "Iteration 1918, loss = 0.40541811\n",
      "Iteration 1919, loss = 0.40529360\n",
      "Iteration 1920, loss = 0.40516903\n",
      "Iteration 1921, loss = 0.40504440\n",
      "Iteration 1922, loss = 0.40492045\n",
      "Iteration 1923, loss = 0.40479661\n",
      "Iteration 1924, loss = 0.40467261\n",
      "Iteration 1925, loss = 0.40454909\n",
      "Iteration 1926, loss = 0.40442552\n",
      "Iteration 1927, loss = 0.40430222\n",
      "Iteration 1928, loss = 0.40417921\n",
      "Iteration 1929, loss = 0.40405618\n",
      "Iteration 1930, loss = 0.40393318\n",
      "Iteration 1931, loss = 0.40381028\n",
      "Iteration 1932, loss = 0.40368758\n",
      "Iteration 1933, loss = 0.40356499\n",
      "Iteration 1934, loss = 0.40344243\n",
      "Iteration 1935, loss = 0.40331995\n",
      "Iteration 1936, loss = 0.40319757\n",
      "Iteration 1937, loss = 0.40307536\n",
      "Iteration 1938, loss = 0.40295318\n",
      "Iteration 1939, loss = 0.40283109\n",
      "Iteration 1940, loss = 0.40270950\n",
      "Iteration 1941, loss = 0.40258793\n",
      "Iteration 1942, loss = 0.40246678\n",
      "Iteration 1943, loss = 0.40234561\n",
      "Iteration 1944, loss = 0.40222460\n",
      "Iteration 1945, loss = 0.40210361\n",
      "Iteration 1946, loss = 0.40198263\n",
      "Iteration 1947, loss = 0.40186171\n",
      "Iteration 1948, loss = 0.40174063\n",
      "Iteration 1949, loss = 0.40161927\n",
      "Iteration 1950, loss = 0.40149803\n",
      "Iteration 1951, loss = 0.40137660\n",
      "Iteration 1952, loss = 0.40125487\n",
      "Iteration 1953, loss = 0.40113318\n",
      "Iteration 1954, loss = 0.40101150\n",
      "Iteration 1955, loss = 0.40088978\n",
      "Iteration 1956, loss = 0.40076793\n",
      "Iteration 1957, loss = 0.40064601\n",
      "Iteration 1958, loss = 0.40052411\n",
      "Iteration 1959, loss = 0.40040257\n",
      "Iteration 1960, loss = 0.40028093\n",
      "Iteration 1961, loss = 0.40015947\n",
      "Iteration 1962, loss = 0.40003819\n",
      "Iteration 1963, loss = 0.39991816\n",
      "Iteration 1964, loss = 0.39979846\n",
      "Iteration 1965, loss = 0.39967872\n",
      "Iteration 1966, loss = 0.39955954\n",
      "Iteration 1967, loss = 0.39944039\n",
      "Iteration 1968, loss = 0.39932122\n",
      "Iteration 1969, loss = 0.39920206\n",
      "Iteration 1970, loss = 0.39908325\n",
      "Iteration 1971, loss = 0.39896462\n",
      "Iteration 1972, loss = 0.39884595\n",
      "Iteration 1973, loss = 0.39872747\n",
      "Iteration 1974, loss = 0.39860913\n",
      "Iteration 1975, loss = 0.39849081\n",
      "Iteration 1976, loss = 0.39837269\n",
      "Iteration 1977, loss = 0.39825473\n",
      "Iteration 1978, loss = 0.39813690\n",
      "Iteration 1979, loss = 0.39801943\n",
      "Iteration 1980, loss = 0.39790178\n",
      "Iteration 1981, loss = 0.39778444\n",
      "Iteration 1982, loss = 0.39766719\n",
      "Iteration 1983, loss = 0.39755005\n",
      "Iteration 1984, loss = 0.39743306\n",
      "Iteration 1985, loss = 0.39731614\n",
      "Iteration 1986, loss = 0.39719963\n",
      "Iteration 1987, loss = 0.39708307\n",
      "Iteration 1988, loss = 0.39696645\n",
      "Iteration 1989, loss = 0.39684979\n",
      "Iteration 1990, loss = 0.39673378\n",
      "Iteration 1991, loss = 0.39661772\n",
      "Iteration 1992, loss = 0.39650177\n",
      "Iteration 1993, loss = 0.39638588\n",
      "Iteration 1994, loss = 0.39626994\n",
      "Iteration 1995, loss = 0.39615427\n",
      "Iteration 1996, loss = 0.39603878\n",
      "Iteration 1997, loss = 0.39592332\n",
      "Iteration 1998, loss = 0.39580800\n",
      "Iteration 1999, loss = 0.39569282\n",
      "Iteration 2000, loss = 0.39557774\n",
      "Iteration 2001, loss = 0.39546275\n",
      "Iteration 2002, loss = 0.39534778\n",
      "Iteration 2003, loss = 0.39523287\n",
      "Iteration 2004, loss = 0.39511807\n",
      "Iteration 2005, loss = 0.39500333\n",
      "Iteration 2006, loss = 0.39488870\n",
      "Iteration 2007, loss = 0.39477412\n",
      "Iteration 2008, loss = 0.39465986\n",
      "Iteration 2009, loss = 0.39454564\n",
      "Iteration 2010, loss = 0.39443136\n",
      "Iteration 2011, loss = 0.39431712\n",
      "Iteration 2012, loss = 0.39420281\n",
      "Iteration 2013, loss = 0.39408886\n",
      "Iteration 2014, loss = 0.39397495\n",
      "Iteration 2015, loss = 0.39386088\n",
      "Iteration 2016, loss = 0.39374698\n",
      "Iteration 2017, loss = 0.39363315\n",
      "Iteration 2018, loss = 0.39351931\n",
      "Iteration 2019, loss = 0.39340566\n",
      "Iteration 2020, loss = 0.39329211\n",
      "Iteration 2021, loss = 0.39317851\n",
      "Iteration 2022, loss = 0.39306493\n",
      "Iteration 2023, loss = 0.39295176\n",
      "Iteration 2024, loss = 0.39283837\n",
      "Iteration 2025, loss = 0.39272487\n",
      "Iteration 2026, loss = 0.39261156\n",
      "Iteration 2027, loss = 0.39249831\n",
      "Iteration 2028, loss = 0.39238507\n",
      "Iteration 2029, loss = 0.39227167\n",
      "Iteration 2030, loss = 0.39215859\n",
      "Iteration 2031, loss = 0.39204566\n",
      "Iteration 2032, loss = 0.39193259\n",
      "Iteration 2033, loss = 0.39181967\n",
      "Iteration 2034, loss = 0.39170696\n",
      "Iteration 2035, loss = 0.39159444\n",
      "Iteration 2036, loss = 0.39148166\n",
      "Iteration 2037, loss = 0.39136899\n",
      "Iteration 2038, loss = 0.39125686\n",
      "Iteration 2039, loss = 0.39114459\n",
      "Iteration 2040, loss = 0.39103232\n",
      "Iteration 2041, loss = 0.39092018\n",
      "Iteration 2042, loss = 0.39080816\n",
      "Iteration 2043, loss = 0.39069630\n",
      "Iteration 2044, loss = 0.39058433\n",
      "Iteration 2045, loss = 0.39047228\n",
      "Iteration 2046, loss = 0.39036050\n",
      "Iteration 2047, loss = 0.39024886\n",
      "Iteration 2048, loss = 0.39013726\n",
      "Iteration 2049, loss = 0.39002556\n",
      "Iteration 2050, loss = 0.38991409\n",
      "Iteration 2051, loss = 0.38980285\n",
      "Iteration 2052, loss = 0.38969157\n",
      "Iteration 2053, loss = 0.38958044\n",
      "Iteration 2054, loss = 0.38946922\n",
      "Iteration 2055, loss = 0.38935803\n",
      "Iteration 2056, loss = 0.38924720\n",
      "Iteration 2057, loss = 0.38913625\n",
      "Iteration 2058, loss = 0.38902523\n",
      "Iteration 2059, loss = 0.38891444\n",
      "Iteration 2060, loss = 0.38880399\n",
      "Iteration 2061, loss = 0.38869354\n",
      "Iteration 2062, loss = 0.38858319\n",
      "Iteration 2063, loss = 0.38847279\n",
      "Iteration 2064, loss = 0.38836240\n",
      "Iteration 2065, loss = 0.38825247\n",
      "Iteration 2066, loss = 0.38814249\n",
      "Iteration 2067, loss = 0.38803263\n",
      "Iteration 2068, loss = 0.38792277\n",
      "Iteration 2069, loss = 0.38781314\n",
      "Iteration 2070, loss = 0.38770362\n",
      "Iteration 2071, loss = 0.38759402\n",
      "Iteration 2072, loss = 0.38748487\n",
      "Iteration 2073, loss = 0.38737578\n",
      "Iteration 2074, loss = 0.38726661\n",
      "Iteration 2075, loss = 0.38715761\n",
      "Iteration 2076, loss = 0.38704860\n",
      "Iteration 2077, loss = 0.38693980\n",
      "Iteration 2078, loss = 0.38683117\n",
      "Iteration 2079, loss = 0.38672236\n",
      "Iteration 2080, loss = 0.38661386\n",
      "Iteration 2081, loss = 0.38650534\n",
      "Iteration 2082, loss = 0.38639698\n",
      "Iteration 2083, loss = 0.38628898\n",
      "Iteration 2084, loss = 0.38618093\n",
      "Iteration 2085, loss = 0.38607284\n",
      "Iteration 2086, loss = 0.38596491\n",
      "Iteration 2087, loss = 0.38585708\n",
      "Iteration 2088, loss = 0.38574935\n",
      "Iteration 2089, loss = 0.38564166\n",
      "Iteration 2090, loss = 0.38553402\n",
      "Iteration 2091, loss = 0.38542659\n",
      "Iteration 2092, loss = 0.38531944\n",
      "Iteration 2093, loss = 0.38521217\n",
      "Iteration 2094, loss = 0.38510498\n",
      "Iteration 2095, loss = 0.38499761\n",
      "Iteration 2096, loss = 0.38489051\n",
      "Iteration 2097, loss = 0.38478354\n",
      "Iteration 2098, loss = 0.38467658\n",
      "Iteration 2099, loss = 0.38456954\n",
      "Iteration 2100, loss = 0.38446251\n",
      "Iteration 2101, loss = 0.38435560\n",
      "Iteration 2102, loss = 0.38424889\n",
      "Iteration 2103, loss = 0.38414216\n",
      "Iteration 2104, loss = 0.38403550\n",
      "Iteration 2105, loss = 0.38392896\n",
      "Iteration 2106, loss = 0.38382203\n",
      "Iteration 2107, loss = 0.38371548\n",
      "Iteration 2108, loss = 0.38360866\n",
      "Iteration 2109, loss = 0.38350171\n",
      "Iteration 2110, loss = 0.38339472\n",
      "Iteration 2111, loss = 0.38328766\n",
      "Iteration 2112, loss = 0.38318110\n",
      "Iteration 2113, loss = 0.38307432\n",
      "Iteration 2114, loss = 0.38296787\n",
      "Iteration 2115, loss = 0.38286163\n",
      "Iteration 2116, loss = 0.38275607\n",
      "Iteration 2117, loss = 0.38264977\n",
      "Iteration 2118, loss = 0.38254347\n",
      "Iteration 2119, loss = 0.38243736\n",
      "Iteration 2120, loss = 0.38233133\n",
      "Iteration 2121, loss = 0.38222506\n",
      "Iteration 2122, loss = 0.38211908\n",
      "Iteration 2123, loss = 0.38201317\n",
      "Iteration 2124, loss = 0.38190703\n",
      "Iteration 2125, loss = 0.38180109\n",
      "Iteration 2126, loss = 0.38169529\n",
      "Iteration 2127, loss = 0.38158958\n",
      "Iteration 2128, loss = 0.38148470\n",
      "Iteration 2129, loss = 0.38137966\n",
      "Iteration 2130, loss = 0.38127440\n",
      "Iteration 2131, loss = 0.38116956\n",
      "Iteration 2132, loss = 0.38106540\n",
      "Iteration 2133, loss = 0.38096115\n",
      "Iteration 2134, loss = 0.38085684\n",
      "Iteration 2135, loss = 0.38075283\n",
      "Iteration 2136, loss = 0.38064874\n",
      "Iteration 2137, loss = 0.38054480\n",
      "Iteration 2138, loss = 0.38044113\n",
      "Iteration 2139, loss = 0.38033744\n",
      "Iteration 2140, loss = 0.38023387\n",
      "Iteration 2141, loss = 0.38013052\n",
      "Iteration 2142, loss = 0.38002729\n",
      "Iteration 2143, loss = 0.37992409\n",
      "Iteration 2144, loss = 0.37982115\n",
      "Iteration 2145, loss = 0.37971820\n",
      "Iteration 2146, loss = 0.37961529\n",
      "Iteration 2147, loss = 0.37951275\n",
      "Iteration 2148, loss = 0.37941027\n",
      "Iteration 2149, loss = 0.37930778\n",
      "Iteration 2150, loss = 0.37920526\n",
      "Iteration 2151, loss = 0.37910317\n",
      "Iteration 2152, loss = 0.37900109\n",
      "Iteration 2153, loss = 0.37889907\n",
      "Iteration 2154, loss = 0.37879717\n",
      "Iteration 2155, loss = 0.37869526\n",
      "Iteration 2156, loss = 0.37859368\n",
      "Iteration 2157, loss = 0.37849224\n",
      "Iteration 2158, loss = 0.37839074\n",
      "Iteration 2159, loss = 0.37828936\n",
      "Iteration 2160, loss = 0.37818817\n",
      "Iteration 2161, loss = 0.37808710\n",
      "Iteration 2162, loss = 0.37798590\n",
      "Iteration 2163, loss = 0.37788502\n",
      "Iteration 2164, loss = 0.37778437\n",
      "Iteration 2165, loss = 0.37768371\n",
      "Iteration 2166, loss = 0.37758305\n",
      "Iteration 2167, loss = 0.37748244\n",
      "Iteration 2168, loss = 0.37738217\n",
      "Iteration 2169, loss = 0.37728212\n",
      "Iteration 2170, loss = 0.37718207\n",
      "Iteration 2171, loss = 0.37708200\n",
      "Iteration 2172, loss = 0.37698212\n",
      "Iteration 2173, loss = 0.37688242\n",
      "Iteration 2174, loss = 0.37678272\n",
      "Iteration 2175, loss = 0.37668303\n",
      "Iteration 2176, loss = 0.37658334\n",
      "Iteration 2177, loss = 0.37648370\n",
      "Iteration 2178, loss = 0.37638422\n",
      "Iteration 2179, loss = 0.37628480\n",
      "Iteration 2180, loss = 0.37618533\n",
      "Iteration 2181, loss = 0.37608589\n",
      "Iteration 2182, loss = 0.37598668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Cross-validated AUC: 0.9167\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# random seed to ensure consistency\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# import cohort 1\n",
    "cohort_1 = pd.read_csv(\"AthleteTBIData.csv\") # original file name\n",
    "\n",
    "cohort_1_filtered = cohort_1[cohort_1[\"OrigiinalGroupID\"].str.contains(\"post 6 hours\", na=False)]\n",
    "\n",
    "# preprocessing\n",
    "cohort_1_filtered = cohort_1_filtered.drop(columns=['SampleID', 'OrigiinalGroupID'])\n",
    "cohort_1_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# split dataset\n",
    "X = cohort_1_filtered.drop(columns=['GroupNo'])  \n",
    "y = cohort_1_filtered['GroupNo'].values  # Ensure y is a NumPy array\n",
    "\n",
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim * 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "          \n",
    "    nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# Convert to PyTorch \n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "vae = VAE(input_dim, latent_dim=10)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Train VAE\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reconstructed, mu, log_var = vae(X_tensor)\n",
    "    reconstruction_loss = loss_fn(reconstructed, X_tensor)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    loss = reconstruction_loss + kl_divergence\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate 20 samples (identified through cross-validation optimization algorithm)\n",
    "num_synthetic_samples = 20\n",
    "fixed_z_sample = torch.randn((num_synthetic_samples, 10), generator=torch.Generator().manual_seed(seed))\n",
    "synthetic_data = vae.decoder(fixed_z_sample).detach().numpy()\n",
    "\n",
    "# add synthetic data to the original dataset\n",
    "np.random.seed(seed)\n",
    "y_extended = np.concatenate([y, np.random.choice(y, size=num_synthetic_samples)])\n",
    "X_extended = np.vstack([X_scaled, synthetic_data])\n",
    "\n",
    "# stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_extended, y_extended, test_size=0.2, random_state=seed, stratify=y_extended)\n",
    "\n",
    "# LASSO\n",
    "lasso = Lasso(alpha=0.1, random_state=seed)\n",
    "lasso.fit(X_train, y_train)\n",
    "selected_features = lasso.coef_ != 0\n",
    "X_train_selected = X_train[:, selected_features]\n",
    "X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "print(f\"Number of selected features after Lasso: {np.sum(selected_features)}\")\n",
    "\n",
    "# apply PCA\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components, random_state=seed)\n",
    "X_train_pca = pca.fit_transform(X_train_selected)\n",
    "X_test_pca = pca.transform(X_test_selected)\n",
    "\n",
    "# train MLP\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', max_iter=3000, \n",
    "                          alpha=0.8, solver='adam', random_state=seed, learning_rate_init=0.0001, verbose=True)\n",
    "mlp_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# training\n",
    "y_train_pred = mlp_model.predict(X_train_pca)\n",
    "train_auc = roc_auc_score(y_train, mlp_model.predict_proba(X_train_pca)[:, 1])\n",
    "\n",
    "# evaluate on test data\n",
    "y_test_pred = mlp_model.predict(X_test_pca)\n",
    "test_auc = roc_auc_score(y_test, mlp_model.predict_proba(X_test_pca)[:, 1])\n",
    "\n",
    "# evaluation metrics for training and testing sets\n",
    "print(\"\\nTraining Classification Report:\\n\", classification_report(y_train, y_train_pred))\n",
    "print(f'Training ROC AUC Score: {train_auc}')\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(f'Test ROC AUC Score: {test_auc}')\n",
    "\n",
    "\n",
    "# cross-validation with StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cross_val_auc = cross_val_score(mlp_model, X_train_pca, y_train, cv=stratified_kfold, scoring='roc_auc')\n",
    "print(f\"Cross-validated AUC: {cross_val_auc.mean():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c069d4f6-c37a-4cd3-b521-1e22849c6fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot ROC curve\n",
    "y_test_proba = mlp_model.predict_proba(X_test_pca)[:, 1]\n",
    "y_test_pred = (y_test_proba >= 0.92).astype(int) \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba, pos_label=6.0)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# seaborn\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=3, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "plt.xlim([-0.02, 1.02])\n",
    "plt.ylim([-0.02, 1.02])\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('AUC-ROC Curve for Cohort 1', fontsize=18, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# re-encoding labels (in the dataset, 0 is 1 and 1 is 6)\n",
    "y_test_binary = (y_test == 6.0).astype(int)  # Re-encode 6.0 as 1 and 1.0 as 0\n",
    "y_test_pred = (mlp_model.predict_proba(X_test_pca)[:, 1] >= 0.39).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0bdc629-198e-4c08-88b9-6b7a29ac1e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_test_pred)\n",
    "\n",
    "#confusion matrix heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=['Non-mTBI', 'mTBI'], \n",
    "            yticklabels=['Non-mTBI', 'mTBI'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix Heatmap for Cohort 1')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
